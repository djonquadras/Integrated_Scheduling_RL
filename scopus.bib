Scopus
EXPORT DATE: 30 April 2022

@ARTICLE{Yan2022,
author={Yan, Q. and Wang, H. and Wu, F.},
title={Digital twin-enabled dynamic scheduling with preventive maintenance using a double-layer Q-learning algorithm},
journal={Computers and Operations Research},
year={2022},
volume={144},
doi={10.1016/j.cor.2022.105823},
art_number={105823},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127545413&doi=10.1016%2fj.cor.2022.105823&partnerID=40&md5=c1577473e0c725f0f6dbae56680d2827},
affiliation={College of Information Science and Engineering, Northeastern University, Shenyang, 110819, China},
abstract={Dynamic scheduling methods are essential and critical to manufacturing systems because of uncertain events in the production process, such as new job insertions, order cancellations, worker absences, and machine breakdowns. Emerging digital twin (DT) technology can help detect disturbances by continuously comparing physical space with virtual space and triggering a rescheduling policy immediately after a disturbance. This enables dynamic scheduling and greatly reduces the deviation between preschedules and actual schedules. This study focuses on a DT-enabled integrated optimisation problem of flexible job shop scheduling and flexible preventive maintenance (PM) considering both machine and worker resources. A double-layer Q-learning algorithm (DLQL) is designed as the underlying key optimisation method to simultaneously learn the selection process of machines and operations to achieve efficient real-time scheduling. The superior solution performance of DLQL was verified by comparing it with two well-known metaheuristic algorithms and a single-layer Q-learning algorithm under several benchmarks. Furthermore, different disturbance settings were designed to illustrate the DLQL-based dynamic scheduling process in detail. The proposed reinforcement learning (RL)-driven DT enables efficient collaborative scheduling between production and maintenance departments and helps manufacturing companies improve the real-time decision-making process under uncertain perturbations. © 2022 Elsevier Ltd},
author_keywords={Digital twin;  Double-resource flexible job shop;  Preventive maintenance;  Reinforcement learning;  Uncertain disturbances},
keywords={Benchmarking;  Decision making;  E-learning;  Job shop scheduling;  Learning algorithms;  Manufacture;  Preventive maintenance;  Uncertainty analysis, Double layers;  Double-resource flexible job shop;  Dynamic scheduling;  Dynamic scheduling methods;  Flexible job shops;  Production process;  Q-learning algorithms;  Uncertain disturbances;  Uncertain events;  Workers', Reinforcement learning},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 62173076, 71671032},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2020YFB1708200},
funding_text 1={This work was supported in part by the National Key R&D Program of China under Grant No. 2020YFB1708200, and the National Natural Science Foundation of China under Grant Nos. 62173076 and 71671032 .},
references={Wang, S., Yu, J., An effective heuristic for flexible job-shop scheduling problem with maintenance activities (2010) Comput. Ind. Eng., 59 (3), pp. 436-447; Huang, J., Chang, Q., Arinez, J., Deep reinforcement learning based preventive maintenance policy for serial production lines (2020) Expert Syst. Appl., 160, p. 113701; Rahmati, S.H.A., Ahmadi, A., Govindan, K., A novel integrated condition-based maintenance and stochastic flexible job shop scheduling problem: simulation-based optimization approach (2018) Ann. Oper. Res., 269 (1-2), pp. 583-621; Cassady, C.R., Kutanoglu, E., Minimizing job tardiness using integrated preventive maintenance planning and production scheduling (2003) IIE Trans., 35 (6), pp. 503-513; Gao, J., Gen, M., Sun, L., Scheduling jobs and maintenances in flexible job shop with a hybrid genetic algorithm (2006) J. Intell. Manuf., 17 (4), pp. 493-507; Rajkumar, M., Asokan, P., Vamsikrishna, V., A GRASP algorithm for flexible job-shop scheduling with maintenance constraints (2010) Int. J. Prod. Res., 48 (22), pp. 6821-6836; Li, J.-Q., Pan, Q.-K., Chemical-reaction optimization for flexible job-shop scheduling problems with maintenance activity (2012) Appl. Soft Comput., 12 (9), pp. 2896-2912; Li, J.-Q., Pan, Q.-K., Tasgetiren, M.F., A discrete artificial bee colony algorithm for the multi-objective flexible job-shop scheduling problem with maintenance activities (2014) Appl. Math. Model., 38 (3), pp. 1111-1132; Zandieh, M., Khatami, A.R., Rahmati, S.H.A., Flexible job shop scheduling under condition-based maintenance: improved version of imperialist competitive algorithm (2017) Appl. Soft Comput., 58, pp. 449-464; Fang, Y., Peng, C., Lou, P., Zhou, Z., Hu, J., Yan, J., Digital-twin-based job shop scheduling toward smart manufacturing (2019) IEEE Trans. Ind. Inf., 15 (12), pp. 6425-6435; Wang, Y.-F., Adaptive job shop scheduling strategy based on weighted Q-learning algorithm (2020) J. Intell. Manuf., 31 (2), pp. 417-432; Tao, F., Zhang, H.E., Liu, A., Nee, A.Y.C., Digital twin in industry: State-of-the-art (2019) IEEE Trans. Ind. Inf., 15 (4), pp. 2405-2415; Grieves, M., Digital twin: manufacturing excellence through virtual factory replication (2014) White paper., 1, pp. 1-7; Ivanov, D., Dolgui, A., A digital supply chain twin for managing the disruption risks and resilience in the era of Industry 4.0 (2021) Production Planning & Control., 32 (9), pp. 775-788; Jiang, Y., Li, M., Guo, D., Wu, W., Zhong, R.Y., Huang, G.Q., Digital twin-enabled smart modular integrated construction system for on-site assembly (2022) Comput. Ind., 136, p. 103594; Tao, F., Zhang, M., Liu, Y., Nee, A.Y.C., Digital twin driven prognostics and health management for complex equipment (2018) CIRP Ann., 67 (1), pp. 169-172; Zhang, M., Tao, F., Nee, A.Y.C., Digital twin enhanced dynamic job-shop scheduling (2021) J. Manuf. Syst., 58, pp. 146-156; Liu, Z., Chen, W., Zhang, C., Yang, C., Cheng, Q., Intelligent scheduling of a feature-process-machine tool supernetwork based on digital twin workshop (2021) J. Manuf. Syst., 58, pp. 157-167; Errandonea, I., Beltrán, S., Arrizabalaga, S., Digital Twin for maintenance: A literature review (2020) Comput. Ind., 123, p. 103316; Fitouhi, M.-C., Nourelfath, M., Gershwin, S.B., Performance evaluation of a two-machine line with a finite buffer and condition-based maintenance (2017) Reliab. Eng. Syst. Saf., 166, pp. 61-72; Ruiz, R., Carlos García-Díaz, J., Maroto, C., Considering scheduling and preventive maintenance in the flowshop sequencing problem (2007) Comput. Oper. Res., 34 (11), pp. 3314-3330; Chen, J.-S., Scheduling of nonresumable jobs and flexible maintenance activities on a single machine to minimize makespan (2008) Eur. J. Oper. Res., 190 (1), pp. 90-102; Yang, S.-L., Ma, Y., Xu, D.-L., Yang, J.-B., Xu D-l, Yang J-b. Minimizing total completion time on a single machine with a flexible maintenance activity (2011) Comput. Oper. Res., 38 (4), pp. 755-770; Qi, X., Chen, T., Tu, F., Scheduling the maintenance on a single machine (1999) Journal of the operational Research Society., 50 (10), pp. 1071-1078; Mosheiov, G., Sarig, A., Scheduling a maintenance activity to minimize total weighted completion-time (2009) Comput. Math. Appl., 57 (4), pp. 619-623; Xu, D., Liu, M., Yin, Y., Hao, J., Scheduling tool changes and special jobs on a single machine to minimize makespan (2013) Omega., 41 (2), pp. 299-304; Wang, T., Baldacci, R., Lim, A., Hu, Q., A branch-and-price algorithm for scheduling of deteriorating jobs and flexible periodic maintenance on a single machine (2018) Eur. J. Oper. Res., 271 (3), pp. 826-838; Dominic, P.D., Kaliyamoorthy, S., Kumar, M.S., Efficient dispatching rules for dynamic job shop scheduling (2004) The International Journal of Advanced Manufacturing Technology., 24 (1), pp. 70-75; Kundakcı, N., Kulak, O., Hybrid genetic algorithms for minimizing makespan in dynamic job shop scheduling problem (2016) Comput. Ind. Eng., 96, pp. 31-51; Li, Y., He, Y., Wang, Y., Tao, F., Sutherland, J.W., An optimization method for energy-conscious production in flexible machining job shops with dynamic job arrivals and machine breakdowns (2020) J. Cleaner Prod., 254, p. 120009; Zhang, F., Mei, Y., Nguyen, S., Zhang, M., Collaborative multifidelity-based surrogate models for genetic programming in dynamic flexible job shop scheduling. IEEE Transactions on (2021) Cybernetics.; Van Moffaert, K., Nowé, A., Multi-objective reinforcement learning using sets of pareto dominating policies (2014) The Journal of Machine Learning Research., 15 (1), pp. 3483-3512; Zou, F., Yen, G.G., Tang, L., Wang, C., A reinforcement learning approach for dynamic multi-objective optimization (2021) Inf. Sci., 546, pp. 815-834; Mazyavkina, N., Sviridov, S., Ivanov, S., Burnaev, E., Reinforcement learning for combinatorial optimization: A survey (2021) Comput. Oper. Res., 134, p. 105400; Chen, J., Li, K., Li, K., Yu, P.S., Zeng, Z., Dynamic Bicycle Dispatching of Dockless Public Bicycle-sharing Systems Using Multi-objective Reinforcement Learning (2021) ACM Transactions on Cyber-Physical Systems (TCPS)., 5 (4), pp. 1-24; Lamghari, A., Dimitrakopoulos, R., Hyper-heuristic approaches for strategic mine planning under uncertainty (2020) Comput. Oper. Res., 115, p. 104590; Wei, Y., Zhao, M., A reinforcement learning-based approach to dynamic job-shop scheduling (2005) Acta Autom. Sin., 31 (5), p. 765; Luo, S., Dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning (2020) Appl. Soft Comput., 91, p. 106208; Adibi, M.A., Zandieh, M., Amiri, M., Multi-objective scheduling of dynamic job shop using variable neighborhood search (2010) Expert Syst. Appl., 37 (1), pp. 282-287; Shahrabi, J., Adibi, M.A., Mahootchi, M., A reinforcement learning approach to parameter estimation in dynamic job shop scheduling (2017) Comput. Ind. Eng., 110, pp. 75-82; Lei, D., Guo, X., Variable neighbourhood search for dual-resource constrained flexible job shop scheduling (2014) Int. J. Prod. Res., 52 (9), pp. 2519-2529; Zheng, X.-L., Wang, L., A knowledge-guided fruit fly optimization algorithm for dual resource constrained flexible job-shop scheduling problem (2016) Int. J. Prod. Res., 54 (18), pp. 5554-5566; Gong, G., Deng, Q., Gong, X., Liu, W., Ren, Q., A new double flexible job-shop scheduling problem integrating processing time, green production, and human factor indicators (2018) J. Cleaner Prod., 174, pp. 560-576; Gong, G., Chiong, R., Deng, Q., Gong, X., A hybrid artificial bee colony algorithm for flexible job shop scheduling with worker flexibility (2020) Int. J. Prod. Res., 58 (14), pp. 4406-4420; Sun, A., Song, Y., Yang, Y., Lei, Q., Dual Resource-constrained Flexible Job shop Scheduling Algorithm Considering the Quality of Key Jobs. China (2022) Mechanical Engineering., pp. 1-13. , http://kns.cnki.net/kcms/detail/42.1294.TH.20220104.0839.002.html, in Chinese; Kong, X.T.R., Luo, H., Huang, G.Q., Yang, X., Industrial wearable system: the human-centric empowering technology in Industry 4.0 (2019) J. Intell. Manuf., 30 (8), pp. 2853-2869; Zhao, Z., Shen, L., Yang, C., Wu, W., Zhang, M., Huang, G.Q., IoT and digital twin enabled smart tracking for safety management (2021) Comput. Oper. Res., 128, p. 105183; Pan, S., Ballot, E., Huang, G.Q., Montreuil, B., Physical Internet and interconnected logistics services: research and applications (2017) Taylor & Francis, 55 (9), pp. 2603-2609; Qiu, X., Luo, H., Xu, G., Zhong, R., Huang, G.Q., Physical assets and service sharing for IoT-enabled Supply Hub in Industrial Park (SHIP) (2015) Int. J. Prod. Econ., 159, pp. 4-15; Wang, H., Yan, Q., Wang, J., Blockchain-secured multi-factory production with collaborative maintenance using Q learning-based optimisation approach (2021) Int. J. Prod. Res., 1-18},
correspondence_address1={Wang, H.; College of Information Science and Engineering, China; email: hfwang@mail.neu.edu.cn},
publisher={Elsevier Ltd},
issn={03050548},
coden={CMORA},
language={English},
abbrev_source_title={Comp. Oper. Res.},
document_type={Article},
source={Scopus},
}

@ARTICLE{George2022,
author={George, B. and Loo, J. and Jie, W.},
title={Recent advances and future trends on maintenance strategies and optimisation solution techniques for offshore sector},
journal={Ocean Engineering},
year={2022},
volume={250},
doi={10.1016/j.oceaneng.2022.110986},
art_number={110986},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126134927&doi=10.1016%2fj.oceaneng.2022.110986&partnerID=40&md5=11463d5483bcbd1e980e6001ba82cc61},
affiliation={School of Computing and Engineering, University of West London, St Mary's Rd, Ealing, London, W5 5RF, United Kingdom},
abstract={Maintenance planning program of offshore assets is a complex activity due to its impact on the operational and safety risks and consequences, dependence on personnel resource availabilities, site constraints due to operational requirements and environmental factors, and uncertainties related to various vulnerabilities on asset. This paper elaborates the challenges on offshore maintenance frameworks and have carried out a review of recent state-of-the-art literature from which have observed that the current state-of-the-art does not incorporate site constraints of the asset related to offshore personnel resource availability and impact of time required to carry out activities, into the maintenance plan and its impact on other activities due to the maintenance. Also, it has been identified that dynamic and autonomous resource allocations for maintenance activities are not employed in the offshore maintenance planning program that allows each maintenance item to independently adjust its resource allocation based on the time required to complete the activity, to improve the resource utilisation. © 2022 Elsevier Ltd},
author_keywords={Deep reinforcement learning;  Machine learning;  Maintenance;  Multi-objective optimisation;  Offshore system;  Planning},
keywords={Maintenance;  Multiobjective optimization;  Offshore oil well production;  Personnel;  Planning;  Reinforcement learning;  Resource allocation, Future trends;  Maintenance planning;  Multi-objectives optimization;  Offshore systems;  Offshores;  Personnel resources;  Planning projects;  Resource availability;  Resources allocation;  State of the art, Deep learning, environmental factor;  future prospect;  machine learning;  maintenance;  offshore engineering;  optimization},
references={Abbas, M., Shafiee, M., An overview of maintenance management strategies for corroded steel structures in extreme marine environments (2020) Mar. Struct., 71, p. 102718; Adumene, S., Khan, F., Adedigba, S., Zendehboudi, S., Offshore system safety and reliability considering microbial influenced multiple failure modes and their interdependencies (2021) Reliab. Eng. Syst. Saf., 215, p. 107862; Ahmadi, O., Mortazavi, S.B., Mahabadi, H.A., Hosseinpouri, M., Development of a dynamic quantitative risk assessment methodology using fuzzy DEMATEL-BN and leading indicators (2020) Process Saf. Environ. Protect., 142, pp. 15-44; Alizpurua, J.I., Catterson, V.M., Papadopoulos, Y., Chiacchio, F., D'Urso, D., Supporting group maintenance through prognostics-enhanced dynamic dependability prediction (2017) Reliab. Eng. Syst. Saf., 168, pp. 171-188; Allal, A., Sahnoun, M., Adjoudj, R., Benslimane, S.M., Mazar, M., Multi-agent based simulation-optimization of maintenance routing in offshore wind farms (2021) Comput. Ind. Eng., 157, p. 107342; Asuquo, M.P., Wang, J., Zhang, L., Phylip-Jones, G., Application of a multiple attribute group decision making (MAGDM) model for selecting appropriate maintenance strategy for marine and offshore machinery operations (2019) Ocean. Eng., 179, pp. 246-260; Broek, M.A.J.U.H., Veldman, J., Fazi, S., Greijdanus, R., Evaluating resource sharing for offshore wind farm maintenance: the case of jack-up vessels (2019) Renew. Sustain. Energy Rev., 109, pp. 619-632; Chaabane, K., Khatab, A., Diallo, C., Aghezzaf, E.H., Venkatadri, U., Integrated imperfect multimission selective maintenance and repairpersons assignment problem (2020) Reliab. Eng. Syst. Saf., 199, p. 106895; Dehghani, A., Aslani, F., A review of defects in steel offshore structures and developed strengthening techniques (2019) Structures, 20, pp. 635-657; Diallo, C., Khatab, A., Venkatadri, U., Developing a bi-objective imperfect selective maintenance optimization model for multicomponent systems (2019) IFAC-PapersOnLine, 52-13, pp. 1079-1084; Fan, D., Ren, Y., Feng, Q., Zhu, B., Liu, Y., Wang, Z., A hybrid heuristic optimization of maintenance routing and scheduling for offshore wind farms (2019) J. Loss Prev. Process. Ind., 62, p. 103949; Fan, D., Zhang, A., Feng, Q., Cai, B., Liu, Y., Ren, Y., Group maintenance optimization of subsea Xmas trees with stochastic dependency (2021) Reliab. Eng. Syst. Saf., 209, p. 107450; Ferreira, N.N., Martins, M.R., Figueiredo, M.A.G.D., Gagno, V.H., Guidelines for life extension process management in oil and gas facilities (2020) J. Loss Prev. Process. Ind., 68, p. 104290; Ford, G., McMohan, C., Rowley, C., An examination of significant issues in naval maintenance (2015) Proced. CIRP, 38, pp. 197-203; Galante, G.M., Fata, C.M.L., Lupo, T., Passannanti, G., Handling the epistemic uncertainty in the selective maintenance problem (2020) Comput. Ind. Eng., 141, p. 106293; Garcia-Teruel, A., Rinaldi, G., Thies, P.R., Johanning, L., Jeffrey, H., (2022), "Life cycle assessment of floating offshore wind farms: an evaluation of operation and maintenance". In: Appl. Energy 307, 118067; Gass, S.I., Linear Programming: Methods and Applications” (1984), McGraw Hill New York -13: 978-0070229822; Hageman, R.B., Meulen, F.H.V.D., Rouhan, A., Kaminski, M.L., Quantifying uncertainties for risk-based inspection planning using in-service hull structure monitoring of FPSO hulls (2022) Mar. Struct., 81, p. 103100; Halim, S.Z., Janardanan, S., Flechas, T., Mannan, M.S., In search of causes behind offshore incidents: fire in offshore oil and gas Facilities (2018) J. Loss Prev. Process. Ind., 54, pp. 254-265; Han, Y., Zhen, X., Huang, Y., Vinnem, J.E., Integrated methodology for determination of preventive maintenance interval of safety barriers on offshore installations (2019) Process Saf. Environ. Protect., 132, pp. 313-324; Han, Y., Zhen, X., Huang, Y., Hybrid dynamic risk modelling for safety critical equipment on offshore installations (2021) Process Saf. Environ. Protect., 156, pp. 482-495; Hernandez, O.C.M., Shadman, M., Amiri, M.M., Silva, C., Estefen, S.F., Rovere, E.L., Environmental impacts of offshore wind installation operation and maintenance and decommissioning activities: a case study of Brazil (2021) Renew. Sustain. Energy Rev., 144, p. 110994; Hesabi, H., Nourelfath, M., Hajji, A., A deep learning predictive model for selective maintenance optimization (2022) Reliab. Eng. Syst. Saf., 219, p. 108191; Horrocks, P., Mansfield, D., Parker, K., Thomson, J., Atkinson, T., Worsley, J., (2010) Managing Ageing Asset- A Summary Guide”, , Warrington HSE; Hwang, H.J., Lee, J.H., Hwang, J.S., Jun, H.B., A study of the development of a condition-based maintenance system for an LNG FPSO (2018) Ocean. Eng., 164, pp. 604-615; Ibrion, M., Paltrinieri, N., Nejad, A.R., Learning from failures: accidents of marine structures on Norwegian continental shelf over 40 years time period (2020) Eng. Fail. Anal., 111, p. 104487; Ikonen, T.J., Mostafaei, H., Ye, Y., Bernal, D.E., Grossmann, I.E., Harjunkoski, I., Large-scale selective maintenance optimization using bathtub-shaped failure rates (2020) Comput. Chem. Eng., 139, p. 106876; Jamshidi, A., Jamshidi, F., Ait-Kadi, D., Ramudhin, A., Applied risk analysis approaches for maintenance of offshore wind turbines; a literature review (2019) IFAC-PapersOnLine, 52-13, pp. 1075-1078; Kan, M.S., Tan, A.C.C., Mathew, J., A review on prognostic techniques for non-stationary and non-linear rotating systems (2015) Mech. Syst. Signal Process., 62-63, pp. 1-20; Kang, J., Soares, C.G., An opportunistic maintenance policy for offshore wind farms (2020) Ocean. Eng., 216, p. 108075; Khatab, A., Diallo, C., Aghezzaf, E.H., Venkatadri, U., Joint optimization of the selective maintenance and repairperson assignment problem when using new and remanufactured spare parts (2019) IFAC-PapersOnLine, 52-13, pp. 1063-1068; Lazakis, I., Khan, S., An optimization framework for daily route planning and scheduling of maintenance vessel activities in offshore wind farms (2021) Ocean. Eng., 225, p. 108752; Leimeister, M., Kolios, A., Collu, M., Critical review of floating support structures for offshore wind farm deployment (2018) IOP Conf. Series: J. Phys. Conf., 1104; Lewandowski, M., Oelker, S., Towards autonomous control in maintenance and spare part logistics- challenges and opportunities for preacting maintenance concepts (2014) Proced. Technol., 15, pp. 333-340; Li, Y., Hu, Z., A review of multi-attributes decision-making models for offshore oil and gas facilities decommissioning (2021) Journal of Ocean Engineering and Science; Li, M., Wang, M., Kang, J., Sun, L., Jin, P., An opportunistic maintenance strategy for offshore wind turbine system considering optimal maintenance intervals of subsystems (2020) Ocean. Eng., 216, p. 108067; Li, M., Jiang, X., Negenborn, R.R., Opportunistic maintenance for offshore wind farms with multiple-component age-based preventive dispatch (2021) Ocean. Eng., 231, p. 109062; Lin, Z., Cevasco, D., Collu, M., A methodology to develop reduced-order models to support the operation and maintenance of offshore wind turbines (2020) Appl. Energy, 259, p. 114228; Liu, Y., Chen, Y., Jiang, T., On sequence planning for selective maintenance of multi-state systems under stochastic maintenance durations (2018) Eur. J. Oper. Res., 268, pp. 113-127; Liu, Y., Chen, Y., Jiang, T., Dynamic selective maintenance optimization for multi-state systems over a finite horizon: a deep reinforcement learning approach (2020) Eur. J. Oper. Res., 283, pp. 166-181; Liu, L., Yang, J., Kong, X., Xiao, Y., Multi-mission selective maintenance and repairpersons assignment problem with stochastic durations (2022) Reliab. Eng. Syst. Saf., 219, p. 108209; Lu, Y., Sun, L., Zhang, X., Feng, F., Kang, J., Fu, G., Condition based maintenance optimization for offshore wind turbine considering opportunities based on neural network approach (2018) Appl. Ocean Res., 74, pp. 69-79; Martin, R., Lazakis, I., Barbouchi, S., Johanning, L., Sensitivity analysis of offshore wind farm operation and maintenance cost and availability (2016) Renew. Energy, 85, pp. 1226-1236; Martinetti, A., Rajabalinejad, M., Dongen, L.V., Shaping the future maintenance operations: reflections on the adoptions of Augmented Reality through problems and opportunities (2017) Proced. CIRP, 59, pp. 14-17; Matias, J., Agotnes, J., Jaschke, J., Health-aware advanced control applied to a gas-lifted oil well network (2020) IFAC-PapersOnLine, 53-3, pp. 301-306; Mentes, A., Turan, O., A new resilient risk management model for offshore wind turbine maintenance (2019) Saf. Sci., 119, pp. 360-374; National Oceanic, Types of offshore oil and gas structures (2008) Ocean Explorer, , https://oceanexplorer.noaa.gov/explorations/06mexico/background/oil/media/types_600.html; Ni, W., Zhang, X., Zhang, W., Modified approximation method for structural failure probability analysis of high-dimensional systems (2021) Ocean. Eng., 237, p. 109486; Olugu, E.U., Mammedov, Y.D., Young, J.C.E., Yeap, P.S., Integrating spherical fuzzy Delphi and TOPSIS technique to identify indicators for sustainable maintenance management in the oil and gas industry (2021) Journal of King Saud University - Engineering Sciences; Ozguc, O., Fatigue assessment of FPSO hull side shell longitudinals using component stochastic and full spectral method (2020) Appl. Ocean Res., 101, p. 102289; Paik, J.K., Lee, J.M., Ko, M.J., Ultimate compressive strength of plate elements with pit corrosion wastage (2003) J. Eng. Marit. Environ., 217, pp. 185-200; Paik, J.K., Lee, J.M., Ko, M.J., Ultimate shear strength of plate elements with pit corrosion Wastage (2004) Thin-Walled Struct., 42, pp. 1161-1176; Ramirez-Ledesma, A.L., Juarez-Islas, J.A., Modification of the remaining useful life equation for pipes and plate processing of offshore oil platforms (2022) Process Saf. Environ. Protect., 157, pp. 429-442; Raza, A., Ulansky, V., Modelling of predictive maintenance for a periodically inspected system (2017) Proced. CIRP, 59, pp. 95-101; Ren, Z., Verma, A.S., Li, Y., Teuwen, J.J.E., Jiang, Z., Offshore wind turbine operations and maintenance: a state-of-the-art review (2021) Renew. Sustain. Energy Rev., 144, p. 110886; Rinaldi, G., Garcia-Teruel, A., Jeffrey, H., Thies, P.R., Johanning, L., Incorporating stochastic operation and maintenance models into the techno-economic analysis of floating offshore wind farms (2021) Appl. Energy, 301, p. 117420; Scheu, M., Matha, D., Hofmann, M., Muskulus, M., Maintenance strategies for large offshore wind farms (2012) Energy Proc., 24, pp. 281-288; Scheu, M.N., Tremps, L., Smolka, U., Kolios, A., Brennan, F., A systematic Failure Mode Effects and Criticality Analysis for offshore wind turbine systems towards integrated condition based maintenance strategies (2019) Ocean. Eng., 176, pp. 118-133; Schouten, T.N., Dekker, R., Hekimoglu, M., Eruguz, A.S., Maintenance optimization for a single wind turbine component under time-varying costs (2021) European Journal of Operational Research; Schrotenboer, A.H., Broek, M.A.J.U.H., Jargalsaikhan, B., Roodbergen, K.J., Coordinating technician allocation and maintenance routing for offshore wind farms (2018) Comput. Oper. Res., 98, pp. 185-197; Schrotenboer, A.H., Ursavas, E., Vis, I.F.A., Mixed integer programming models for planning maintenance at offshore wind farms under uncertainty (2020) Transport. Res. Part C, 112, pp. 180-202; Seiti, H., Hafezalkotob, A., Najafi, S.E., Khalaj, M., Developing a novel risk-based MCDM approach based on D numbers and fuzzy information axiom and its applications in preventive maintenance planning (2019) Appl. Soft Comp. J., 82, p. 105559; Shin, J., Jun, H., On condition-based maintenance policy (2015) J. Comput. Des. Eng., 2, pp. 119-127; Stock-Williams, C., Swamy, S.K., Automated daily maintenance planning for offshore wind farms (2019) Renew. Energy, 133, pp. 1393-1403; Sun, H., Bai, Y., Time-variant reliability assessment of FPSO hull girders (2003) Mar. Struct., 16, pp. 219-253; Teixeira, H.N., Lopes, I., Braga, A.C., Condition-based maintenance implementation: a literature review (2020) Procedia Manuf., 51, pp. 228-235; Tracht, K., Westerholt, J., Schuh, P., Spare parts planning for offshore wind turbines subject to restrictive maintenance conditions (2013) Proced. CIRP, 7, pp. 563-568; Viera, M., Henriques, E., Snyder, B., Reis, L., (2022), "Insights on the impact of structural health monitoring systems on the operation and maintenance of offshore wind support structures". In: Struct. Saf. 94, 102154; Werneck, R.D.O., Prates, R., Moura, R., Data-driven deep-learning forecasting for oil production and pressure (2021) Journal of Petroleum Science and Engineering; Yang, D., Wang, H., Feng, Q., Ren, Y., Sun, B., Wang, Z., Fleet-level selective maintenance problem under a phased mission scheme with short breaks: a heuristic sequential game approach (2018) Comput. Ind. Eng., 119, pp. 404-415; Yazdi, M., Nedjati, A., Abbassi, R., Fuzzy dynamic risk-based maintenance investment optimization for offshore process facilities (2019) J. Loss Prev. Process. Ind., 57, pp. 194-207; Yazdi, M., Nedjati, A., Zarei, E., Abbassi, R., A novel extension of DEMATEL approach for probabilistic safety analysis in process systems (2020) Saf. Sci., 121, pp. 119-136; Yazdi, M., Khan, F., Abbassi, R., Rusli, R., Improved DEMATEL methodology for effective safety management decision-making (2020) Saf. Sci., 127, p. 104705; Yeter, B., Garbatov, Y., Soares, C.G., Life extension classification of offshore wind assets using unsupervised machine learning (2022) Reliab. Eng. Syst. Saf., 219, p. 108229; Zagorowska, M., Spuntrup, F.S., Ditlefsen, A., Imsland, L., Lunde, E., Thornhill, N.F., Adaptive detection and prediction of performance degradation in off-shore turbomachinery (2020) Appl. Energy, 268, p. 114934; Zhang, C., Yang, T., Optimal maintenance planning and resource allocation for wind farms based on non-dominated sorting genetic algorithm-II (2021) Renew. Energy, 164, pp. 1540-1549; Zhang, B., Zhang, Z., A two-stage model for asynchronously scheduling offshore wind farm maintenance tasks and power productions (2021) Electr. Power Energy Syst., 130, p. 107013; Zhang, C., Gao, W., Yang, T., Guo, S., Opportunistic maintenance strategy for wind turbines considering weather conditions and spare parts inventory management (2019) Renew. Energy, 133, pp. 703-711; Zhong, S., Pantelous, A.A., Goh, M., Zhou, J., A reliability-and-cost based fuzzy approach to optimize preventive maintenance scheduling for offshore wind farms (2019) Mech. Syst. Signal Process., 124, pp. 643-663; Zhou, P., Yin, P.T., An opportunistic condition-based maintenance strategy for offshore wind farm based on predictive analytics (2019) Renew. Sustain. Energy Rev., 109, pp. 1-9; Zhu, W., Castanier, B., Bettayeb, B., A dynamic programming-based maintenance model of offshore wind turbine considering logistic delay and weather condition (2019) Reliab. Eng. Syst. Saf., 190, p. 106512; Zou, G., Faber, M.H., Gonzalez, A., Banisoleiman, K., Fatigue inspection and maintenance optimization: a comparison of information value, life cycle cost and reliability based approaches (2021) Ocean. Eng., 220, p. 108286},
correspondence_address1={George, B.; School of Computing and Engineering, St Mary's Rd, Ealing, United Kingdom; email: 21374270@student.uwl.ac.uk},
publisher={Elsevier Ltd},
issn={00298018},
language={English},
abbrev_source_title={Ocean Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lee2022,
author={Lee, Y.H. and Lee, S.},
title={Deep reinforcement learning based scheduling within production plan in semiconductor fabrication},
journal={Expert Systems with Applications},
year={2022},
volume={191},
doi={10.1016/j.eswa.2021.116222},
art_number={116222},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121264528&doi=10.1016%2fj.eswa.2021.116222&partnerID=40&md5=8e4cff72cd363c67a79c14fa85610e76},
affiliation={Department of Industrial Engineering, Yonsei University, 50 Yonsei-ro Seodaemun-gu, Seoul, 03722, South Korea},
abstract={In the semiconductor industry, efficient production planning and scheduling decisions are required to enhance the manufacturing productivity of a company as the system is complicated due to a re-entry characteristic and requires a long production lead time. Production planning is implemented before scheduling and is important for successful manufacturing operations. However, if scheduling at the operation level cannot execute the production plan, failures occur because of inconsistent decisions. Therefore, scheduling needs to fulfill the production plan to ensure realistic decision-making processes for the companies aiming for economic growth and global competitiveness. In this study, deep reinforcement learning (RL) is employed to deal with a scheduling process operating within the production plan. As the algorithm of the deep RL, Deep Q-network is conjugated, and a novel state, action, and reward are suggested to optimize the scheduling policy. As a result, the performance of the proposed deep RL method is in comparison with other dispatching rules, and the proposed method outperforms the other scheduling methods in diverse cases. © 2021 Elsevier Ltd},
author_keywords={Deep reinforcement learning;  Intelligent manufacturing;  Production planning;  Scheduling;  Semiconductor fabrication},
keywords={Decision making;  Economics;  Fabrication;  Planning;  Production control;  Reinforcement learning;  Scheduling;  Semiconductor device manufacture, Intelligent Manufacturing;  Leadtime;  Manufacturing operations;  Production Planning;  Production planning and scheduling;  Production planning IS;  Production plans;  Scheduling decisions;  Semi-conductor fabrication;  Semiconductor industry, Deep learning},
references={Atallah, R., Assi, C., Khabbaz, M., Deep reinforcement learning-based scheduling for roadside communication networks. In 2017 15th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt), 2017 (pp. 1-8): IEEE; Bergstra, J., Bengio, Y., Random search for hyper-parameter optimization (2012) The Journal of Machine Learning Research, 13 (1), pp. 281-305; Chen *, J.C., Chen, C.-W., Tai, C.-Y., Tyan, J.C., Dynamic state-dependent dispatching for wafer fabrication (2004) International Journal of Production Research, 42 (21), pp. 4547-4562; Connors, D., Feigin, G., Yao, D., Scheduling semiconductor lines using a fluid network model (1994) IEEE Transactions on Robotics and Automation, 10 (2), pp. 88-98; Dabbas, R.M., Fowler, J.W., A new scheduling approach using combined dispatching criteria in wafer fabs (2003) IEEE Transactions on semiconductor manufacturing, 16 (3), pp. 501-510; Demirkol, E., Uzsoy, R., Performance of decomposition methods for complex workshops under multiple criteria (1997) Computers & Industrial Engineering, 33 (1-2), pp. 261-264; Guo, C., Zhibin, J., Zhang, H., Li, N.A., Decomposition-based classified ant colony optimization algorithm for scheduling semiconductor wafer fabrication system (2012) Computers & Industrial Engineering, 62 (1), pp. 141-151; Hubbs, C.D., Li, C., Sahinidis, N.V., Grossmann, I.E., Wassick, J.M., A deep reinforcement learning approach for chemical production scheduling (2020) Computers & Chemical Engineering, 141, p. 106982; Jain, V., Swarnkar, R., Tiwari, M.K., Modelling and analysis of wafer fabrication scheduling via generalized stochastic Petri net and simulated annealing (2003) International Journal of Production Research, 41 (15), pp. 3501-3527; Kaskavelis, C.A., Caramanis, M.C., Efficient Lagrangian relaxation algorithms for industry size job-shop scheduling problems (1998) IIE Transactions, 30 (11), pp. 1085-1097; Kim, S.H., Lee, Y.H., Synchronized production planning and scheduling in semiconductor fabrication (2016) Computers & Industrial Engineering, 96, pp. 72-85; Kim, Y.-D., Lee, D.-H., Kim, J.-U., Roh, H.-K., A simulation study on lot release control, mask scheduling, and batch scheduling in semiconductor wafer fabrication facilities (1998) Journal of Manufacturing Systems, 17 (2), pp. 107-117; Kumar, S., Kumar, P., Queueing network models in the design and analysis of semiconductor wafer fabs (2001) IEEE Transactions on Robotics and Automation, 17 (5), pp. 548-561; Lang, S., Behrendt, F., Lanzerath, N., Reggelin, T., Müller, M., Integration of Deep Reinforcement Learning and Discrete-Event Simulation for Real-Time Scheduling of a Flexible Job Shop Production. In 2020 Winter Simulation Conference (WSC), 2020 (pp. 3057-3068): IEEE; Lee, S., Cho, Y., Lee, Y.H., Injection Mold Production Sustainable Scheduling Using Deep Reinforcement Learning (2020) Sustainability, 12 (20), p. 8718; Lee, Y.H., Kim, J.W., Daily stepper scheduling rule in the semiconductor manufacturing for MTO products (2011) The International Journal of Advanced Manufacturing Technology, 54 (1-4), pp. 323-336; Lee, S., Lee, Y.H., Improving Emergency Department Efficiency by Patient Scheduling Using Deep Reinforcement Learning (2020) Healthcare, 8 (2), p. 77; Li, L., Multi-ant colony-based sequencing method for semiconductor wafer fabrication facilities with multi-bottleneck (2012) International Journal of Modelling, Identification and Control, 15 (4), pp. 259-266; Liao, D.-Y., Chang, S.-C., Pei, K.-W., Chang, C.-M., Daily scheduling for R&D semiconductor fabrication (1996) IEEE Transactions on semiconductor manufacturing, 9 (4), pp. 550-561; Lu, S.C., Ramaswamy, D., Kumar, P., Efficient scheduling policies to reduce mean and variance of cycle-time in semiconductor manufacturing plants (1994) IEEE Transactions on Semiconductor Manufacturing, 7 (3), pp. 374-388; Luo, S., Dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning (2020) Applied Soft Computing, 91, p. 106208; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Human-level control through deep reinforcement learning (2015) Nature, 518 (7540), pp. 529-533; Park, I.-B., Huh, J., Kim, J., Park, J., A Reinforcement Learning Approach to Robust Scheduling of Semiconductor Manufacturing Facilities (2019) IEEE Transactions on Automation Science and Engineering; Pinedo, M., (2012) Scheduling, 29. , Springer; Qiao, F., Ma, Y.-M., Li, L.I., Yu, H.-X., A Petri net and extended genetic algorithm combined scheduling method for wafer fabrication (2013) IEEE Transactions on Automation Science and Engineering, 10 (1), pp. 197-204; Shi, D., Fan, W., Xiao, Y., Lin, T., Xing, C., Intelligent scheduling of discrete automated production line via deep reinforcement learning (2020) International Journal of Production Research, 58 (11), pp. 3362-3380; Sourirajan, K., Uzsoy, R., Hybrid decomposition heuristics for solving large-scale scheduling problems in semiconductor wafer fabrication (2007) Journal of Scheduling, 10 (1), pp. 41-65; Stricker, N., Kuhnle, A., Sturm, R., Friess, S., Reinforcement learning for adaptive order dispatching in the semiconductor industry (2018) CIRP Annals, 67 (1), pp. 511-514; Sung, C.S., Choung, Y.I., Minimizing makespan on a single burn-in oven in semiconductor manufacturing (2000) European Journal of Operational Research, 120 (3), pp. 559-574; Tao, F., Qi, Q., Liu, A., Kusiak, A., Data-driven smart manufacturing (2018) Journal of Manufacturing Systems, 48, pp. 157-169; Tao, F., Qi, Q., Wang, L., Nee, A., Digital twins and cyber–physical systems toward smart manufacturing and industry 4.0: Correlation and comparison (2019) Engineering, 5 (4), pp. 653-661; Tsai, C.-H., Feng, Y.-M., Li, R.-K., A hybrid dispatching rules in wafer fabrication factories (2003) International Journal of the Computer, the Internet and Management, 11 (1), pp. 64-72; Wang, J., Ma, Y., Zhang, L., Gao, R.X., Wu, D., Deep learning for smart manufacturing: Methods and applications (2018) Journal of Manufacturing Systems, 48, pp. 144-156; Waschneck, B., Reichstaller, A., Belzner, L., Altenmüller, T., Bauernhansl, T., Knapp, A., Deep reinforcement learning for semiconductor production scheduling. In 2018 29th annual SEMI advanced semiconductor manufacturing conference (ASMC), 2018a (pp. 301-306): IEEE; Waschneck, B., Reichstaller, A., Belzner, L., Altenmüller, T., Bauernhansl, T., Knapp, A., Optimization of global production scheduling with deep reinforcement learning (2018) Procedia CIRP, 72, pp. 1264-1269; Watkins, C., (1989), J. C. H. Learning from delayed rewards; Wein, L.M., Scheduling semiconductor wafer fabrication (1988) IEEE Transactions on Semiconductor Manufacturing, 1 (3), pp. 115-130; Wen, H.-W., Fu, L.-C., Huang, S.-S. Modeling, scheduling, and prediction in wafer fabrication systems using queueing Petri net and genetic algorithm. In Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No. 01CH37164), 2001 (Vol. 4, pp. 3559-3564): IEEE; Zhang, J., Ding, G., Zou, Y., Qin, S., Fu, J., Review of job shop scheduling research and its new perspectives under Industry 4.0 (2019) Journal of Intelligent Manufacturing, 30 (4), pp. 1809-1830; Zhong, R.Y., Xu, X., Klotz, E., Newman, S.T., Intelligent manufacturing in the context of industry 4.0: A review (2017) Engineering, 3 (5), pp. 616-630},
correspondence_address1={Lee, S.email: shbrandonlee@yonsei.ac.kr},
publisher={Elsevier Ltd},
issn={09574174},
coden={ESAPE},
language={English},
abbrev_source_title={Expert Sys Appl},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dong2022,
author={Dong, Z. and Ren, T. and Weng, J. and Qi, F. and Wang, X.},
title={Minimizing the Late Work of the Flow Shop Scheduling Problem with a Deep Reinforcement Learning Based Approach},
journal={Applied Sciences (Switzerland)},
year={2022},
volume={12},
number={5},
doi={10.3390/app12052366},
art_number={2366},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125473401&doi=10.3390%2fapp12052366&partnerID=40&md5=3a0dd857910b4aa8e0513bda0ab625df},
affiliation={Department of Software, Northeastern University, Shenyang, 110819, China},
abstract={In the field of industrial manufacturing, assembly line production is the most common production process that can be modeled as a permutation flow shop scheduling problem (PFSP). Minimizing the late work criteria (tasks remaining after due dates arrive) of production planning can effectively reduce production costs and allow for faster product delivery. In this article, a novel learning‐based approach is proposed to minimize the late work of the PFSP using deep reinforcement learning (DRL) and graph isomorphism network (GIN), which is an innovative combination of the field of combinatorial optimization and deep learning. The PFSPs are the well‐known permutation flow shop problem and each job comes with a release date constraint. In this work, the PFSP is defined as a Markov decision process (MDP) that can be solved by reinforcement learning (RL). A complete graph is introduced for describing the PFSP instance. The proposed policy network combines the graph representation of PFSP and the sequence information of jobs to predict the dis-tribution of candidate jobs. The policy network will be invoked multiple times until a complete sequence is obtained. In order to further improve the quality of the solution obtained by reinforcement learning, an improved iterative greedy (IG) algorithm is proposed to search the solution locally. The experimental results show that the proposed RL and the combined method of RL+IG can obtain better solutions than other excellent heuristic and meta‐heuristic algorithms in a short time. © 2022 by the authors. Li-censee MDPI, Basel, Switzerland.},
author_keywords={Combinatorial optimization;  Deep reinforcement learning;  Flow shop scheduling;  Graph neural network;  Iterated greedy;  Late work;  Pointer network},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61902057},
funding_details={State Key Laboratory of RoboticsState Key Laboratory of Robotics, 2020‐KF‐12‐11},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, N181703005, N181706001, N182608003, N2017008, N2017009},
funding_text 1={Funding: This research was funded by Fundamental Research Funds for the Central Universities (N181706001, N2017009, N2017008, N182608003, N181703005), National Natural Science Founda‐ tion of China (61902057), Joint Fund of Science & Technology Department of Liaoning Province and State Key Laboratory of Robotics, China (2020‐KF‐12‐11).},
funding_text 2={This research was funded by Fundamental Research Funds for the Central Universities (N181706001, N2017009, N2017008, N182608003, N181703005), National Natural Science Foundation of China (61902057), Joint Fund of Science & Technology Department of Liaoning Province and State Key Laboratory of Robotics, China (2020?KF?12?11).},
references={Johnson, S.M., Optimal two‐and three‐stage production schedules with setup times included (1954) Nav. Res. Logist. Q, 1, pp. 61-68; Garey, M.R., Johnson, D.S., Sethi, R., The complexity of flowshop and jobshop scheduling (1976) Math. Oper. Res, 1, pp. 117-129; Ruiz, R., Maroto, C., A comprehensive review and evaluation of permutation flowshop heuristics (2005) Eur. J. Oper. Res, 165, pp. 479-494; Nawaz, M., Enscore, E.E., Ham, I., A heuristic algorithm for the m‐machine, n‐job flow‐shop sequencing problem (1983) Omega, 11, pp. 91-95; Campbell, H.G., Dudek, R.A., Smith, M.L., A heuristic algorithm for the n job, m machine sequencing problem (1970) Manag. Sci, 16. , B‐630–B‐637; Tseng, L, Lin, Y, A hybrid genetic algorithm for no‐wait flowshop scheduling problem (2010) Int. J. Prod. Econ, 128, pp. 144-152. , Y T; Nowicki, E., Smutnicki, C., A fast tabu search algorithm for the permutation flow‐shop problem (1996) Eur. J. Oper. Res, 91, pp. 160-175; Pan, Q, Tasgetiren, M.F., Liang, Y, A discrete particle swarm optimization algorithm for the no‐wait flowshop scheduling problem (2008) Comput. Oper. Res, 35, pp. 2807-2839. , K C; Tasgetiren, M.F., Sevkli, M., Liang, Y.‐C., Gencyilmaz, G., Particle swarm optimization algorithm for permutation flowshop sequencing problem (2004) Proceedings of the International Workshop on Ant Colony Optimization and Swarm Intelligence, pp. 382-389. , Brussels, Belgium, 5–8 September; Potts, C.N., van Wassenhove, L.N., Single machine scheduling to minimize total late work (1992) Oper. Res, 40, pp. 586-595; Błażewicz, J., Pesch, E., Sterna, M., Werner, F., Total late work criteria for shop scheduling problems (1999) Proceedings of the Operations Research Proceedings 1999, pp. 354-359. , Magdeburg, Germany, 1–3 September; Chen, R., Yuan, J., Ng, C., Cheng, T., Single‐machine scheduling with deadlines to minimize the total weighted late work (2019) Nav. Res. Logist, 66, pp. 582-595; Chen, X., Sterna, M., Han, X., Blazewicz, J., Scheduling on parallel identical machines with late work criterion: Offline and online cases (2016) J. Sched, 19, pp. 729-736; Leung, J., Minimizing total weighted error for imprecise computation tasks and related problems (2004) Handbook of Scheduling: Algorithms, Models, and Performance Analysis, p. 34. , CRC Press: Boca Raton, FL, USA; Ren, J., Zhang, Y., Sun, G., The NP‐hardness of minimizing the total late work on an unbounded batch machine (2009) Asia‐Pac. J. Oper. Res, 26, pp. 351-363; Ren, J., Du, D., Xu, D., The complexity of two supply chain scheduling problems (2013) Inf. Processing Lett, 113, pp. 609-612; Sterna, M., A survey of scheduling problems with late work criteria (2011) Omega, 39 (2), p. 120129; Pesch, E., Sterna, M., Late work minimization in flow shops by a genetic algorithm (2009) Comput. Ind. Eng, 57, pp. 1202-1209; Gerstl, E., Mor, B., Mosheiov, G., Scheduling on a proportionate flowshop to minimise total late work (2019) Int. J. Prod. Res, 57, pp. 531-543; Bello, I., Pham, H., Le, Q.V., Norouzi, M., Bengio, S., (2016) Neural combinatorial optimization with reinforcement learning, , arXiv arXiv:1611.09940; Hu, H., Zhang, X., Yan, X., Wang, L., Xu, Y., (2017) Solving a new 3d bin packing problem with deep reinforcement learning method, , arXiv arXiv:1708.05930; Kool, W., van Hoof, H., Welling. M. Attention, Learn to Solve Routing Problems! (2018) Proceedings of the International Conference on Learning Representations, , Vancouver, BC, Canada, 30 April; Zhang, R., Prokhorchuk, A., Dauwels, J., Deep reinforcement learning for traveling salesman problem with time windows and rejections (2020) Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. , Glasgow, UK, 28 September; Vinyals, O., Fortunato, M., Jaitly, N., Pointer networks (2015) Adv. Neural Inf. Processing Syst, 28, pp. 1-9; Nazari, M., Oroojlooy, A., Snyder, L., Takáč, M., Reinforcement learning for solving the vehicle routing problem (2018) Adv. Neural Inf. Processing Syst, 31, pp. 1-11; Chen, X., Tian, Y., Learning to perform local rewriting for combinatorial optimization (2019) Adv. Neural Inf. Processing Syst, 32, pp. 6281-6292; Lu, H., Zhang, X., Yang, S., A learning‐based iterative method for solving vehicle routing problems (2019) Proceedings of the International Conference on Learning Representations, , New Orleans, LA, USA, 6–9 May; Wu, Y., Song, W., Cao, Z., Zhang, J., Lim, A., (2019) Learning improvement heuristics for solving the travelling salesman problem, , arXiv arXiv:1912.05784; Khalil, E., Dai, H., Zhang, Y., Dilkina, B., Song, L., Learning combinatorial optimization algorithms over graphs (2017) Adv. Neural Inf. Processing Syst, 30, pp. 1-11; Lederman, G., Rabe, M.N., Seshia, S.A., (2018) Learning heuristics for automated reasoning through deep reinforcement learning, , arXiv arXiv:1807.08058; Gupta, J.N., Stafford, E.F., Flowshop scheduling research after five decades (2006) Eur. J. Oper. Res, 169, pp. 699-711; Baker, K.R., Trietsch, D., (2013) Principles of Sequencing and Scheduling, , John Wiley & Sons: New York, NY, USA; Sutskever, I., Vinyals, O., Le, Q.V., Sequence to sequence learning with neural networks (2014) Adv. Neural Inf. Processing Syst, 27, pp. 1-9; Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M., Monfardini, G., The graph neural network model (2008) IEEE Trans. Neural Netw, 20, pp. 61-80; Ma, Q., Ge, S., He, D., Thaker, D., Drori, I., (2019) Combinatorial optimization by graph pointer networks and hierarchical reinforcement learning, , arXiv arXiv:1911.04936; Xu, K., Hu, W., Leskovec, J., Jegelka, S., How Powerful are Graph Neural Networks? (2018) Proceedings of the International Conference on Learning Representations, , Vancouver, BC, Canada, 30 April; Kipf, T.N., Welling, M., (2016) Semi‐supervised classification with graph convolutional networks, , arXiv arXiv:1609.02907; Bennett, C.C., Hauser, K., Artificial intelligence framework for simulating clinical decision‐making: A Markov decision process approach (2013) Artif. Intell. Med, 57, pp. 9-19; Williams, R.J., Simple statistical gradient‐following algorithms for connectionist reinforcement learning (1992) Mach. Learn, 8, pp. 229-256; Rennie, S.J., Marcheret, E., Mroueh, Y., Ross, J., Goel, V., Self‐critical sequence training for image captioning (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7008-7024. , Honolulu, HI, USA, 21–26 July; Ruiz, R., Stützle, T., A simple and effective iterated greedy algorithm for the permutation flowshop scheduling problem (2007) Eur. J. Oper. Res, 177, pp. 2033-2049; Dubois‐Lacoste, J., Pagnozzi, F., Stützle, T., An iterated greedy algorithm with optimization of partial solutions for the makespan permutation flowshop problem (2017) Comput. Oper. Res, 81, pp. 160-166; Taillard, E., Benchmarks for basic scheduling problems (1993) Eur. J. Oper. Res, 64, pp. 278-285; Ivković, N., Jakobović, D., Golub, M., Measuring performance of optimization algorithms in evolutionary computation (2016) Int. J. Mach. Learn. Comput, 6, pp. 167-171; Wang, H., Wang, W., Sun, H., Cui, Z., Rahnamayan, S., Zeng, S., A new cuckoo search algorithm with hybrid strategies for flow shop scheduling problems (2017) Soft Comput, 21, pp. 4297-4307; Ince, Y., Karabulut, K., Tasgetiren, M.F., Pan, Q, A discrete artificial bee colony algorithm for the permutation flowshop scheduling problem with sequence‐dependent setup times (2016) Proceedings of the 2016 IEEE congress on Evolutionary computation (CEC), pp. 3401-3408. , K Vancouver, BC, Canada, 24–29 July; Wang, X., Ren, T., Bai, D., Ezeh, C., Zhang, H., Dong, Z., Minimizing the sum of makespan on multi‐agent single‐machine scheduling with release dates (2021) Swarm Evol. Comput, 69, p. 100996},
correspondence_address1={Ren, T.; Department of Software, China; email: chinarentao@163.com},
publisher={MDPI},
issn={20763417},
language={English},
abbrev_source_title={Appl. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wurster2022575,
author={Wurster, M. and Michel, M. and May, M.C. and Kuhnle, A. and Stricker, N. and Lanza, G.},
title={Modelling and condition-based control of a flexible and hybrid disassembly system with manual and autonomous workstations using reinforcement learning},
journal={Journal of Intelligent Manufacturing},
year={2022},
volume={33},
number={2},
pages={575-591},
doi={10.1007/s10845-021-01863-3},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122662773&doi=10.1007%2fs10845-021-01863-3&partnerID=40&md5=8dee46d72bb2b3352b32fecb10c9ecb6},
affiliation={wbk – Institute of Production Science, Karlsruhe Institute of Technology (KIT), Kaiserstr. 12, Karlsruhe, 76131, Germany},
abstract={Remanufacturing includes disassembly and reassembly of used products to save natural resources and reduce emissions. While assembly is widely understood in the field of operations management, disassembly is a rather new problem in production planning and control. The latter faces the challenge of high uncertainty of type, quantity and quality conditions of returned products, leading to high volatility in remanufacturing production systems. Traditionally, disassembly is a manual labor-intensive production step that, thanks to advances in robotics and artificial intelligence, starts to be automated with autonomous workstations. Due to the diverging material flow, the application of production systems with loosely linked stations is particularly suitable and, owing to the risk of condition induced operational failures, the rise of hybrid disassembly systems that combine manual and autonomous workstations can be expected. In contrast to traditional workstations, autonomous workstations can expand their capabilities but suffer from unknown failure rates. For such adverse conditions a condition-based control for hybrid disassembly systems, based on reinforcement learning, alongside a comprehensive modeling approach is presented in this work. The method is applied to a real-world production system. By comparison with a heuristic control approach, the potential of the RL approach can be proven simulatively using two different test cases. © 2022, The Author(s).},
author_keywords={Disassembly automation;  Hybrid disassembly;  Production control;  Reinforcement learning;  Remanufacturing},
keywords={Failure analysis;  Reinforcement learning, Condition;  Disassembly and reassembly;  Disassembly automation;  Disassembly systems;  Hybrid disassembly;  Operation management;  Production planning and control;  Production system;  Remanufacturing;  Used product, Production control},
funding_details={Carl-Zeiss-StiftungCarl-Zeiss-Stiftung, CZS},
funding_text 1={The project AgiProbot is funded by the Carl Zeiss Foundation.},
references={Altekin, F.T., Akkan, C., Task-failure-driven rebalancing of disassembly lines (2012) International Journal of Production Research, 50, pp. 4955-4976; Altenmüller, T., Stüker, T., Waschneck, B., Kuhnle, A., Lanza, G., Reinforcement learning for an intelligent and autonomous production control of complex job-shops under time constraints (2020) Production Engineering, 14, pp. 319-328; Environmental Responsibility Report: 2019 Progress Report, covering fiscal year 2018 (2019) Apple Inc., , https://www.apple.com/environment/pdf/Apple_Environmental_Responsibility_Report_2019.pdf, Cupertino, CA, Accessed 21 November 2020; Aytug, H., Lawley, M.A., McKay, K., Mohan, S., Uzsoy, R., Executing production schedules in the face of uncertainties: A review and some future directions (2005) European Journal of Operational Research, 161, pp. 86-110; Bdiwi, M., Rashid, A., Putz, M., Autonomous disassembly of electric vehicle motors based on robot cognition (2016) 2016 IEEE International Conference on Robotics and Automation (ICRA), Stockholm, Sweden, 5/16/2016 - 5/21/2016, pp. 2500-2505. , https://doi.org/10.1109/ICRA.2016.7487404, A. Okamura & A. Menciassi (Eds.), Piscataway, NJ, IEEE; Büker, U., Drüe, S., Götze, N., Hartmann, G., Kalkreuter, B., Stemmer, R., Vision-based control of an autonomous disassembly station (2001) Robotics and Autonomous Systems, 35, pp. 179-189; Colledani, M., Battaïa, O., A decision support system to manage the quality of End-of-Life products in disassembly systems (2016) CIRP Annals, 65, pp. 41-44; Csáji, B.C., Monostori, L., Kádár, B., Reinforcement learning in a distributed market-based production control system (2006) Advanced Engineering Informatics, 20, pp. 279-288; Cunha, B., Madureira, A., Fonseca, B., Coelho, D., (2018) Deep Reinforcement Learning as a Job Shop Scheduling Solver: A Literature Review, pp. 350-359. , . In Ana Maria Madureira, Ajith Abraham, Niketa Gandhi, Maria Leonilde Varela, & Janusz Kacprzyk (Eds.), Hybrid Intelligent Systems; Cunha, B., Madureira, A.M., Fonseca, B., Coelho, D., Deep Reinforcement Learning as a Job Shop Scheduling Solver: A Literature Review (2020) Hybrid Intelligent Systems: 18Th International Conference on Hybrid Intelligent Systems (, 923, pp. 350-359. , A. Abraham, HIS 2018) held in Porto, Portugal, December 13-15,, Cham, Springer International Publishing; Dios, M., Framinan, J., A review and classification of computer-based manufacturing scheduling tools (2016) Computers & Industrial Engineering, 99, pp. 229-249; Duflou, J.R., Seliger, G., Kara, S., Umeda, Y., Ometto, A., Willems, B., Efficiency and feasibility of product disassembly: A case-based study (2008) CIRP Annals, 57, pp. 583-600; Duta, L., Henrioud, J.M., Caciula, I., A real time solution to control disassembly processes (2007) IFAC Proceedings Volumes, 40, pp. 789-794. , &, (,).,., https://doi.org/10.3182/20070927-4-RO-3905.00130; Eisele, W., Knobloch, A.P., (2014) Technik des betrieblichen Rechnungswesens: Buchführung und Bilanzierung, Kosten- und Leistungsrechnung, , Sonderbilanzen (8th ed., Vahlens Handbücher). München: Verlag Franz Vahlen; Gao, M., Zhou, M.C., Fuzzy reasoning Petri nets for demanufacturing process decision (2001) 2001 IEEE International Symposium on Electronics and the Environment. 2001 IEEE ISEE, Denver, CO, USA, 7-9 May 2001, pp. 167-172. , https://doi.org/10.1109/ISEE.2001.924521, Piscataway, N.J, IEEE; Guide, V.D.R., Production planning and control for remanufacturing: industry practice and research needs (2000) Journal of Operations Management, 18, pp. 467-483; Gungor, A., Gupta, S.M., A solution approach to the disassembly line balancing problem in the presence of task failures (2001) International Journal of Production Research, 39, pp. 1427-1467; Gupta, S.M., Taleb, K.N., Scheduling disassembly (1994) International Journal of Production Research, 32, pp. 1857-1866; Häfner, B., (2020), http://agiprobot.de/, AgiProbot, Accessed 31 July 2020; Junior, M.L., Filho, M.G., Production planning and control for remanufacturing: literature review and analysis (2012) Production Planning & Control, 23, pp. 419-435; Kim, H.J., Chiotellis, S., Seliger, G., Dynamic process planning control of hybrid disassembly systems (2009) The International Journal of Advanced Manufacturing Technology, 40, pp. 1016-1023; Kim, H.J., Ciupek, M., Buchholz, A., Seliger, G., Adaptive disassembly sequence control by using product and system information (2006) Robotics and Computer-Integrated Manufacturing, 22, pp. 267-278; Kim, H.J., Harms, R., Seliger, G., Automatic Control Sequence Generation for a Hybrid Disassembly System (2007) IEEE Transactions on Automation Science and Engineering, 4, pp. 194-205; Kim, H.J., Lee, D.H., Xirouchakis, P., Disassembly scheduling: literature review and future research directions (2007) International Journal of Production Research, 45, pp. 4465-4484; Kimemia, J., Gershwin, S.B., An Algorithm for the Computer Control of a Flexible Manufacturing System (1983) IIE Transactions, 15, pp. 353-362; Kopacek, P., Kopacek, B., Robotized Disassembly of Mobile Phones (2003) IFAC Proceedings Volumes, 36, pp. 103-105. , &, (,).,., https://doi.org/10.1016/S1474-6670(17)37669-3; Kopacek, P., Kopacek, B., Intelligent, flexible disassembly (2006) The International Journal of Advanced Manufacturing Technology, 30, pp. 554-560; Kuhnle, A., (2020), SimRLFab: Simulation and reinforcement learning framework for production planning and control of complex job shop manufacturing systems, GitHub; Kuhnle, A., Kaiser, J.P., Theiß, F., Stricker, N., Lanza, G., Designing an adaptive production control system using reinforcement learning (2021) Journal of Intelligent Manufacturing, 32, pp. 855-876; Kuhnle, A., Lanza, G., Application of Reinforcement Learning in Production Planning and Control of Cyber Physical Production Systems (2019) Machine Learning for Cyber Physical Systems: Selected Papers from the International Conference ML4CPS 2018 (, 9, pp. 123-132. , J. Beyerer, Berlin, Germany, Springer Vieweg; Kuhnle, A., May, M.C., Schäfer, L., Lanza, G., Explainable reinforcement learning in production control of job shop manufacturing system (2021) International Journal of Production Research, 24, pp. 1-23; Kuhnle, A., Röhrig, N., Lanza, G., Autonomous order dispatching in the semiconductor industry using reinforcement learning (2019) Procedia CIRP, 79, pp. 391-396; Kuhnle, A., Schaarschmidt, M., Fricke, K., (2017) Tensorforce: a TensorFlow library for applied reinforcement learning., , https://github.com/tensorforce/tensorforce; Kuhnle, A., Schäfer, L., Stricker, N., Lanza, G., Design, Implementation and Evaluation of Reinforcement Learning for an Adaptive Order Dispatching in Job Shop Manufacturing Systems (2019) Procedia CIRP, 81, pp. 234-239; Kurilova-Palisaitiene, J., Sundin, E., Challenges and Opportunities of Lean Remanufacturing (2014) International Journal of Automation Technology, 8, pp. 644-652; Lambert, A.J.D., Gupta, S.M., (2004) Disassembly Modeling for Assembly, Maintenance, Reuse and Recycling. CRC Press; Lawler, E.L., Lenstra, J.K., Kan, R., Shmoys, D.B., Chapter 9 Sequencing and scheduling: Algorithms and complexity (2005) Logistics of production and inventory (Vol, 4, pp. 445-522. , Graves SC, (ed), Elsevier, Handbooks Operations Research and Management Science, 4). Amsterdam; Lee, D.H., Kang, J.G., Xirouchakis, P., Disassembly planning and scheduling: Review and further research (2001) Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture, 215, pp. 695-709. , &, (,).,., https://doi.org/10.1243/0954405011518629; Lund, R.T., (1984) Remanufacturing: The experience of the United States and implications for developing countries, 2. , Integrated resource recovery,, Washington, DC, The World Bank; Madureira, A., Pereira, I., Falcao, D., Dynamic adaptation for scheduling under rush manufacturing orders with case-based reasoning (2013) In Int. Conf. on Algebraic and Symbolic Computation.; McKay, K.N., Safayeni, F.R., Buzacott, J.A., Job-Shop Scheduling Theory: What Is Relevant? (1988) Interfaces, 18, pp. 84-90; Mhada, F., Hajji, A., Malhamé, R., Gharbi, A., Pellerin, R., Production control of unreliable manufacturing systems producing defective items (2011) Journal of Quality in Maintenance Engineering, 17, pp. 238-253; Moore, K.E., Gungor, A., Gupta, S.M., A Petri net approach to disassembly process planning (1998) Computers & Industrial Engineering, 35, pp. 165-168; Pinedo, M.L., (2016) Scheduling, , Springer International Publishing, Cham; Poschmann, H., Brüggemann, H., Goldmann, D., Disassembly 4.0: A Review on Using Robotics in Disassembly Tasks as a Way of Automation (2020) Chemie Ingenieur Technik, 92, pp. 341-359; Priyono, A., Ijomah, W., Bititci, U., Disassembly for remanufacturing: A systematic literature review, new model development and future research needs (2016) Journal of Industrial Engineering and Management, 9, p. 899; Reisig, W., (2013) Understanding Petri Nets, , Springer Berlin Heidelberg, Berlin, Heidelberg; Riggs, R.J., Battaïa, O., Hu, S.J., Disassembly line balancing under high variety of end of life states using a joint precedence graph approach (2015) Journal of Manufacturing Systems, 37, pp. 638-648; Rujanavech, C., Lessard, J., Chandler, S., Shannon, S., Dahmus, J., Guzzo, R., (2016), https://www.apple.com/environment/pdf/Liam_white_paper_Sept2016.pdf, Liam - An Innovation Story. Cupertino, CA, Accessed 21 November 2020; Scholz-Reiter, B., Scharke, H., Hucht, A., Flexible robot-based disassembly cell for obsolete TV-sets and monitors (1999) Robotics and Computer-Integrated Manufacturing, 15, pp. 247-255; Slama, I., Ben-Ammar, O., Masmoudi, F., Dolgui, A., Disassembly scheduling problem: literature review and future research directions (2019) IFAC-PapersOnLine, 52, pp. 601-606; Tang, Y., Zhou, M., Caudill, R.J., An integrated approach to disassembly planning and demanufacturing operation (2001) IEEE Transactions on Robotics and Automation, 17, pp. 773-784; Tolio, T., Bernard, A., Colledani, M., Kara, S., Seliger, G., Duflou, J., Design, management and control of demanufacturing and remanufacturing systems (2017) CIRP Annals, 66, pp. 585-609; Tumkor, S., Senol, G., Disassembly Precedence Graph Generation (2007) In Assembly and Manufacturing, 2007. ISAM ‘07. IEEE International Symposium On, pp. 70-75. , https://doi.org/10.1109/ISAM.2007.4288451; Ullerich, C., Buscher, U., Flexible disassembly planning considering product conditions (2013) International Journal of Production Research, 51, pp. 6209-6228; Vongbunyong, S., Chen, W.H., (2015) Disassembly automation: Automated systems with cognitive abilities (Sustainable production, , Springer, life cycle engineering and management). Cham; Vongbunyong, S., Kara, S., Pagnucco, M., Basic behaviour control of the vision-based cognitive robotic disassembly automation (2013) Assembly Automation, 33, pp. 38-56; Vongbunyong, S., Vongseela, P., Sreerattana-aporn, J., A Process Demonstration Platform for Product Disassembly Skills Transfer (2017) Procedia CIRP, 61, pp. 281-286; Waschneck, B., Reichstaller, A., Belzner, L., Altenmuller, T., Bauernhansl, T., Knapp, A., Deep reinforcement learning for semiconductor production scheduling (2018) In 2018 29Th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC), Saratoga Springs, NY, USA, pp. 301-306. , https://doi.org/10.1109/ASMC.2018.8373191, 30.04.2018 - 03.05.2018, IEEE; (2019) A New Circular Vision for Electronics: Time for a Global Reboot., , http://www3.weforum.org/docs/WEF_A_New_Circular_Vision_for_Electronics.pdf, Accessed 12 February 2021; Harnessing the Fourth Industrial Revolution for the Circular Economy: Consumer Electronics and Plastics Packaging (2019) World Economic Forum, & Accenture Strategy, , http://www3.weforum.org/docs/WEF_Harnessing_4IR_Circular_Economy_report_2018.pdf, Accessed 3 February 2021; Wurster, M., Häfner, B., Gauder, D., Stricker, N., Lanza, G., Fluid Automation—A Definition and an Application in Remanufacturing Production Systems (2021) Procedia CIRP, 97, pp. 508-513; Zussman, E., Zhou, M., A methodology for modeling and adaptive planning of disassembly processes (1999) IEEE Transactions on Robotics and Automation, 15, pp. 190-194; Zussman, E., Zhou, M.C., Design and implementation of an adaptive process planner for disassembly processes (2000) IEEE Transactions on Robotics and Automation, 16, pp. 171-179},
correspondence_address1={Wurster, M.; wbk – Institute of Production Science, Kaiserstr. 12, Germany; email: marco.wurster@kit.edu},
publisher={Springer},
issn={09565515},
coden={JIMNE},
language={English},
abbrev_source_title={J Intell Manuf},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kosanoglu2022,
author={Kosanoglu, F. and Atmis, M. and Turan, H.H.},
title={A deep reinforcement learning assisted simulated annealing algorithm for a maintenance planning problem},
journal={Annals of Operations Research},
year={2022},
doi={10.1007/s10479-022-04612-8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126291625&doi=10.1007%2fs10479-022-04612-8&partnerID=40&md5=3098096d891a3cb0a677994b49dfb706},
affiliation={Department of Industrial Engineering, Yalova University, Yalova, Turkey; Capability Systems Centre, University of New South Wales, Canberra, Australia},
abstract={Maintenance planning aims to improve the reliability of assets, prevent the occurrence of asset failures, and reduce maintenance costs associated with downtime of assets and maintenance resources (such as spare parts and workforce). Thus, effective maintenance planning is instrumental in ensuring high asset availability with the minimum cost. Nevertheless, to find such optimal planning is a nontrivial task due to the (i) complex and usually nonlinear inter-relationship between different planning decisions (e.g., inventory level and workforce capacity), and (ii) stochastic nature of the system (e.g., random failures of parts installed in assets). To alleviate these challenges, we study a joint maintenance planning problem by considering several decisions simultaneously, including workforce planning, workforce training, and spare parts inventory management. We develop a hybrid solution algorithm (DRLSA) that is a combination of Double Deep Q-Network based Deep Reinforcement Learning (DRL) and Simulated Annealing (SA) algorithms. In each episode of the proposed algorithm, the best solution found by DRL is delivered to SA to be used as an initial solution, and the best solution of SA is delivered to DRL to be used as the initial state. Different from the traditional SA algorithms where neighborhood structures are selected only randomly, the DRL part of DRLSA learns to choose the best neighborhood structure to use based on experience gained from previous episodes. We compare the performance of the proposed solution algorithm with several well-known meta-heuristic algorithms, including Simulated Annealing, Genetic Algorithm (GA), and Variable Neighborhood Search (VNS). Further, we also develop a Machine Learning (ML) algorithm (i.e., K-Median) as another benchmark in which different properties of spare parts (e.g., failure rates, holding costs, and repair rates) are used as clustering features for the ML algorithm. Our study reveals that the DRLSA finds the optimal solutions for relatively small-size instances, and it has the potential to outperform traditional meta-heuristic and ML algorithms. © 2022, The Author(s).},
author_keywords={Deep reinforcement learning;  Double deep Q-network;  Inventory management;  Maintenance planning;  Simulated annealing;  Workforce planning and training},
references={Allen, T.T., Roychowdhury, S., Liu, E., Reward-based Monte Carlo-Bayesian reinforcement learning for cyber preventive maintenance (2018) Computers & Industrial Engineering, 126, pp. 578-594; Andriotis, C., Papakonstantinou, K., Managing engineering systems with large state and action spaces through deep reinforcement learning (2019) Reliability Engineering & System Safety, 191, p. 106483; (2018) Managing engineering systems with large state and action spaces through deep reinforcement learning, , CoRR, arXiv:1811.02052; Arsenault, R., (2016) Stat of The Week: The (Rising!) Cost of Downtime, , https://www.aberdeen.com/techpro-essentials/stat-of-the-week-the-rising-cost-of-downtime/, Accessed: 2021-03-07; (2017) Neural Combinatorial Optimization with Reinforcement Learning, , . arXiv:1611.09940; (2020) Machine Learning for Combinatorial Optimization: A Methodological Tour d’horizon. Arxiv, 1811, p. 06128; (2017) Deep reinforcement learning for multi-resource multi-machine job scheduling, , . arXiv preprint arXiv:1711.07440; (2019) ). Learning to perform local rewriting for combinatorial optimization, , . arXiv:1810.00337; Connolly, D.T., An improved annealing scheme for the QAP (1990) European Journal of Operational Research, 46, pp. 93-100; Learning heuristics for the TSP by policy gradient (2018) In International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research, pp. 170-181. , . Springer; Du, K.-L., Swamy, M.N.S., (2016) Simulated Annealing. Search and Optimization by Metaheuristics: Techniques and Algorithms Inspired by Nature, pp. 29-36. , https://doi.org/10.1007/978-3-319-41192-7_2, Cham, Springer International Publishing; Duan, L., Hu, H., Qian, Y., Gong, Y., Zhang, X., Xu, Y., Wei, J., (2019) A Multi-Task Selected Learning Approach for Solving 3D Flexible Bin Packing Problem. Arxiv, 1804, p. 06896; Emami, P., Ranka, S., (2018) Learning Permutations with Sinkhorn Policy Gradient; Etheve, M., Alès, Z., Bissuel, C., Juan, O., Kedad-Sidhoum, S., Reinforcement learning for variable selection in a branch and bound algorithm (2020) Lecture Notes in Computer Science, pp. 176-185; François-Lavet, V., Henderson, P., Islam, R., Bellemare, M.G., Pineau, J., An introduction to deep reinforcement learning. Foundations and Trends® (2018) Machine Learning, 11, pp. 219-354; Gama, R., Fernandes, H.L., (2020) A Reinforcement Learning Approach to the Orienteering Problem with Time Windows. Arxiv, 2011, p. 03647; Hicks, G., (2019) How Much is Equipment Downtime Costing Your Workplace?, pp. 2021-2103. , https://www.iofficecorp.com/blog/equipment-downtime.Accessed; Hoong Ong, K.S., Niyato, D., Yuen, C., Predictive maintenance for edge-based sensor networks: A deep reinforcement learning approach (2020) 2020 IEEE 6Th World Forum on Internet of Things (Wf-Iot, pp. 1-6. , https://doi.org/10.1109/WF-IoT48130.2020.9221098; Hu, H., Zhang, X., Yan, X., Wang, L., Xu, Y., (2017) Solving a New 3D Bin Packing Problem with Deep Reinforcement Learning Method. Arxiv, 1708, p. 05930; Hu, J., Niu, H., Carrasco, J., Lennox, B., Arvin, F., Voronoi-based multi-robot autonomous exploration in unknown environments via deep reinforcement learning (2020) IEEE Transactions on Vehicular Technology, 69, pp. 14413-14423; Huang, J., Chang, Q., Arinez, J., Deep reinforcement learning based preventive maintenance policy for serial production lines (2020) Expert Systems with Applications, 160, p. 113701; Hubbs, C.D., Li, C., Sahinidis, N.V., Grossmann, I.E., Wassick, J.M., A deep reinforcement learning approach for chemical production scheduling (2020) Computers & Chemical Engineering, 141, p. 106982; Jordan, W.C., Graves, S.C., Principles on the benefits of manufacturing process flexibility (1995) Management Science, 41, pp. 577-594; Kandel, I., Castelli, M., The effect of batch size on the generalizability of the convolutional neural networks on a histopathology dataset (2020) ICT Express, 6, pp. 312-315. , https://doi.org/10.1016/j.icte.2020.04.010https://www.sciencedirect.com/science/article/pii/S2405959519303455; Kingma, D.P., Ba, J., (2017) Adam: A Method for Stochastic Optimization., , arXiv:1412.6980; Kirkpatrick, S., Optimization by simulated annealing: Quantitative studies (1984) Journal of Statistical Physics, 34, pp. 975-986; Kirkpatrick, S., Gelatt, C.D., Vecchi, M.P., Optimization by simulated annealing (1983) Science, 220, pp. 671-680; Kool, W., van Hoof, H., Welling, M., (2019) Attention, learn to solve routing problems!; Kosanoglu, F., Turan, H.H., Atmis, M., A simulated annealing algorithm for integrated decisions on spare part inventories and cross-training policies in repairable inventory systems (2018) Proceedings of International Conference on Computers and Industrial Engineering, pp. 1-14; (2019) Reinforcement learning for pricing strategy optimization in the insurance industry. Engineering Applications of Artificial Intelligence, 80, pp. 8-19. , https://doi.org/10.1016/j.engappai.2019.01.010http://www.sciencedirect.com/science/article/pii/S0952197619300107; Levner, E., Perlman, Y., Cheng, T., Levner, I., A network approach to modeling the multi-echelon spare-part inventory system with backorders and interval-valued demand (2011) International Journal of Production Economics, 132, pp. 43-51; Li, Z., Zhong, S., Lin, L., An aero-engine life-cycle maintenance policy optimization algorithm: Reinforcement learning approach (2019) Chinese Journal of Aeronautics, 32, pp. 2133-2150; Liang, S., Yang, Z., Jin, F., Chen, Y., Data centers job scheduling with deep reinforcement learning (2020) Advances in Knowledge Discovery and Data Mining, pp. 906-917. , H. W. Lauw, R.C.-W. Wong, A. Ntoulas, E.-P. Lim, S.-K. Ng, S. J. Pan, Cham, Springer International Publishing; Lin, B., Ghaddar, B., Nathwani, J., (2020) Deep reinforcement learning for electric vehicle routing problem with time windows; Liu, C., Chang, C., Tseng, C., Actor-critic deep reinforcement learning for solving job shop scheduling problems (2020) IEEE Access, 8, pp. 71752-71762; Ma, Q., Ge, S., He, D., Thaker, D., Drori, I., (2019) ). Combinatorial optimization by graph pointer networks and hierarchical reinforcement learning., , arXiv:1911.04936; Mahmoodzadeh, Z., Wu, K.-Y., Droguett, E.L., Mosleh, A., Condition-based maintenance with reinforcement learning for dry gas pipeline subject to internal corrosion (2020) Sensors, 20, p. 5708; Mao, H., Alizadeh, M., Menache, I., Kandula, S., Resource management with deep reinforcement learning (2016) In Proceedings of the 15Th ACM Workshop on Hot Topics in Networks, pp. 50-56; Mazyavkina, N., Sviridov, S., Ivanov, S., Burnaev, E., (2020) Reinforcement Learning for Combinatorial Optimization: A Survey, , . arXiv:2003.03600; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Hassabis, D., Human-level control through deep reinforcement learning (2015) Nature, 518, pp. 529-533; Muckstadt, J.A., A model for a multi-item, multi-echelon, multi-indenture inventory system (1973) Management Science, 20, pp. 472-481; Muckstadt, J.A., (2005) Analysis and algorithms for service parts supply chains, , Springer Science & Business Media, Germany; Nazari, M., Oroojlooy, A., Snyder, L.V., Takáč, M., (2018) Reinforcement Learning for Solving the Vehicle Routing Problem, , . arXiv:1802.04240; ). Predictive maintenance for edge-based sensor networks: A deep reinforcement learning approach (2020) IEEE 6Th World Forum on Internet of Things (Wf-Iot, pp. 1-6; Petsagkourakis, P., Sandoval, I., Bradford, E., Zhang, D., del Rio-Chanona, E., Reinforcement learning for batch bioprocess optimization (2020) Computers & Chemical Engineering, 133, p. 106649. , http://www.sciencedirect.com/science/article/pii/S0098135419304168; Rahmati, S.H.A., Ahmadi, A., Govindan, K., A novel integrated condition-based maintenance and stochastic flexible job shop scheduling problem: simulation-based optimization approach (2018) Annals of Operations Research, 269, pp. 583-621; Rocchetta, R., Bellani, L., Compare, M., Zio, E., Patelli, E., A reinforcement learning framework for optimal operation and maintenance of power grids (2019) Applied Energy, 241, pp. 291-301; Salari, N., Makis, V., Joint maintenance and just-in-time spare parts provisioning policy for a multi-unit production system (2020) Annals of Operations Research, 287, pp. 351-377; Samouei, P., Kheirkhah, A.S., Fattahi, P., A network approach modeling of multi-echelon spare-part inventory system with backorders and quantity discount (2015) Annals of Operations Research, 226, pp. 551-563; Sherbrooke, C.C., Metric: A multi-echelon technique for recoverable item control (1968) Operations Research, 16, pp. 122-141; Sherbrooke, C.C., VARI-METRIC: Improved approximations for multi-indenture, multi-echelon availability models (1986) Operations Research, 34, pp. 311-319; Skordilis, E., Moghaddass, R., A deep reinforcement learning approach for real-time sensor-driven decision making and predictive analytics (2020) Computers & Industrial Engineering, 147, p. 106600; Sleptchenko, A., Hanbali, A.A., Zijm, H., Joint planning of service engineers and spare parts (2018) European Journal of Operational Research, 271, pp. 97-108; Sleptchenko, A., van der Heijden, M., Joint optimization of redundancy level and spare part inventories (2016) Reliability Engineering & System Safety, 153, pp. 64-74; Sleptchenko, A., Turan, H.H., Pokharel, S., ElMekkawy, T.Y., Cross-training policies for repair shops with spare part inventories (2019) International Journal of Production Economics, 209, pp. 334-345; Suman, B., Kumar, P., A survey of simulated annealing as a tool for single and multiobjective optimization (2006) Journal of the Operational Research Society, 57, pp. 1143-1160; Tang, Y., Agrawal, S., Faenza, Y., (2020) Reinforcement Learning for Integer Programming: Learning to Cut. Arxiv, 1906, p. 04859; Turan, H.H., Atmis, M., Kosanoglu, F., Elsawah, S., Ryan, M.J., A risk-averse simulation-based approach for a joint optimization of workforce capacity, spare part stocks and scheduling priorities in maintenance planning (2020) Reliability Engineering & System Safety, 204, p. 107199; Turan, H.H., Kosanoglu, F., Atmis, M., A multi-skilled workforce optimisation in maintenance logistics networks by multi-thread simulated annealing algorithms (2020) International Journal of Production Research, pp. 1-23. , https://doi.org/10.1080/00207543.2020.1735665, (b); Turan, H.H., Sleptchenko, A., Pokharel, S., ElMekkawy, T.Y., A clustering-based repair shop design for repairable spare part supply systems (2018) Computers & Industrial Engineering, 125, pp. 232-244; Turan, H.H., Sleptchenko, A., Pokharel, S., ElMekkawy, T.Y., A sorting based efficient heuristic for pooled repair shop designs (2020) Computers & Operations Research, 117, p. 104887; Van Harten, A., Sleptchenko, A., On Markovian multi-class, multi-server queueing (2003) Queueing systems, 43, pp. 307-328; Wang, Y., Tang, J., Optimized skill configuration for the seru production system under an uncertain demand (2020) Annals of Operations Research, pp. 1-21; Waschneck, B., Reichstaller, A., Belzner, L., Altenmüller, T., Bauernhansl, T., Knapp, A., Kyek, A., Deep reinforcement learning for semiconductor production scheduling (2018) 2018 29Th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC), pp. 301-306. , https://doi.org/10.1109/ASMC.2018.8373191; Waschneck, B., Reichstaller, A., Belzner, L., Altenmüller, T., Bauernhansl, T., Knapp, A., Kyek, A., Optimization of global production scheduling with deep reinforcement learning (2018) Procedia CIRP, 72, pp. 1264-1269; Watkins, C.J.C.H., Dayan, P., Q-learning (1992) Machine Learning, 8, pp. 279-292; Wei, S., Bao, Y., Li, H., Optimal policy for structure maintenance: A deep reinforcement learning framework (2020) Structural Safety, 83, p. 101906; Wu, Y., Liu, L., Bae, J., Chow, K.-H., Iyengar, A., Pu, C., Wei, W., Zhang, Q., (2019) Demystifying learning rate policies for high accuracy training of deep neural networks; Yu, J.J.Q., Yu, W., Gu, J., Online vehicle routing with neural combinatorial optimization and deep reinforcement learning (2019) IEEE Transactions on Intelligent Transportation Systems, 20, pp. 3806-3817; Zhang, C., Gupta, C., Farahat, A., Ristovski, K., Ghosh, D., Equipment health indicator learning using deep reinforcement learning (2019) Machine Learning and Knowledge Discovery in Databases, pp. 488-504. , Brefeld U, Curry E, Daly E, MacNamee B, Marascu A, Pinelli F, Berlingerio M, Hurley N, (eds), Springer International Publishing, Cham; Zhang, N., Si, W., Deep reinforcement learning for condition-based maintenance planning of multi-component systems under dependent competing risks (2020) Reliability Engineering & System Safety, 203, p. 107094; Zhao, J., Mao, M., Zhao, X., Zou, J., A hybrid of deep reinforcement learning and local search for the vehicle routing problems (2020) IEEE Transactions on Intelligent Transportation Systems, pp. 1-11. , https://doi.org/10.1109/TITS.2020.3003163},
correspondence_address1={Turan, H.H.; Capability Systems Centre, Australia; email: h.turan@adfa.edu.au},
publisher={Springer},
issn={02545330},
language={English},
abbrev_source_title={Ann. Oper. Res.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dreher2022,
author={Dreher, A. and Bexten, T. and Sieker, T. and Lehna, M. and Schütt, J. and Scholz, C. and Wirsum, M.},
title={AI agents envisioning the future: Forecast-based operation of renewable energy storage systems using hydrogen with Deep Reinforcement Learning},
journal={Energy Conversion and Management},
year={2022},
doi={10.1016/j.enconman.2022.115401},
art_number={115401},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126278256&doi=10.1016%2fj.enconman.2022.115401&partnerID=40&md5=ebc1f70ea2407f65fd9e99a212f077b4},
affiliation={Fraunhofer Institute for Energy Economics and Energy System Technology, Department Energy Informatics and Information Systems, Joseph-Beuys-Straße 8, Kassel, 34117, Germany; RWTH Aachen University, Institute of Power Plant Technology, Steam and Gas Turbines, Mathieustraße 9, Aachen, 52074, Germany},
abstract={Hydrogen-based energy storage has the potential to compensate for the volatility of renewable power generation in energy systems with a high renewable penetration. The operation of these storage facilities can be optimized using automated energy management systems. This work presents a Reinforcement Learning-based energy management approach in the context of CO2-neutral hydrogen production and storage for an industrial combined heat and power application. The economic performance of the presented approach is compared to a rule-based energy management strategy as a lower benchmark and a Dynamic Programming-based unit commitment as an upper benchmark. The comparative analysis highlights both the potential benefits and drawbacks of the implemented Reinforcement Learning approach. The simulation results indicate a promising potential of Reinforcement Learning-based algorithms for hydrogen production planning, outperforming the lower benchmark. Furthermore, a novel approach in the scientific literature demonstrates that including energy and price forecasts in the Reinforcement Learning observation space significantly improves optimization results and allows the algorithm to take variable prices into account. An unresolved challenge, however, is balancing multiple conflicting objectives in a setting with few degrees of freedom. As a result, no parameterization of the reward function could be found that fully satisfied all predefined targets, highlighting one of the major challenges for Reinforcement Learning -based energy management algorithms to overcome. © 2022 Fraunhofer Institute for Energy Economics and Energy System Technology},
author_keywords={Deep reinforcement learning;  Dynamic programming;  Energy management;  Hydrogen;  Renewable energy storage},
keywords={Benchmarking;  Degrees of freedom (mechanics);  Dynamic programming;  Energy management;  Energy management systems;  Hydrogen production;  Hydrogen storage;  Production control;  Reinforcement learning;  Renewable energy resources, Combined heat and power applications;  Economic performance;  Energy systems;  Neutral hydrogen;  Reinforcement learnings;  Renewable energy storages;  Renewable power generation;  Rule based;  Storage facilities;  Storage systems, Deep learning},
funding_details={511/17.001},
funding_details={RWTH Aachen UniversityRWTH Aachen University},
funding_text 1={This work was funded by the Hessian Ministry of Higher Education, Research, Science and the Arts through the Competence Center for Cognitive Energy Systems (K-ES) project under reference number: 511/17.001. The methods utilized in this study were partially developed within the research project “Future Municipal Energy Supply Systems“ supported by the RWTH Aachen University Strategy Fund.},
references={(2021), https://www.iea.org/data-and-statistics, International Energy Agency. Data and statistics. (online) (accessed 2021/06/21); Schellnhuber, H.J., Rahmstorf, S., Winkelmann, R., Why the right climate target was agreed in Paris (2016) Nat Clim Change, 6 (7), pp. 649-653; International Energy Agency, Net Zero by 2020 (2021), A Roadmap for the Global Energy Sector Report International Energy Agency; Holttinen, H., Impact of Hourly Wind Power Variations on the System Operation in the Nordic Countries (2005) Wind Energy, 8 (2), pp. 197-218; Stolten, D., Emonts, B., (2016) Hydrogen Science and Engineering, 1. , Wiley-VCH Verlag Weinheim, Germany; Robinius, M., Otto, A., Heuser, P., Welder, L., Syranidis, K., Ryberg, D., Linking the Power and Transport Sectors - Part 1: The Principle of Sector Coupling (2017) Energies, 10 (7), p. 956; (2020), European Commission A european green deal: Striving to be the first climate-neutral continent; (2020), European Commission A hydrogen strategy for a climate-neutral europe; (2021), German Federal Ministry for Economic Affairs and Energy Entwurf eines Gesetzes zur Umsetzung unionsrechtlicher Vorgaben und zur Regelung reiner Wasserstoffnetze im Energiewirtschaftsrecht: Energiewirtschaftsrechtsänderungsgesetz (energy industry law amendment act); Meng, L., Sanseverino, E.R., Luna, A., Dragicevic, T., Vasquez, J.C., Guerrero, J.M., Microgrid supervisory controllers and energy management systems: A literature review (2016) Renew Sustain Energy Rev, 60, pp. 1263-1273; Zia, M.F., Elbouchikhi, E., Benbouzid, M., Microgrids energy management systems: A critical review on methods, solutions, and prospects (2018) Appl Energy, 222, pp. 1033-1055; Lee, D., Cheng, C.-C., Energy savings by energy management systems: A review (2016) Renew Sustain Energy Rev, 56, pp. 760-777; Shareef, H., Ahmed, M.S., Mohamed, A., Al Hassan, E., Review on home energy management system considering demand responses, smart technologies, and intelligent controllers (2018) IEEE Access, 6, pp. 24498-24509; Rouzbahani, H.M., Karimipour, H., Lei, L., A review on virtual power plant for energy management (2021) Sustainable Energy Technol Assess, 47, p. 101370; Karavas, C.-S., Kyriakarakos, G., Arvanitis, K.G., Papadakis, G., A multi-agent decentralized energy management system based on distributed intelligence for the design and control of autonomous polygeneration microgrids (2015) Energy Convers Manage, 103, pp. 166-179; García Vera, Y.E., Dufo-López, R., Bernal-Agustín, J.L., Energy management in microgrids with renewable energy sources: A literature review (2019) Appl Sci, 9 (18), p. 3854; Marchand, S., Richter, L., Scholz, C., Dreher, A., Lehna, M., Lenk, S., (2021), Artificial intelligence for energy supply chain automation. in review; Lechner, C., Seume, J., (2010) Stationäre Gasturbinen, 2. , Springer-Verlag Berlin Heidelberg, Germany; Global, E.T.N., Hydrogen Gas Turbines (2020), Technical report ETN Global, Brussels, Belgium; Bexten, T., Sieker, T., Wirsum, M., Techno-economic Analysis of a Hydrogen Production and Storage System for the On-site Fuel Supply of Hydrogen-fired Gas Turbines (2021) J Eng Gas Turbine Power, 143 (12), p. 121020; Wang, A., Van der Leun, K., Peters, D., Buseman, M., European Hydrogen Backbone (2020), Technical report Guidehouse, Utrecht, The Netherlands; (2021), https://cordis.europa.eu/project/id/884229, European Commission. HYdrogen as a FLEXible energy storage for a fully renewable European POWER system. (online) (accessed 2021/06/22; Bexten, T., Wirsum, M., Roscher, B., Schelenz, R., Jacobs, G., Model-based analysis of a combined heat and power system featuring a hydrogen-fired gas turbine with on-site hydrogen production and storage (2021) J Eng Gas Turbines Power, 143 (8); Xia, J., Zhao, P., Dai, Y., Operation and simulation of hybrid wind and gas turbine power system employing wind power forecasting (2012), American Society of Mechanical Engineers; Branchini, L., Bianchi, M., Cavina, N., Cerofolini, A., De Pascale, A., Melino, F., Wind-hydro-gas turbine unit commitment to guarantee firm dispatchable power (2014) Proceedings of the ASME Turbo Expo 2014: Turbomachinery Technical Conference and Exposition, volume 3B: Oil and Gas Applications Organic Rankine Cycle Power Systems Supercritical CO2 Power Cycles Wind Energy, , American Society of Mechanical Engineers; Ebaid, M.S.Y., Hammad, M., Alghamdi, T., Thermo-economic analysis of PV and hydrogen gas turbine hybrid power plant of 100 MW power output (2015) Int J Hydrogen Energy, 40 (36), pp. 12120-12143; Colbertaldo, P., Guandalini, G., Crespi, E., Campanari, S., (2020), 6. , 2020, Balancing a High-Renewables Electric Grid With Hydrogen-Fuelled Combined Cycles: A Country Scale Analysis. In: Proceedings of the ASME Turbo Expo 2020: Turbomachinery Technical Conference and Exposition, Education; Electric Power. Virtual, Online. September 21-25 page V006T09A006. ASME; Bexten, T., Wirsum, M., Roscher, B., Schelenz, R., Jacobs, G., Weintraub, D., Optimal operation of a gas turbine cogeneration unit with energy storage for wind power system integration (2018) J Eng Gas Turbines Power, 141 (1); Sutton, R.S., Barto, A.G., (2018), Reinforcement learning: An introduction. Adaptive computation and machine learning series. The MIT Press, Cambridge Massachusetts, second edition; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., (2013), Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Human-level control through deep reinforcement learning (2015) Nature, 518 (7540), pp. 529-533; Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., A general reinforcement learning algorithm that masters chess, shogi, and go through self-play (2018) Science (New York, N.Y.), 362 (6419), pp. 1140-1144; Silverm, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van den Driessche, G., Mastering the game of go with deep neural networks and tree search (2016) Nature, 529 (7587), pp. 484-489; Zhang, D., Han, X., Deng, C., Review on the research and practice of deep learning and reinforcement learning in smart grids (2018) CSEE J Power Energy Syst, 4 (3), pp. 362-370; Ye, Y., Qiu, D., Wu, X., Strbac, G., Ward, J., Model-free real-time autonomous control for a residential multi-energy system using deep reinforcement learning (2020) IEEE Trans Smart Grid, 11 (4), pp. 3068-3082; Mbuwir, B., Ruelens, F., Spiessens, F., Deconinck, G., 2017, Battery energy management in a microgrid using batch reinforcement learning (1846) Energies, 10 (11); Lu, R., Hong, S.H., Yu, M., Demand response for home energy management using reinforcement learning and artificial neural network (2019) IEEE Trans Smart Grid, 10 (6), pp. 6629-6639; Huang, X., Hong, S.H., Yu, M., Ding, Y., Jiang, J., Demand response management for industrial facilities: A deep reinforcement learning approach (2019) IEEE Access, 7, pp. 82194-82205; Lin, L., Guan, X., Peng, Y., Wang, N., Maharjan, S., Ohtsuki, T., Deep reinforcement learning for economic dispatch of virtual power plant in internet of energy (2020) IEEE Internet Things J, 7 (7), pp. 6288-6301; Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., Kavukcuoglu, K., (2016), Asynchronous methods for deep reinforcement learning. In: International conference on machine learning, pages 1928–1937. PMLR; Zhou, S., Hu, Z., Gu, W., Jiang, M., Chen, M., Hong, Q., Combined heat and power system intelligent economic dispatch: A deep reinforcement learning approach (2020) Int J Electr Power Energy Syst, 120, p. 106016; François-Lavet, V., Taralla, D., Ernst, D., Fonteneau, R., 2016 (2016), Deep reinforcement learning solutions for energy microgrids management European Workshop on Reinforcement Learning (EWRL; Tomin, N., Zhukov, A., Domyshev, A., (2019), 217. , Deep reinforcement learning for energy microgrids management considering flexible energy sources. In EPJ Web of Conferences, page 01016. EDP Sciences; Nyong-Bassey, B.E., Giaouris, D., Patsios, C., Papadopoulou, S., Papadopoulos, A.I., Walker, S., Reinforcement learning based adaptive power pinch analysis for energy management of stand-alone hybrid energy storage systems considering uncertainty (2020) Energy, 193, p. 116622; Bellman, R., Dynamic programming (1966) Science, 153 (3731), pp. 34-37; Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O., (2017), Proximal policy optimization algorithms; Kingma, D.P., Adam, J.B., (2014), A method for stochastic optimization. arXiv preprint arXiv:1412.6980; (2020), https://cdc.dwd.de, Deutscher Wetterdienst CDC-Climate Data Center. (online) (accessed 2020/10/12); Bexten, T., Wirsum, M., Roscher, B., Schelenz, R., Jacobs, G., Weintraub, D., Jeschke, P., (2017), 9. , 2017 Techno-Economic Study of Wind Farm Forecast Error Compensation by Flexible Heat-Driven CHP Units. In Proceedings of the ASME Turbo Expo 2017: Turbomachinery Technical Conference and Exposition, Oil and Gas Applications; Supercritical CO2 Power Cycles; Wind Energy. Charlotte, North Carolina, USA. June 26-30 page V009T49A004. ASME; (2016), ENERCON GmbH ENERCON Produktübersicht. Technical report, ENERCON GmbH, Aurich, Germany; Ruangpattana, S., Klabjan, D., Arinez, J., Biller, S., Optimization of on-site renewable energy generation for industrial sites (2011) 2011 IEEE/PES Power Systems Conference and Exposition. IEEE; Giampieri, A., Ling-Chin, J., Ma, Z., Smallbone, A., Roskilly, A.P., A review of the current automotive manufacturing practice from an energy perspective (2020) Appl Energy, 261, p. 114074; SIEMENS AG, SGT-300 Industrial Gas Turbine (2015), Technical report SIEMENS AG, Munich, Germany; Vetter, G., Leckfreie Pumpen (1998), Vulkan-Verlag, Essen, Germany Verdichter und Vakuumpumpen; Bludszuweit, H., Dominguez-Navarro, J.-A., Llombart, A., Statistical analysis of wind power forecast error (2008) IEEE Trans Power Syst, 23 (3), pp. 983-991; Icha, P., Kuhs, G., Entwicklung der spezifischen Kohlendioxid-Emissionen des deutschen Strommix in den Jahren 1990–2019 (2020), Technical report Umweltbundesamt, Dessau-Roßlau, Germany; Sundstrom, O., Ambühl, D., Guzzella, L., On implementation of dynamic programming for optimal control problems with final state constraints (2009) Oil & Gas Science and Technology Revue de l'Institut Français du Pétrole, 65 (1), pp. 91-102; Sundstrom, O., Guzzella, L., A generic dynamic programming matlab function (2009) 2009 IEEE International Conference on Control Applications. IEEE; Bexten, T., Jörg, S., Petersen, N., Wirsum, M., Liu, P., Li, Z., Model-based thermodynamic analysis of a hydrogen-fired gas turbine with external exhaust gas recirculation (2021) J Eng Gas Turbines Power, 143 (8); Kuznetsova, E., Li, Y.-F., Ruiz, C., Zio, E., Ault, G., Bell, K., Reinforcement learning for microgrid energy management (2013) Energy, 59, pp. 133-146; Buttler, A., Spliethoff, H., Current status of water electrolysis for energy storage, grid balancing and sector coupling via power-to-gas and power-to liquids: A review (2018) Renewable Sustain Energy Rev, 82; Preuster, P., Alekseev, A., Wasserscheid, P., Hydrogen Storage Technologies for Future Energy Systems (2017) Ann Rev Chem Biomol Eng, 8 (1); Funke, H.H.-W., Beckmann, N., Keinz, J., Horikawa, A., 30 years of dry-low-NOx micromix combustor research for hydrogen-rich fuels -an overview of past and present activities (2021) J Eng Gas Turbines Power, 143 (7)},
correspondence_address1={Dreher, A.; Fraunhofer Institute for Energy Economics and Energy System Technology, Joseph-Beuys-Straße 8, Germany; email: alexander.dreher@iee.fraunhofer.de},
publisher={Elsevier Ltd},
issn={01968904},
coden={ECMAD},
language={English},
abbrev_source_title={Energy Convers. Manage.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dasbach2022334,
author={Dasbach, T. and Olbort, J. and Wenk, F. and Ander, R.},
title={Sequencing Through a Global Decision Instance Based on a Neural Network},
journal={IFIP Advances in Information and Communication Technology},
year={2022},
volume={639 IFIP},
pages={334-344},
doi={10.1007/978-3-030-94335-6_24},
note={cited By 0; Conference of 18th IFIP WG 5.1 International Conference on Product Lifecycle Management, PLM 2021 ; Conference Date: 11 July 2021 Through 14 July 2021;  Conference Code:272129},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125236282&doi=10.1007%2f978-3-030-94335-6_24&partnerID=40&md5=e56244d8e9505c720cc630281c33bdb9},
affiliation={TU Darmstadt, DiK, Otto-Berndt-Straße 2, Darmstadt, Germany},
abstract={An existing concept for sequence planning in production planning and control was extended by a global decision instance based on neural networks. Therefore, information regarding the state of the production and available orders were normalized and analyzed by one agent. In contrast to a partially observable Markow Decision Problem one single agent was allowed and used to process all available information. Feasibility and problems were examined and compared with a concept for decentralized decisions. The implementation consists of two parts, which continuously interact with each other. One part is a simulation of a job shop, including multiple machines. The other parts tackle the Markow Decision Problem with the use of double Q reinforcement learning in order to estimate the best sequence at any given time. Later, problems due to scaling and comparisons to the usage of multiple agents are given. © 2022, IFIP International Federation for Information Processing.},
author_keywords={Artificial intelligence;  Neural network;  Sequence planning},
keywords={Production control;  Reinforcement learning, Decentralised;  Decision problems;  Job-shop;  Multiple machine;  Neural-networks;  One parts;  Production planning and control;  Scalings;  Sequence planning;  Single-agent, Multi agent systems},
references={(2015) Produktionsplanung Und –steuerung, , https://doi.org/10.1007/978-3-662-43542-7, Claus, T., Herrmann, F., Manitz, M. (eds.), Springer, Heidelberg; Jaehn, F., Pesch, E., (2014) Ablaufplanung. Einführung in Scheduling, , https://doi.org/10.1007/978-3-642-54439-2, Springer, Heidelberg; Wuendahl, H.-P., (2010) Betriebsorganisation für Ingenieure, , Hanser, München; Swamidass, P.M., (2000) Encyclopedia of Production and Manufacturing Management, , Kluwer Academic Publishers, Boston; Lödding, H., (2016) Verfahren Der Fertigungssteuerung, , https://doi.org/10.1007/978-3-662-48459-3, Springer, Heidelberg; LNCS Homepage. http://www.springer.com/lncs. Accessed 21 Nov 2016; Lillicrap, T., Continuous control with deep reinforcement learning (2016) On ICLR; Thomas, A., (2020), https://adventuresinmachinelearning.com/double-q-reinforcement-learning-in-tensorflow-2/. Accessed 29, Mar; Zhou, H., (2019) Concept for Optimization of Scheduling in Production Planning and Control Using Machine Learning; (2006) ICIC 2006. LNCS (LNAI), 4114. , https://doi.org/10.1007/11816171, Huang, D.-S., Li, K., Irwin, G.W. (eds.), vol. , Springer, Heidelberg; Sutton, R.S., Barto, A.G., (2018) Reinforcement Learning, , MIT Press, Cambridge; Rosenblatt, F., The perceptron: A probabilistic model for information storage and organization in the brain (1958) Psychol. Rev., 65 (6), pp. 386-408},
correspondence_address1={Dasbach, T.; TU Darmstadt, Otto-Berndt-Straße 2, Germany; email: dasbach@dik.tu-darmstadt.de},
editor={Canciglieri Junior O., Noel F., Rivest L., Bouras A.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18684238},
isbn={9783030943349},
language={English},
abbrev_source_title={IFIP Advances in Information and Communication Technology},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Feng202218910,
author={Feng, M. and Li, Y.},
title={Predictive Maintenance Decision Making Based on Reinforcement Learning in Multistage Production Systems},
journal={IEEE Access},
year={2022},
volume={10},
pages={18910-18921},
doi={10.1109/ACCESS.2022.3151170},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124743585&doi=10.1109%2fACCESS.2022.3151170&partnerID=40&md5=2111dd1e4221f2ed3a73062ee89d285e},
affiliation={School of Foreign Language Studies, Chang'An University, Shaanxi, China; Department of Industrial Engineering, School of Mechanical Engineering, Northwestern Polytechnical University, Shaanxi, China},
abstract={Predictive maintenance has become increasingly prevalent in modern production systems that are challenged by high-mix low-volume production and short production life cycle. It is very helpful to prevent costly equipment failures, and reduce significant production loss caused by unscheduled machine breakdown. Although important, decision models for joint predictive maintenance and production in manufacturing systems have not been fully explored. Therefore, we propose a reinforcement learning based decision model, that brings together production system modeling and approximate dynamic programming. We start from the development of a state-based model by analyzing the dynamics of a multistage production system with predictive maintenance. It provides an approach to quantitatively evaluate the various disruptions as well as the maintenance decision's impact on production. Then a reinforcement learning method is proposed to explore optimal maintenance policies, that optimize the production and maintenance cost. To further improve the performance of the production system, machine stoppage bottlenecks are defined. An event-based indicator is proved to identify bottlenecks with production data. We test the proposed models in simulation case studies. The proposed predictive maintenance decision model is compared with three policies, which are state-based policy (SBP), time-based policy (TBP) and greedy policy (GP). The numerical studies show that the proposed decision model outperforms the policies, and it has the lowest system cost that is 9.68%, 39.07%, and 39.56% lower than SBP, TBP, and GP, respectively. In addition, the research shows that bottleneck identification and mitigation could help manufacturing systems to achieve more than 9.00% throughput improvement. © 2013 IEEE.},
author_keywords={approximate dynamic programming;  bottleneck;  decision making;  Markov chain model;  predictive maintenance;  Production system analysis},
keywords={Costs;  Dynamic programming;  Industrial research;  Life cycle;  Maintenance;  Manufacture;  Markov processes;  Reinforcement learning, Approximate dynamic programming;  Bottleneck;  Decision modeling;  Decisions makings;  Maintenance decisions;  Markov chain models;  Multi-stage production systems;  Predictive maintenance;  Production system;  Production system analyse, Decision making},
references={Li, Y., Chang, Q., Xiao, G., Biller, S., Event-based modeling of distributed sensor networks in battery manufacturing (2013) Int. J. Prod. Res, 52 (14), pp. 4239-4252; Takata, S., Kirnura, F., Houten, F.J.V., Westkamper, E., Shpitalni, M., Ceglarek, D., Lee, J., Maintenance: Changing role in life cycle management (2004) CIRP Ann. Manuf. Technol, 53 (2), pp. 643-655; Raza, A., Ulansky, V., Modelling of predictive maintenance for a periodically inspected system (2017) Proc. CIRP, 59, pp. 95-101. , Jan; Ahmad, R., Kamaruddin, S., An overview of time-based and conditionbased maintenance in industrial application (2012) Comput. Ind. Eng, 63 (1), pp. 135-149; Jardine, A.K.S., Lin, D., Banjevic, D., A review on machinery diagnostics and prognostics implementing condition-based maintenance (2006) Mech. Syst. Signal Process, 20 (7), pp. 1483-1510. , Oct; Auschitzky, E., Hammer, M., Rajagopaul, A., (2014) Howbig Data Can Improve Manufacturing, p. 822. , http://www.mckinsey.com/insights/operations/how_big_data_can_improve_manufacturing, McKinsey Company, New York, NY, USA; Horenbeek, A.V., Liliane, P., A dynamic predictive maintenance policy for complex multi-component systems (2013) Rel. Eng. Syst. Saf, 120, pp. 39-51. , Dec; Wong, C.S., Chan, F.T.S., Chung, S.H., A joint production scheduling approach considering multiple resources and preventive maintenance tasks (2013) Int. J. Prod. Res, 51 (3), pp. 883-896. , Feb; Najid, N.M., Alaoui-Selsouli, M., Mohad, A., An integrated production and maintenance planning model with time Windows and shortage cost (2011) Int. J. Prod. Res, 49 (8), pp. 2265-2283. , Apr; Zhang, P., Zhu, X., Xie, M., A model-based reinforcement learning approach for maintenance optimization of degrading systems in a large state space (2021) Comput. Ind. Eng, 161. , Nov, Art 107622; Yang, H., Li, W., Wang, B., Joint optimization of preventive maintenance and production scheduling for multi-state production systems based on reinforcement learning Rel. Eng. Syst. Saf, 214. , Oct 2021, Art 107713; Chen, Y., Liu, Y., Xiahou, T., A deep reinforcement learning approach to dynamic loading strategy of repairable multistate systems IEEE Trans. Rel., Early Access, 25, p. 2021. , Jan; Peng, S., Feng, Q., Reinforcement learning with Gaussian processes for condition-based maintenance Comput. Ind. Eng, 158. , Aug 2021, Art 107321; Li, Y., Tang, Q., Chang, Q., Brundage, M.P., An event-based analysis of condition-based maintenance decision-making in multistage production systems (2017) Int. J. Prod. Res, 55 (16), pp. 4753-4764. , Aug; Iravani, S.M.R., Duenyas, I., Integrated maintenance and production control of a deteriorating production system (2002) IIE Trans, 34 (5), pp. 423-435. , May; Xia, T., Xi, L., Zhou, X., Lee, J., Condition-based maintenance for intelligent monitored series system with independent machine failure modes (2013) Int. J. Prod. Res, 51 (15), pp. 4585-4596. , Aug; Chang, Q., Ni, J., Bandyopadhyay, P., Biller, S., Xiao, G., Maintenance opportunity planning system (2007) J. Manuf. Sci. Eng, 129 (3), pp. 661-668. , Jun; Cui, P.-H., Wang, J.-Q., Li, Y., Data-driven modelling, analysis and improvement of multistage production systems with predictive maintenance and product quality Int. J. Prod. Res, 2021 (2021), pp. 1-18. , Sep; Zhang, L., Wang, C., Arinez, J., Biller, S., Transient analysis of Bernoulli serial lines: Performance evaluation and system-Theoretic properties (2013) IIE Trans, 45 (5), pp. 528-543; Li, Y., Chang, Q., Brundage, M.P., Biller, S., Arinez, J., Xiao, G., Market demand oriented data-driven modeling for dynamic manufacturing system control (2014) IEEE Trans. Syst., Man, Cybern., Syst, 45 (1), pp. 109-121. , Jan; Li, J., Performance analysis of manufacturing systems with rework loops (2004) IIE Trans, 36 (8), pp. 755-765; Meerkov, S.M., Zhang, L., Product quality inspection in Bernoulli lines: Analysis, bottlenecks, and design (2010) Int. J. Prod. Res, 48 (16), pp. 4745-4766; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Hassabis, D., Human-level control through deep reinforcement learning (2014) Nature, 518, pp. 529-533. , Feb; Lecun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; McDonnell, P., Joshi, S., Qiu, R.G., A learning approach to enhancing machine reconguration decision-making games in a heterarchical manufacturing environment (2005) Int. J. Prod. Res, 43 (20), pp. 4321-4334. , Oct; Csáji, B.C., Monostori, L., Kádár, B., Reinforcement learning in a distributed market-based production control system (2006) Adv. Eng. Informat, 20 (3), pp. 279-288. , Jul; Tesauro, G., Temporal difference learning and TD-Gammon (1995) Commun ACM, 38 (3), pp. 58-68. , Mar; Shahrabi, J., Adibi, M.A., Mahootchi, M., A reinforcement learning approach to parameter estimation in dynamic job shop scheduling (2017) Com-put. Ind. Eng, 110, pp. 75-82. , Aug; Schmidhuber, J., Deep learning in neural networks: An overview (2014) Neural Netw, 61, pp. 85-117. , Jan; Qing, C., Xiao, G., Biller, S., Li, L., Energy saving opportunity analysis of automotive serial production systems (2013) IEEE Trans. Autom. Sci. Eng, 10 (2), pp. 334-342. , Apr; Li, Y., Chang, Q., Ni, J., Brundage, M., Event-based supervisory control for energy efficient manufacturing systems (2018) IEEE Trans. Automat. Sci. Eng, 15 (1), pp. 92-103. , May},
correspondence_address1={Feng, M.; School of Foreign Language Studies, China; email: mfeng7@chd.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yousefi202216,
author={Yousefi, N. and Tsianikas, S. and Coit, D.W.},
title={Dynamic maintenance model for a repairable multi-component system using deep reinforcement learning},
journal={Quality Engineering},
year={2022},
volume={34},
number={1},
pages={16-35},
doi={10.1080/08982112.2021.1977950},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115334123&doi=10.1080%2f08982112.2021.1977950&partnerID=40&md5=eff853c19b3c20f029833d5402bce804},
affiliation={Department of Industrial & System Engineering, Rutgers University, Piscataway, NJ, United States; Department of Industrial Engineering, Tsinghua University, Beijing, China},
abstract={Using artificial intelligence for maintenance planning is useful for many industries to have a smart decision-making tool that delivers the best maintenance policy to minimize the expected maintenance costs. In this paper, a deep reinforcement learning method is used to provide a new dynamic maintenance model for a degrading repairable system subject to degradation and random shock. At any time, the degradation level of the system can be considered as the state of the system, and based on the available actions, it transits to different levels. The gamma process is used to formulate the degradation form of the system. The maintenance problem is formulated as a Markov decision process, and Deep Q learning algorithm is used to solve the problem. For most of the models in the literature, the degradation state of the system must be discretized. However, discretization of the degradation states brings inaccuracy and inefficiency to the model. In this paper, instead of discretizing the degradation state, we consider the exact level of degradation as the state of the system. The Deep Q learning method tries to recognize patterns instead of mapping every state to its best action. A neural network is trained during the learning process of the algorithm, and it can be used as a decision-making tool for the maintenance team to find the best maintenance action based on the current degradation level of the system. A numerical example illustrates how the deep reinforcement learning algorithm can be applied to find the optimal maintenance action at each degradation level. © 2021 Taylor & Francis Group, LLC.},
author_keywords={Deep Q network;  Deep reinforcement learning;  dynamic maintenance;  gamma process;  Markov decision process},
keywords={Behavioral research;  Decision making;  Learning algorithms;  Maintenance;  Markov processes;  Planning;  Reinforcement learning, Decision making tool;  Deep Q network;  Degradation state;  Dynamic maintenances;  Gamma process;  Maintenance Action;  Maintenance cost;  Maintenance models;  Maintenance planning;  Markov Decision Processes, Deep learning},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 71731008},
funding_text 1={The third author?s work was partially supported by the National Natural Science Foundation of China under key project grant 71731008.},
references={Bian, L., Gebraeel, N., Stochastic modeling and real-time prognostics for multi-component systems with degradation rate interactions (2014) IIE Transactions, 46 (5), pp. 470-482; Chen, N., Ye, Z.-S., Xiang, Y., Zhang, L., Condition-based maintenance using the inverse Gaussian degradation model (2015) European Journal of Operational Research, 243 (1), pp. 190-199; Correa-Jullian, C., Droguett, E.L., Cardemil, J.M., Operation scheduling in a solar thermal system: A reinforcement learning-based framework (2020) Applied Energy, 268, p. 114943; Dehghani, N.L., Darestani, Y.M., Shafieezadeh, A., Optimal life-cycle resilience enhancement of aging power distribution systems: A MINLP-based preventive maintenance planning (2020) IEEE Access, 8, pp. 22324-22334; Dong, W., Liu, S., Cao, Y., Javed, S.A., Du, Y., Reliability modeling and optimal random preventive maintenance policy for parallel systems with damage self-healing (2020) Computers & Industrial Engineering, 142, p. 106359; Grabot, B., Vallespir, B., Samuel, G., Bouras, A., Kiritsis, D., (2014), Advances Production Management Systems: Innovative and Knowledge-Based Production Management a Global-Local World: IFIP WG 5.7 International Conference, APMS 2014, Ajaccio, France, September 20-24, 2014, Proceedings, Springer; https://github.com/ray-project/ray, XXXX; Kharoufeh, J.P., Cox, S.M., Stochastic models for degradation-based reliability (2005) IIE Transactions, 37 (6), pp. 533-542; Liao, H., Elsayed, E.A., Reliability inference for field conditions from accelerated degradation testing (2006) Naval Research Logistics (NRL), 53 (6), pp. 576-587; Liu, B., Lin, J., Zhang, L., Xie, M., A dynamic maintenance strategy for prognostics and health management of degrading systems: Application in locomotive wheel-sets (2018) 2018 IEEE International Conference on Prognostics and Health Management (ICPHM), pp. 1-5. , IEEE; Lu, C.J., Meeker, W.O., Using degradation measures to estimate a time-to-failure distribution (1993) Technometrics, 35 (2), pp. 161-174; Mireh, S., Khodadadi, A., Haghighi, F., Copula-based reliability analysis of gamma degradation process and Weibull failure time (2019) International Journal of Quality & Reliability Management, 36 (5), pp. 654-668; Nguyen, T.A.T., Chou, S.-Y., Maintenance strategy selection for improving cost-effectiveness of offshore wind systems (2018) Energy Conversion and Management, 157, pp. 86-95; Omshi, E.M., Grall, A., Shemehsavar, S., A dynamic auto-adaptive predictive maintenance policy for degradation with unknown parameters (2020) European Journal of Operational Research, 282 (1), pp. 81-92; Park, C., Padgett, W., Accelerated degradation models for failure based on geometric Brownian motion and gamma processes (2005) Lifetime Data Analysis, 11 (4), pp. 511-527; Peng, Y., Dong, M., Zuo, M.J., Current status of machine prognostics in condition-based maintenance: A review (2010) The International Journal of Advanced Manufacturing Technology, 50 (1-4), pp. 297-313; Peng, W., Li, Y.-F., Yang, Y.-J., Huang, H.-Z., Zuo, M.J., Inverse Gaussian process models for degradation analysis: A Bayesian perspective (2014) Reliability Engineering & System Safety, 130, pp. 175-189; Peng, W., Zhu, S.-P., Shen, L., The transformed inverse Gaussian process as an age-and state-dependent degradation model (2019) Applied Mathematical Modelling, 75, pp. 837-852; Rocchetta, R., Bellani, L., Compare, M., Zio, E., Patelli, E., A reinforcement learning framework for optimal operation and maintenance of power grids (2019) Applied Energy, 241, pp. 291-301; Sabri-Laghaie, K., Noorossana, R., Reliability and maintenance models for a competing-risk system subjected to random usage (2016) IEEE Transactions on Reliability, 65 (3), pp. 1271-1283; Shi, Y., Xiang, Y., Li, M., Optimal maintenance policies for multi-level preventive maintenance with complex effects (2019) IISE Transactions, 51 (9), pp. 999-1011; Sun, B., Yan, M., Feng, Q., Li, Y., Ren, Y., Zhou, K., Zhang, W., Gamma degradation process and accelerated model combined reliability analysis method for rubber O-rings (2018) IEEE Access, 6, pp. 10581-10590; Sutton, R.S., Barto, A.G., (1998) Introduction to reinforcement learning (no. 4), , Cambridge, MA: MIT Press; Sutton, R.S., Barto, A.G., (2018) Reinforcement learning: An introduction, , MIT Press; Tang, L., Kacprzynski, G.J., Bock, J.R., Begin, M., An intelligent agent-based self-evolving maintenance and operations reasoning system (2006) 2006 IEEE aerospace conference, p. 12. , IEEE, p; Van Huynh, N., Nguyen, D.N., Hoang, D.T., Dutkiewicz, E., Jam Me If You Can:” defeating jammer with deep dueling neural network architecture and ambient backscattering augmented communications (2019) IEEE Journal on Selected Areas in Communications, 37 (11), pp. 2603-2620; Van Noortwijk, J., A survey of the application of gamma processes in maintenance (2009) Reliability Engineering & System Safety, 94 (1), pp. 2-21; Van Noortwijk, J.M., Cooke, R.M., Kok, M., A Bayesian failure model based on isotropic deterioration (1995) European Journal of Operational Research, 82 (2), pp. 270-282; Wang, C., Hou, Y., Qin, Z., Peng, C., Zhou, H., Dynamic coordinated condition-based maintenance for multiple components with external conditions (2015) IEEE Transactions on Power Delivery, 30 (5), pp. 2362-2370; Wang, X., Wang, H., Qi, C., Multi-agent reinforcement learning based maintenance policy for a resource constrained flow line system (2016) Journal of Intelligent Manufacturing, 27 (2), pp. 325-333; Watkins, C., (1989) Learning form delayed rewards, , Kings College, University of Cambridge,. PhD thesis; Yousefi, N., Coit, D.W., Song, S., Reliability analysis of systems considering clusters of dependent degrading components (2020) Reliability Engineering & System Safety, 202, p. 107005; Yousefi, N., Coit, D.W., Song, S., Feng, Q., Optimization of on-condition thresholds for a system of degrading components with competing dependent failure processes (2019) Reliability Engineering & System Safety, 192, p. 106547; Yousefi, N., Coit, D.W., Zhu, X., Dynamic maintenance policy for systems with repairable components subject to mutually dependent competing failure processes (2020) Computers & Industrial Engineering, 143, p. 106398; Yousefi, N., Tsianikas, S., Coit, D.W., Reinforcement learning for dynamic condition-based maintenance of a system with individually repairable components (2020) Quality Engineering, 32 (3), p. 388; Yousefi, N., Tsianikas, S., Zhou, J., Coit, D.W., Inspection plan prediction for multi-repairable component systems using neural network (2020) Institute of Industrial and Systems Engineers Conference (IISE) Proceeding 2020, , arXiv Preprint; Yousefi, N., Coit, D.W., Dynamic inspection planning for systems with individually repairable components (2019) 11th International Conference on Mathematical Methods in Reliability (MMR) proceeding., , t. I. C. o. M. M. i. R. (MMR), Ed; Yousefi, N., Coit, D.W., Reliability analysis of systems subject to mutually dependent competing failure processes with changing degradation rate (2019) 11th International Conference on Mathematical Methods in Reliability (MMR) proceeding. arXiv:1903.00076, , p. t. I. C. o. M. M. i. R. (MMR), Ed; Zhao, X., Liang, Z., Parlikad, A., Xie, M., Dynamic imperfect condition-based maintenance for systems subject to nonlinear degradation paths (2018) Industrial maintenance and reliability, p. 190. , Manchester, UK:, and, 12–15 June, 2018, p; Zhou, X., Xi, L., Lee, J., Reliability-centered predictive maintenance scheduling for a continuously monitored system subject to degradation (2007) Reliability Engineering & System Safety, 92 (4), pp. 530-534},
correspondence_address1={Yousefi, N.; Department of Industrial System Engineering, United States; email: no.yousefi@rutgers.edu},
publisher={Taylor and Francis Ltd.},
issn={08982112},
coden={QUENE},
language={English},
abbrev_source_title={Qual Eng},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhou2021,
author={Zhou, T. and Tang, D. and Zhu, H. and Zhang, Z.},
title={Multi-agent reinforcement learning for online scheduling in smart factories},
journal={Robotics and Computer-Integrated Manufacturing},
year={2021},
volume={72},
doi={10.1016/j.rcim.2021.102202},
art_number={102202},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107792029&doi=10.1016%2fj.rcim.2021.102202&partnerID=40&md5=6406c57312f7c777f2da60302610be1d},
affiliation={College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China},
abstract={Rapid advances in sensing and communication technologies connect isolated manufacturing units, which generates large amounts of data. The new trend of mass customization brings a higher level of disturbances and uncertainties to production planning. Traditional manufacturing systems analyze data and schedule orders in a centralized architecture, which is inefficient and unreliable for the overdependence on central controllers and limited communication channels. Internet of things (IoT) and cloud technologies make it possible to build a distributed manufacturing architecture such as the multi-agent system (MAS). Recently, artificial intelligence (AI) methods are used to solve scheduling problems in the manufacturing setting. However, it is difficult for scheduling algorithms to process high-dimensional data in a distributed system with heterogeneous manufacturing units. Therefore, this paper presents new cyber-physical integration in smart factories for online scheduling of low-volume-high-mix orders. First, manufacturing units are interconnected with each other through the cyber-physical system (CPS) by IoT technologies. Attributes of machining operations are stored and transmitted by radio frequency identification (RFID) tags. Second, we propose an AI scheduler with novel neural networks for each unit (e.g., warehouse, machine) to schedule dynamic operations with real-time sensor data. Each AI scheduler can collaborate with other schedulers by learning from their scheduling experiences. Third, we design new reward functions to improve the decision-making abilities of multiple AI schedulers based on reinforcement learning (RL). The proposed methodology is evaluated and validated in a smart factory by real-world case studies. Experimental results show that the new architecture for smart factories not only improves the learning and scheduling efficiency of multiple AI schedulers but also effectively deals with unexpected events such as rush orders and machine failures. © 2021 Elsevier Ltd},
author_keywords={Composite reward;  Multi-agent system;  Online scheduling;  Reinforcement learning;  Smart factory},
keywords={Agricultural robots;  Clustering algorithms;  Cyber Physical System;  Data warehouses;  Decision making;  Distributed database systems;  E-learning;  Embedded systems;  Intelligent agents;  Manufacture;  Multi agent systems;  Network architecture;  Production control;  Radio frequency identification (RFID);  Reinforcement learning;  Scheduling, Centralized architecture;  Communication technologies;  Cyber-physical systems (CPS);  Distributed manufacturing;  Internet of Things (IOT);  Multi-agent reinforcement learning;  Radio-frequency-identification tags (RFID);  Traditional manufacturing, Internet of things},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 52075257},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2020YFB1710500},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, NP2020304},
funding_text 1={This work is supported by the National Key Research and Development Program of China (No. 2020YFB1710500 ), the National Natural Science Foundation of China (No. 52075257 ), and the Fundamental Research Funds for the Central Universities (No. NP2020304 ). The authors express great thanks and appreciation to the editors and anonymous reviewers whose constructive comments significantly improved the paper.},
references={Harrington, J., Computer integrated manufacturing (1974), Industrial Press New York, USA; Leitão, P., Restivo, F., ADACOR: a holonic architecture for agile and adaptive manufacturing control (2006) Comput. Ind., 57 (2), pp. 121-130; Hao, T., Di, L., Wang, S., Dong, Z., CASOA: an architecture for agent-based manufacturing system in the context of Industry 4.0 (2017) IEEE Access, 6 (99), pp. 12746-12754; Leitão, P., Agent-based distributed manufacturing control: a state-of-the-art survey (2009) Eng. Appl. Artif. Intell., 22 (7), pp. 979-991; Li, K., Zhou, T., Liu, B.-H., Li, H., A multi-agent system for sharing distributed manufacturing resources (2018) Expert Syst. Appl., 99, pp. 32-43; Shen, W., Norrie, D.H., Agent-based systems for intelligent manufacturing: a state-of-the-art survey (1999) Knowl. Inf. Syst., 1 (2), pp. 129-156; Yang, H., Kumara, S., Bukkapatnam, S.T.S., Tsung, F., The internet of things for smart manufacturing: a review (2019) IISE Trans., 51 (11), pp. 1190-1216; Wang, L., An overview of internet-enabled cloud-based cyber manufacturing (2017) Trans. Inst. Meas. Control, 39 (4), pp. 388-397; Liu, C., Jiang, P., Jiang, W., Web-based digital twin modeling and remote control of cyber-physical production systems (2020) Rob. Comput. Integr. Manuf., 64; Lu, Y., Xu, X., Cloud-based manufacturing equipment and big data analytics to enable on-demand manufacturing services (2019) Rob. Comput. Integr. Manuf., 57, pp. 92-102; Shi, W., Cao, J., Zhang, Q., Li, Y., Xu, L., Edge computing: vision and challenges (2016) IEEE Internet of Things J., 3 (5), pp. 637-646; Pinedo, M., Scheduling: theory, algorithms, and systems (2012), Springer New York, USA fourth ed; Conway, R.W., Maxwell, W.L., Miller, L.W., Theory of Scheduling (1967), Addison-Wesley Reading, MA, USA; Blazewicz, J., Dror, M., Weglarz, J., Mathematical programming formulations for machine scheduling: a survey (1991) Eur. J. Oper. Res., 51 (3), pp. 283-300; Dai, M., Tang, D., Giret, A., Salido, M.A., Multi-objective optimization for energy-efficient flexible job shop scheduling problem with transportation constraints (2019) Rob. Comput. Integr. Manuf., 59, pp. 143-157; Park, H.S., Tran, N.H., An autonomous manufacturing system based on swarm of cognitive agents (2012) J. Manuf. Syst., 31 (3), pp. 337-348; Kusiak, A., Chen, M., Expert systems for planning and scheduling manufacturing systems (1988) Eur. J. Oper. Res., 34 (2), pp. 113-130; Jennings, N.R., Wooldridge, M., Applications of intelligent agents (1998) Agent Technology: Foundations, Applications, and Markets, pp. 3-28. , N.R. Jennings M.J. Wooldridge Springer Berlin, Heidelberg, Germany; Zhang, Y., Wang, J., Liu, Y., Game theory based real-time multi-objective flexible job shop scheduling considering environmental impact (2017) J. Clean. Prod., 167, pp. 665-679; Yeung, W.L., Agent-based manufacturing control based on distributed bid selection and publish-subscribe messaging: a simulation case study (2012) Int. J. Prod. Res., 50 (22), pp. 6339-6356; Wang, L.C., Cheng, C.Y., Lin, S.K., Distributed feedback control algorithm in an auction-based manufacturing planning and control system (2013) Int. J. Prod. Res., 51 (9), pp. 2667-2679; Chong, C.S., Sivakumar, A.I., Simulation-based scheduling for dynamic discrete manufacturing (2003) Proceedings of the 2003 Winter Simulation Conference, pp. 1465-1473. , IEEE; Rauf, M., Guan, Z., Sarfraz, S., Mumtaz, J., Shehab, E., Jahanzaib, M., Hanif, M., A smart algorithm for multi-criteria optimization of model sequencing problem in assembly lines (2020) Rob. Comput. Integr. Manuf., 61; Morariu, C., Morariu, O., Răileanu, S., Borangiu, T., Machine learning for predictive scheduling and resource allocation in large scale manufacturing systems (2020) Comput. Ind., 120; Salido, M.A., Escamilla, J., Barber, F., Giret, A., Rescheduling in job-shop problems for sustainable manufacturing systems (2017) J. Clean. Prod., 162, pp. S121-S132; Koren, Y., Shpitalni, M., Design of reconfigurable manufacturing systems (2010) J. Manuf. Syst., 29 (4), pp. 130-141; Ouelhadj, D., Petrovic, S., A survey of dynamic scheduling in manufacturing systems (2008) J. Sched., 12 (4), pp. 417-431; Abumaizar, R.J., Svestka, J.A., Rescheduling job shops under random disruptions (1997) Int. J. Prod. Res., 35 (7), pp. 2065-2082; Kunnathur, A.S., Sundararaghavan, P., Sampath, S., Dynamic rescheduling using a simulation-based expert system (2004) J. Manuf. Technol. Manag.; Ghaleb, M., Zolfagharinia, H., Taghipour, S., Real-time production scheduling in the Industry-4.0 context: Addressing uncertainties in job arrivals and machine breakdowns (2020) Comput. Oper. Res., 123; Zhou, B., Bao, J., Li, J., Lu, Y., Liu, T., Zhang, Q., A novel knowledge graph-based optimization approach for resource allocation in discrete manufacturing workshops (2021) Rob. Comput. Integr. Manuf., 71; Li, D., Tang, H., A semantic-level component-based scheduling method for customized manufacturing (2021) Rob. Comput. Integr. Manuf., 71; Sutton, R.S., Barto, A.G., Reinforcement learning: an introduction (2018), MIT Press Cambridge, MA, USA; Zhang, W., Dietterich, T.G., A reinforcement learning approach to job-shop scheduling (1995) International Joint Conference on Artificial Intelligence (IJCAI), pp. 1114-1120. , Citeseer; Chen, X., Hao, X., Lin, H.W., Murata, T., Rule driven multi objective dynamic scheduling by data envelopment analysis and reinforcement learning (2010) 2010 IEEE International Conference on Automation and Logistics, pp. 396-401. , IEEE; Zhang, Z., Zheng, L., Li, N., Wang, W., Zhong, S., Hu, K., Minimizing mean weighted tardiness in unrelated parallel machine scheduling with reinforcement learning (2012) Comput. Oper. Res., 39 (7), pp. 1315-1324; Mannion, P., Devlin, S., Duggan, J., Howley, E., Reward shaping for knowledge-based multi-objective multi-agent reinforcement learning (2018) Knowl. Eng. Rev., p. 33; Watkins, C.J.C.H., Learning from delayed rewards (1989), King's College London, UK Ph.D. Thesis; Nash, J., Non-cooperative games (1951) Annal. Math., 54 (2), pp. 286-295; Rjoub, G., Bentahar, J., Wahab, O.A., Bataineh, A., Deep smart scheduling: a deep learning approach for automated big data scheduling over the cloud (2019) 2019 7th International Conference on Future Internet of Things and Cloud (FiCloud), pp. 189-196. , IEEE; Zhou, T., Tang, D., Zhu, H., Wang, L., Reinforcement learning with composite rewards for production scheduling in a smart factory (2020) IEEE Access, 9, pp. 752-766},
correspondence_address1={Tang, D.; College of Mechanical and Electrical Engineering, China; email: d.tang@nuaa.edu.cn},
publisher={Elsevier Ltd},
issn={07365845},
coden={RCIME},
language={English},
abbrev_source_title={Rob Comput Integr Manuf},
document_type={Article},
source={Scopus},
}

@ARTICLE{Andriotis2021,
author={Andriotis, C.P. and Papakonstantinou, K.G.},
title={Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints},
journal={Reliability Engineering and System Safety},
year={2021},
volume={212},
doi={10.1016/j.ress.2021.107551},
art_number={107551},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104958870&doi=10.1016%2fj.ress.2021.107551&partnerID=40&md5=fb192093632d00083eabfcf900c772f0},
affiliation={Faculty of Architecture and the Built Environment, Delft University of Technology, 2628 BL Delft, Netherlands; Department of Civil & Environmental Engineering, The Pennsylvania State University, University ParkPA, United States},
abstract={Determination of inspection and maintenance policies for minimizing long-term risks and costs in deteriorating engineering environments constitutes a complex optimization problem. Major computational challenges include the (i) curse of dimensionality, due to exponential scaling of state/action set cardinalities with the number of components; (ii) curse of history, related to exponentially growing decision-trees with the number of decision-steps; (iii) presence of state uncertainties, induced by inherent environment stochasticity and variability of inspection/monitoring measurements; (iv) presence of constraints, pertaining to stochastic long-term limitations, due to resource scarcity and other infeasible/undesirable system responses. In this work, these challenges are addressed within a joint framework of constrained Partially Observable Markov Decision Processes (POMDP) and multi-agent Deep Reinforcement Learning (DRL). POMDPs optimally tackle (ii)-(iii), combining stochastic dynamic programming with Bayesian inference principles. Multi-agent DRL addresses (i), through deep function parametrizations and decentralized control assumptions. Challenge (iv) is herein handled through proper state augmentation and Lagrangian relaxation, with emphasis on life-cycle risk-based constraints and budget limitations. The underlying algorithmic steps are provided, and the proposed framework is found to outperform well-established policy baselines and facilitate adept prescription of inspection and intervention actions, in cases where decisions must be made in the most resource- and risk-aware manner. © 2021 Elsevier Ltd},
author_keywords={Constrained stochastic optimization;  Decentralized multi-agent control;  Deep reinforcement learning;  Inspection and maintenance planning;  Partially observable Markov decision processes;  System risk and reliability},
keywords={Bayesian networks;  Budget control;  Constrained optimization;  Cost engineering;  Deep learning;  Dynamic programming;  Inference engines;  Inspection;  Multi agent systems;  Planning;  Preventive maintenance;  Reinforcement learning;  Risk perception;  Stochastic systems, Constrained stochastic optimization;  Decentralized multi-agent control;  Inspection and maintenance;  Inspection planning;  Maintenance planning;  Multi agent;  Partially observable Markov decision process;  Reinforcement learnings;  System reliability;  System risk, Markov processes},
funding_details={National Science FoundationNational Science Foundation, NSF, 1751941},
funding_text 1={This material is based upon work supported by the U.S. National Science Foundation under CAREER Grant No. 1751941, and the Center for Integrated Asset Management for Multimodal Transportation Infrastructure Systems (CIAMTIS), 2018 U.S. DOT Region 3 University Center.},
references={Frangopol, D.M., Kallen, M.J., Noortwijk, J.M.V., Probabilistic models for life-cycle performance of deteriorating structures: review and future directions (2004) Progress in Structural Engineering and Materials, 6 (4), pp. 197-212; Sanchez-Silva, M., Frangopol, D.M., Padgett, J., Soliman, M., Maintenance and operation of infrastructure systems (2016) Journal of Structural Engineering, 142 (9); Bocchini, P., Frangopol, D.M., A probabilistic computational framework for bridge network optimal maintenance scheduling (2011) Reliability Engineering & System Safety, 96 (2), pp. 332-349; Saydam, D., Frangopol, D., Risk-based maintenance optimization of deteriorating bridges (2014) Journal of Structural Engineering, 141 (4); Yang, D.Y., Frangopol, D.M., Life-cycle management of deteriorating civil infrastructure considering resilience to lifetime hazards: A general approach based on renewal-reward processes (2019) Reliability Engineering & System Safety, 183, pp. 197-212; Marseguerra, M., Zio, E., Podofillini, L., Condition-based maintenance optimization by means of genetic algorithms and Monte Carlo simulation (2002) Reliability Engineering & System Safety, 77 (2), pp. 151-165; Frangopol, D.M., Lin, K.Y., Estes, A.C., Life-cycle cost design of deteriorating structures (1997) Journal of Structural Engineering, 123 (10), pp. 1390-1401; Faber, M.H., Stewart, M.G., Risk assessment for civil engineering facilities: critical overview and discussion (2003) Reliability Engineering & System Safety, 80 (2), pp. 173-184; Straub, D., Faber, M.H., Risk based inspection planning for structural systems (2005) Structural Safety, 27 (4), pp. 335-355; Luque, J., Straub, D., Risk-based optimal inspection strategies for structural systems using dynamic Bayesian networks (2019) Structural Safety, 76, pp. 60-80; Grall, A., Bérenguer, C., Dieulle, L., A condition-based maintenance policy for stochastically deteriorating systems (2002) Reliability Engineering & System Safety, 76 (2), pp. 167-180; Grall, A., Dieulle, L., Berenguer, C., Roussignol, M., Continuous-time predictive-maintenance scheduling for a deteriorating system (2002) IEEE Transactions on Reliability, 51 (2), pp. 141-150; Castanier, B., Bérenguer, C., Grall, A., A sequential condition-based repair/replacement policy with non-periodic inspections for a system subject to continuous wear (2003) Applied Stochastic Models in Business and Industry, 19 (4), pp. 327-347; Rackwitz, R., Lentz, A., Faber, M.H., Socio-economically sustainable civil engineering infrastructures by optimization (2005) Structural Safety, 27 (3), pp. 187-229; Madanat, S., Optimal infrastructure management decisions under uncertainty (1993) Transportation Research Part C: Emerging Technologies, 1 (1), pp. 77-88; Ellis, H., Jiang, M., Corotis, R.B., Inspection, maintenance, and repair with partial observability (1995) Journal of Infrastructure Systems, 1 (2), pp. 92-99; Papakonstantinou, K.G., Shinozuka, M., Optimum inspection and maintenance policies for corroded structures using partially observable Markov decision processes and stochastic, physically based models (2014) Probabilistic Engineering Mechanics, 37, pp. 93-108; Papakonstantinou, K.G., Andriotis, C.P., Shinozuka, M., POMDP and MOMDP solutions for structural life-cycle cost minimization under partial and mixed observability (2018) Structure and Infrastructure Engineering, 14 (7), pp. 869-882; Bocchini, P., Frangopol, D.M., Optimal resilience-and cost-based postdisaster intervention prioritization for bridges along a highway segment (2012) Journal of Bridge Engineering, 17 (1), pp. 117-129; González, A.D., Dueñas-Osorio, L., Sánchez-Silva, M., Medaglia, A.L., The interdependent network design problem for optimal infrastructure system restoration (2016) Computer-Aided Civil and Infrastructure Engineering, 31 (5), pp. 334-350; Nozhati, S., Sarkale, Y., Chong, E.K., Ellingwood, B.R., Optimal stochastic dynamic scheduling for managing community recovery from natural hazards (2020) Reliability Engineering & System Safety, 193; Bellman, R.E., Dynamic programming (1957), Princeton University Press; Pineau, J., Gordon, G., Thrun, S., Point-based value iteration: An anytime algorithm for POMDPs (2003) International Joint Conference on Artificial Intelligence; Kaelbling, L.P., Littman, M.L., Cassandra, A.R., Planning and acting in partially observable stochastic domains (1998) Artificial Intelligence, 101 (1), pp. 99-134; Papakonstantinou, K.G., Shinozuka, M., Planning structural inspection and maintenance policies via dynamic programming and Markov processes. Part I: Theory (2014) Reliability Engineering & System Safety, 130, pp. 202-213; Papakonstantinou, K.G., Shinozuka, M., Planning structural inspection and maintenance policies via dynamic programming and Markov processes. Part II: POMDP implementation (2014) Reliability Engineering & System Safety, 130, pp. 214-224; Papakonstantinou, K.G., Andriotis, C.P., Shinozuka, M., Point-based POMDP solvers for life-cycle cost minimization of deteriorating structures (2016) Proceedings of the 5th International Symposium on Life-Cycle Civil Engineering (IALCCE 2016), Delft, The Netherlands; Memarzadeh, M., Pozzi, M., Integrated inspection scheduling and maintenance planning for infrastructure systems (2015) Computer-Aided Civil and Infrastructure Engineering, 31 (6), pp. 403-415; Schöbi, R., Chatzi, E.N., Maintenance planning using continuous-state partially observable Markov decision processes and non-linear action models (2016) Structure and Infrastructure Engineering, 12 (8), pp. 977-994; Andriotis, C.P., Papakonstantinou, K.G., Managing engineering systems with large state and action spaces through deep reinforcement learning (2019) Reliability Engineering & System Safety, 191; Andriotis, C.P., Papakonstantinou, K.G., Life-cycle policies for large engineering systems under complete and partial observability (2019) 13th International Conference on Applications of Statistics and Probability in Civil Engineering (ICASP), Seoul, South Korea; Wang, Z., Bapst, V., Heess, N., Munos, R., Kavukcuoglu, K., De Freitas, N., Sample efficient actor-critic with experience replay (2016), arXiv:; Degris, T., White, M., Sutton, R.S., Off-policy actor-critic (2012), arXiv:; Shani, G., Pineau, J., Kaplow, R., A survey of point-based POMDP solvers (2013) Autonomous Agents and Multi-Agent Systems, 27 (1), pp. 1-51; Oliehoek, F.A., Amato, C., A concise introduction to decentralized POMDPs (2016), Springer; Bernstein, D.S., Givan, R., Immerman, N., Zilberstein, S., The complexity of decentralized control of Markov decision processes (2002) Mathematics of Operations Research, 27 (4), pp. 819-840; Gupta, J.K., Egorov, M., Kochenderfer, M., Cooperative multi-agent control using deep reinforcement learning (2017) International Conference on Autonomous Agents and Multiagent Systems; Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., Mordatch, I., Emergent tool use from multi-agent autocurricula (2019), arXiv:; Oroojlooyjadid, A., Hajinezhad, D., A review of cooperative multi-agent deep reinforcement learning (2019), arXiv:; Hernandez-Leal, P., Kartal, B., Taylor, M.E., A survey and critique of multiagent deep reinforcement learning (2019) Autonomous Agents and Multi-Agent Systems, 33 (6), pp. 750-797; Goulet, J.A., Der Kiureghian, A., Li, B., Pre-posterior optimization of sequence of measurement and intervention actions under structural reliability constraint (2015) Structural Safety, 52, pp. 1-9; Sørensen, J.D., Framework for risk-based planning of operation and maintenance for offshore wind turbines (2009) Wind Energy: An International Journal for Progress and Applications in Wind Power Conversion Technology, 12 (5), pp. 493-506; Altman, E., Constrained Markov decision processes (1999), CRC Press; Poupart, P., Malhotra, A., Pei, P., Kim, K., Goh, B., Bowling, M., Approximate linear programming for constrained partially observable Markov decision processes (2015) 29th Conference of the Association for the Advancement of Artificial Intelligence (AAAI); Isom, J.D., Meyn, S.P., Braatz, R.D., Piecewise linear dynamic programming for constrained POMDPs (2008) Association for the Advancement of Artificial Intelligence (AAAI) Conference; Kim, D., Lee, J., Kim, K.E., Poupart, P., Point-based value iteration for constrained POMDPs (2011) 22nd International Joint Conference on Artificial Intelligence; Walraven, E., Spaan, M.T., Column generation algorithms for constrained POMDPs (2018) Journal of Artificial Intelligence Research, 62, pp. 489-533; Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P., Trust region policy optimization (2015) International Conference on Machine Learning; Achiam, J., Held, D., Tamar, A., Abbeel, P., Constrained policy optimization (2017) 34th International Conference on Machine Learning; Zhang, Y., Vuong, Q., Ross, K.W., First order optimization in policy space for constrained deep reinforcement learning (2020), arXiv:; Tessler, C., Mankowitz, D.J., Mannor, S., Reward constrained policy optimization (2018), arXiv:; Peng, X.B., Abbeel, P., Levine, S., Van de Panne, M., Deepmimic: Example-guided deep reinforcement learning of physics-based character skills (2018) ACM Transactions on Graphics, 37 (4), pp. 1-4; Garcıa, J., Fernández, F., A comprehensive survey on safe reinforcement learning (2015) Journal of Machine Learning Research, 16 (1), pp. 1437-1480; Chow, Y., Ghavamzadeh, M., Janson, L., Pavone, M., Risk-constrained reinforcement learning with percentile risk criteria (2017) The Journal of Machine Learning Research, 18 (1), pp. 6070-6120; Bertsekas, D., (2005) Dynamic programming and optimal control, 1. , Athena Scientific Belmont, MA; Nicolai, R.P., Dekker, R., Optimal maintenance of multi-component systems: A review (2008) Complex System Maintenance Handbook, pp. 263-286. , Springer London; Memarzadeh, M., Pozzi, M., Kolter, J., Hierarchical modeling of systems with similar components: A framework for adaptive monitoring and control (2016) Reliability Engineering & System Safety, 153, pp. 159-169; Bismut, E., Straub, D., Inspection and maintenance planning in large monitored structures (2018) 6th International Symposium on Reliability and Engineering Risk Management (ISRERM); Rokneddin, K., Ghosh, J., Dueñas-Osorio, L., Padgett, J.E., Bridge retrofit prioritisation for ageing transportation networks subject to seismic hazards (2013) Structure and Infrastructure Engineering, 9 (10), pp. 1050-1066; Zhang, N., Alipour, A., A two-level mixed-integer programming model for bridge replacement prioritization (2020) Computer-Aided Civil and Infrastructure Engineering, 35 (2), pp. 116-133; Putterman, M.L., Markov Decision process: discrete stochastic dynamic programming (1994), Wiley; Sondik, E., The optimal control of partially observable Markov processes (1971), Stanford University, Stanford Electronics Labs; Papakonstantinou, K.G., Andriotis, C.P., Shinozuka, M., POMDP solutions for monitored structures (2016) IFIP WG-7.5 Conference on Reliability and Optimization of Structural Systems; Andriotis, C.P., Papakonstantinou, K.G., Chatzi, E.N., Value of structural health information in partially observable stochastic environments (2020) Structural Safety, , In Print; Papakonstantinou, K.G., Andriotis, C.P., Gao, H., Chatzi, E.N., Quantifying the value of structural health monitoring for decision making (2019) 13th International Conference on Applications of Statistics and Probability in Civil Engineering (ICASP13), Seoul, South Korea; Bellman, R., Dynamic programming and Lagrange multipliers (1956) Proceedings of the National Academy of Sciences of the United States of America, 42, p. 767; Bertsekas, D., Nonlinear Programming (1999), Athena Scientific; Uryasev, R., Rockafellar, S., Conditional value-at-risk for general loss distributions (2002) Journal of Banking and Finance, 26 (7), pp. 1443-1471; Di Castro, D., Tamar, A., Mannor, S., Policy gradients with variance related risk criteria (2012), arXiv:; Prashanth, L.A., Ghavamzadeh, M., Variance-constrained actor-critic algorithms for discounted and average reward MDPs (2016) Machine Learning, 105 (3), pp. 367-417; Smith, A.E., Coit, D.W., Baeck, T., Fogel, D., Michalewicz, Z., Penalty functions (1995) Handbook of Evolutionary Computation, 1, p. 97; Rocchetta, R., Bellani, L., Compare, M., Zio, E., Patelli, E., A reinforcement learning framework for optimal operation and maintenance of power grids (2019) Applied Energy 241, 241, pp. 291-301; Liu, Y., Chen, Y., Jiang, T., Dynamic selective maintenance optimization for multi-state systems over a finite horizon: A deep reinforcement learning approach (2020) European Journal of Operational Research, 283 (1), pp. 166-181; Skordilis, E., Moghaddass, R., A deep reinforcement learning approach for real-time sensor-driven decision making and predictive analytics (2020) Computers & Industrial Engineering, 147; Andriotis, C.P., Papakonstantinou, K.G., Extended and generalized fragility functions (2018) Journal of Engineering Mechanics, 144 (9); Andriotis, C.P., Papakonstantinou, K.G., Probabilistic structural performance assessment in hidden damage spaces (2018) Proceedings of Computational Stochastic Mechanics Conference (CSM), Paros, Greece; Papakonstantinou, K.G., Amir, M., Warn, G.P., A Scaled Spherical Simplex Filter (S3F) with a decreased n+2 sigma points set size and equivalent 2n+1 Unscented Kalman Filter (UKF) accuracy (2021) Mechanical Systems and Signal Processing, , In Print; Amir, M., Papakonstantinou, K.G., Warn, G.P., Scaled Spherical Simplex Filter and state-space damage-plasticity finite element model for computationally efficient system identification (2021) Journal of Engineering Mechanics, , Under Review; Morato, P.G., Papakonstantinou, K.G., Andriotis, C.P., Nielsen, J.S., Rigo, P., Optimal inspection and maintenance planning for deteriorating structures through dynamic Bayesian networks and Markov decision processes (2020) Structural Safety; Morato, P.G., Nielsen, J.S., Mai, A.Q., Rigo, P., POMDP based maintenance optimization of offshore wind substructures including monitoring (2019) 13th International Conference on Applications of Staitstics and Probability in Civil Engineering (ICASP13), Seoul, South Korea; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2014), arXiv:; Colone, L., Dimitrov, N., Straub, D., Predictive repair scheduling of wind turbine drive-train components based on machine learning (2019) Wind Energy, 22 (9), pp. 1230-1242},
correspondence_address1={Andriotis, C.P.; Faculty of Architecture and the Built Environment, 2628 BL Delft, Netherlands; email: c.andriotis@tudelft.nl},
publisher={Elsevier Ltd},
issn={09518320},
coden={RESSE},
language={English},
abbrev_source_title={Reliab Eng Syst Saf},
document_type={Article},
source={Scopus},
}

@ARTICLE{deCarvalho2021,
author={de Carvalho, J.P. and Dimitrakopoulos, R.},
title={Integrating production planning with truck-dispatching decisions through reinforcement learning while managing uncertainty},
journal={Minerals},
year={2021},
volume={11},
number={6},
doi={10.3390/min11060587},
art_number={587},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106872083&doi=10.3390%2fmin11060587&partnerID=40&md5=d7cac39cd19d6f544ff3b0d449d768eb},
affiliation={COSMO—Stochastic Mine Planning Laboratory, Department of Mining and Materials Engineering, McGill University, 3450 University Street, Montreal, QC  H3A 0E8, Canada},
abstract={This paper presents a new truck dispatching policy approach that is adaptive given different mining complex configurations in order to deliver supply material extracted by the shovels to the processors. The method aims to improve adherence to the operational plan and fleet utilization in a mining complex context. Several sources of operational uncertainty arising from the loading, hauling and dumping activities can influence the dispatching strategy. Given a fixed sequence of extraction of the mining blocks provided by the short-term plan, a discrete event simulator model emulates the interaction arising from these mining operations. The continuous repetition of this simulator and a reward function, associating a score value to each dispatching decision, generate sample experiences to train a deep Q-learning reinforcement learning model. The model learns from past dispatching experience, such that when a new task is required, a well-informed decision can be quickly taken. The approach is tested at a copper–gold mining complex, characterized by uncertainties in equipment performance and geological attributes, and the results show improvements in terms of production targets, metal production, and fleet management. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Discrete event simulation;  Mining equipment uncertainties;  Orebody uncertainty;  Q-learning;  Truck dispatching},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC, 239019, CRDPJ 500414-16},
funding_details={AngloGold AshantiAngloGold Ashanti},
funding_details={BHPBHP},
funding_details={COSMO Mining Industry ConsortiumCOSMO Mining Industry Consortium},
funding_details={IAMGOLDIAMGOLD},
funding_text 1={Funding: This work is funded by the National Science and Engineering Research Council of Canada (NSERC) CRD Grant CRDPJ 500414-16, NSERC Discovery Grant 239019, and the COSMO mining industry consortium (AngloGold Ashanti, AngloAmerican, BHP, De Beers, IAMGOLD, Kinross, Newmont Mining and Vale).},
references={Chaowasakoo, P., Seppälä, H., Koivo, H., Zhou, Q., Digitalization of Mine Operations: Scenarios to Benefit in Real-Time Truck Dispatching (2017) Int. J. Min. Sci. Technol, 27, pp. 229-236. , [CrossRef]; Alarie, S., Gamache, M., Overview of Solution Strategies Used in Truck Dispatching Systems for Open Pit Mines (2002) Int. J. Surf. Min. Reclam. Environ, 16, pp. 59-76. , [CrossRef]; Munirathinarn, M., Yingling, J.C., A Review of Computer-Based Truck Dispatching Strategies for Surface Mining Operations (1994) Int. J. Surf. Min. Reclam. Environ, 8, pp. 1-15. , [CrossRef]; Niemann-Delius, C., Fedurek, B., Computer-Aided Simulation of Loading and Transportation in Medium and Small Scale Surface Mines (2004) Mine Planning and Equipment Selection 2004, pp. 579-584. , Hardygora, M., Paszkowska, G., Sikora, M., Eds.; CRC Press: London, UK; Blom, M., Pearce, A.R., Stuckey, P.J., Short-Term Planning for Open Pit Mines: A Review (2018) Int. J. Min. Reclam. Environ, 930, pp. 1-22. , [CrossRef]; Afrapoli, A.M., Askari-Nasab, H., Mining Fleet Management Systems: A Review of Models and Algorithms (2019) Int. J. Min. Reclam. Environ, 33, pp. 42-60. , [CrossRef]; Temeng, V.A., Otuonye, F.O., Frendewey, J.O., Real-Time Truck Dispatching Using a Transportation Algorithm (1997) Int. J. Surf. Min. Reclam. Environ, 11, pp. 203-207. , [CrossRef]; Li, Z., A Methodology for the Optimum Control of Shovel and Truck Operations in Open-Pit Mining (1990) Min. Sci. Technol, 10, pp. 337-340. , [CrossRef]; Ta, C.H., Kresta, J.V., Forbes, J.F., Marquez, H.J., A Stochastic Optimization Approach to Mine Truck Allocation (2005) Int. J. Surf. Min. Reclam. Environ, 19, pp. 162-175. , [CrossRef]; Upadhyay, S.P., Askari-Nasab, H., Truck-Shovel Allocation Optimisation: A Goal Programming Approach (2016) Min. Technol, 125, pp. 82-92. , [CrossRef]; Afrapoli, A.M., Tabesh, M., Askari-Nasab, H., A Transportation Problem-Based Stochastic Integer Programming Model to Dispatch Surface Mining Trucks under Uncertainty (2019) Proceedings of the 27th International Symposium on Mine Planning and Equipment Selection—MPES 2018, pp. 255-264. , https://www.springerprofessional.de/en/a-transportation-problem-based-stochastic-integer-programming-mo/16498454, Springer: Berlin/Heidelberg, Germany, (accessed on 15 May 2021); White, J.W., Olson, J.P., Computer-Based Dispatching in Mines with Concurrent Operating Objectives (1986) Min. Eng, 38, pp. 1045-1054; Soumis, F., Ethier, J., Elbrond, J., Truck Dispatching in an Open Pit Mine (1989) Int. J. Surf. Min. Reclam. Environ, 3, pp. 115-119. , [CrossRef]; Elbrond, J., Soumis, F., Towards Integrated Production Planning and Truck Dispatching in Open Pit Mines (1987) Int. J. Surf. Min. Reclam. Environ, 1, pp. 1-6. , [CrossRef]; Sutton, R.S., Barto, A.G., (2018) Reinforcement Learning: An Introduction, , 2nd ed.; MIT Press: Cambridge, UK; Del Castillo, M.F., Dimitrakopoulos, R., Dynamically Optimizing the Strategic Plan of Mining Complexes under Supply Uncertainty (2019) Resour. Policy, 60, pp. 83-93. , [CrossRef]; Montiel, L., Dimitrakopoulos, R., Simultaneous Stochastic Optimization of Production Scheduling at Twin Creeks Mining Complex, Nevada (2018) Min. Eng, 70, pp. 12-20. , [CrossRef]; Goodfellow, R., Dimitrakopoulos, R., Global Optimization of Open Pit Mining Complexes with Uncertainty (2016) Appl. Soft Comput. J, 40, pp. 292-304. , [CrossRef]; Pimentel, B.S., Mateus, G.R., Almeida, F.A., Mathematical Models for Optimizing the Global Mining Supply Chain (2010) Intelligent Systems in Operations: Methods, Models and Applications in the Supply Chain, pp. 133-163. , Nag, B., Ed.; IGI Global: Hershey, PA, USA; Levinson, Z., Dimitrakopoulos, R., Adaptive Simultaneous Stochastic Optimization of a Gold Mining Complex: A Case Study (2020) J. S. African Inst. Min. Metall, 120, pp. 221-232. , [CrossRef] [PubMed]; Saliba, Z., Dimitrakopoulos, R., Simultaneous stochastic optimization of an open pit gold mining complex with supply and market uncertainty (2019) Min. Technol, 128, pp. 216-229. , [CrossRef]; Both, C., Dimitrakopoulos, R., Joint Stochastic Short-Term Production Scheduling and Fleet Management Optimization for Mining Complexes (2020) Optim. Eng, , [CrossRef]; Whittle, J., The Global Optimiser Works—What Next? (2018) Advances in Applied Strategic Mine Planning, pp. 31-37. , Dimitrakopoulos, R., Ed.; Springer International Publishing: Cham, Switzerland; Whittle, G., Global asset optimization (2007) Orebody Modelling and Strategic Mine Planning: Uncertainty and Risk Management Models, pp. 331-336. , Dimitrakopoulos, R., Ed.; Society of Mining: Carlton, Australia; Hoerger, S., Hoffman, L., Seymour, F., Mine Planning at Newmont’s Nevada Operations (1999) Min. Eng, 51, pp. 26-30; Chanda, E., Network Linear Programming Optimisation of an Integrated Mining and Metallurgical Complex (2007) Orebody Modelling and Strategic Mine Planning, pp. 149-155. , Dimitrakopoulos, R., Ed.; AusIMM: Carlton, Australia; Stone, P., Froyland, G., Menabde, M., Law, B., Pasyar, R., Monkhouse, P., Blasor–Blended Iron Ore Mine Planning Optimization at Yandi, Western Australia (2007) Orebody Modelling and Strategic Mine Planning: Uncertainty and Risk Management Models, 14, pp. 133-136. , Dimitrakopoulos, R., Ed.; Spectrum Series 14; AusIMM: Carlton, Australia; Sitek, P., Wikarek, J., Rutczyńska-Wdowiak, K., Capacitated Vehicle Routing Problem with Pick-Up, Alternative Delivery and Time Windows (CVRPPADTW): A Hybrid Approach (2019) Distributed Computing and Artificial Intelligence, Proceedings of the 16th International Conference, , Special Sessions, Avila, Spain, 26–28 June Springer International Publishing: Cham, Switzerland, 2019; Gola, A., Kłosowski, G., Development of Computer-Controlled Material Handling Model by Means of Fuzzy Logic and Genetic Algorithms (2019) Neurocomputing, 338, pp. 381-392. , [CrossRef]; Bocewicz, G., Nielsen, P., Banaszak, Z., Declarative Modeling of a Milk-Run Vehicle Routing Problem for Split and Merge Supply Streams Scheduling (2019) Information Systems Architecture and Technology: Proceedings of 39th International Conference on Information Systems Architecture and Technology—ISAT 2018, pp. 157-172. , Światek, J., Borzemski, L., Wilimowska, Z., Eds.; Springer: Cham, Switzerland; Pillac, V., Gendreau, M., Guéret, C., Medaglia, A.L., A Review of Dynamic Vehicle Routing Problems (2013) Eur. J. Oper. Res, 225, pp. 1-11. , [CrossRef]; Gendreau, M., Potvin, J.-Y., Dynamic Vehicle Routing and Dispatching (1998) Fleet Management and Logistics, pp. 115-126. , Crainic, T.G., Laporte, G., Eds.; Springer US: Boston, MA, USA; Secomandi, N., Margot, F., Reoptimization Approaches for the Vehicle-Routing Problem with Stochastic Demands (2009) Oper. Res, 57, pp. 214-230. , [CrossRef]; Azi, N., Gendreau, M., Potvin, J.-Y., A Dynamic Vehicle Routing Problem with Multiple Delivery Routes (2012) Ann. Oper. Res, 199, pp. 103-112. , [CrossRef]; Torkamani, E., Askari-Nasab, H., A Linkage of Truck-and-Shovel Operations to Short-Term Mine Plans Using Discrete-Event Simulation (2015) Int. J. Min. Miner. Eng, 6, pp. 97-118. , [CrossRef]; Matamoros, M.E.V., Dimitrakopoulos, R., Stochastic Short-Term Mine Production Schedule Accounting for Fleet Allocation, Operational Considerations and Blending Restrictions (2016) Eur. J. Oper. Res, 255, pp. 911-921. , [CrossRef]; Quigley, M., Dimitrakopoulos, R., Incorporating Geological and Equipment Performance Uncertainty While Optimising Short Term Mine Production Schedules (2019) Int. J. Min. Reclam. Environ, 34, pp. 362-383. , [CrossRef]; Bodon, P., Fricke, C., Sandeman, T., Stanford, C., Combining Optimisation and Simulation to Model a Supply Chain from Pit to Port (2018) Adv. Appl. Strateg. Mine Plan, pp. 251-267. , [CrossRef]; Hashemi, A.S., Sattarvand, J., Application of ARENA Simulation Software for Evaluation of Open Pit Mining Transportation Systems—A Case Study (2015) Proceedings of the 12th International Symposium Continuous Surface Mining—Aachen 2014, pp. 213-224. , https://link.springer.com/chapter/10.1007/978-3-319-12301-1_20, Niemann-Delius, C., Ed.; Springer: Cham, Switzerland, (accessed on 15 May 2021); Jaoua, A., Riopel, D., Gamache, M., A Framework for Realistic Microscopic Modelling of Surface Mining Transportation Systems (2009) Int. J. Min. Reclam. Environ, 23, pp. 51-75. , [CrossRef]; Sturgul, J., (2015) Discrete Simulation and Animation for Mining Engineers, , CRC Press: Boca Raton, FL, USA; Upadhyay, S.P., Askari-Nasab, H., Simulation and Optimization Approach for Uncertainty-Based Short-Term Planning in Open Pit Mines (2018) Int. J. Min. Sci. Technol, 28, pp. 153-166. , [CrossRef]; Yuriy, G., Vayenas, N., Discrete-Event Simulation of Mine Equipment Systems Combined with a Reliability Assessment Model Based on Genetic Algorithms (2008) Int. J. Min. Reclam. Environ, 22, pp. 70-83. , [CrossRef]; Law, A.M., Kelton, W.D., (1982) Simulation Modeling and Analysis, , McGraw-Hill.: New York, NY, USA; Jaoua, A., Gamache, M., Riopel, D., Specification of an Intelligent Simulation-Based Real Time Control Architecture: Application to Truck Control System (2012) Comput. Ind, 63, pp. 882-894. , [CrossRef]; Chaowasakoo, P., Seppälä, H., Koivo, H., Zhou, Q., Improving Fleet Management in Mines: The Benefit of Heterogeneous Match Factor (2017) Eur. J. Oper. Res, 261, pp. 1052-1065. , [CrossRef]; Afrapoli, A.M., Tabesh, M., Askari-Nasab, H., A Multiple Objective Transportation Problem Approach to Dynamic Truck Dispatching in Surface Mines (2019) Eur. J. Oper. Res, 276, pp. 331-342. , [CrossRef]; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Ostrovski, G., Human-Level Control through Deep Reinforcement Learning (2015) Nature, 518, pp. 529-533. , [CrossRef] [PubMed]; Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T., Silver, D., Kavukcuoglu, K., Asynchronous Methods for Deep Reinforcement Learning (2016) Proceedings of the 33rd International Conference on Machine Learning, 48, pp. 1928-1937. , New York, NY, USA, 20–22 June Balcan, M.F., Weinberger, K.Q., Eds; Van Hasselt, H., Guez, A., Silver, D., Van Hasselt, H., Guez, A., Silver, D., Deep Reinforcement Learning with Double Q-Learning (2016) Proceedings of the 30th AAAI Conference, , Shenzhen, China, 21 February; Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Bolton, A., Mastering the Game of Go without Human Knowledge (2017) Nature, 550, pp. 354-359. , [CrossRef]; Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Graepel, T., Mastering Atari, Go, Chess and Shogi by Planning with Learned Model (2020) Nature, 588, pp. 604-609. , [CrossRef]; Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Lanctot, M., Mastering the Game of Go with Deep Neural Networks and Tree Search (2016) Nature, 529, pp. 484-489. , [CrossRef]; Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A.S., Yeo, M., Makhzani, A., Schrittwieser, J., (2017) StarCraft II: A New Challenge for Reinforcement Learning.cs.LG, , arXiv arXiv:1708.04782; Vinyals, O., Babuschkin, I., Czarnecki, W.M., Mathieu, M., Dudzik, A., Chung, J., Choi, D.H., Georgiev, P., Grandmaster level in StarCraft II using multi-agent reinforcement learning (2019) Nat. Cell Biol, 575, pp. 350-354. , [CrossRef]; Paduraru, C., Dimitrakopoulos, R., Responding to new information in a mining complex: Fast mechanisms using machine learning (2019) Min. Technol, 128, pp. 129-142. , [CrossRef]; Kumar, A., Dimitrakopoulos, R., Maulen, M., Adaptive self-learning mechanisms for updating short-term production decisions in an industrial mining complex (2020) J. Intell. Manuf, 31, pp. 1795-1811. , [CrossRef]; Kumar, A., (2020) Artificial Intelligence Algorithms for Real-Time Production Planning with Incoming New Information in Mining Complexes, , Ph.D. Thesis, McGill University, Montréal, QC, Canada; Goovaerts, P., (1997) Geostatistics for Natural Resources Evaluation; Applied Geostatistics Series, , Oxford University Press: New York, NY, USA; Remy, N., Boucher, A., Wu, J., (2009) Applied Geostatistics with SGeMS: A User’s Guide, , Cambridge University Press: Cambridge, UK, 9780521514; Rossi, M.E., Deutsch, C.V., (2014) Mineral Resource Estimation, , Springer: Dordt, The Netherlands; Gómez-Hernández, J.J., Srivastava, R.M., One Step at a Time: The Origins of Sequential Simulation and Beyond (2021) Math. Geol, 53, pp. 193-209. , [CrossRef]; Minniakhmetov, I., Dimitrakopoulos, R., High-Order Data-Driven Spatial Simulation of Categorical Variables (2021) Math. Geosci, , [CrossRef]; Lin, L.-J., (1993) Reinforcement Learning for Robots Using Neural Networks, , Ph.D. Thesis, Carnegie Mellon University, Pittsburgh, PA, USA},
correspondence_address1={de Carvalho, J.P.; COSMO—Stochastic Mine Planning Laboratory, 3450 University Street, Canada; email: joao.decarvalho@mail.mcgill.ca; Dimitrakopoulos, R.; COSMO—Stochastic Mine Planning Laboratory, 3450 University Street, Canada; email: roussos.dimitrakopoulos@mcgill.ca},
publisher={MDPI AG},
issn={2075163X},
language={English},
abbrev_source_title={Minerals},
document_type={Article},
source={Scopus},
}

@ARTICLE{Woo2021,
author={Woo, J.H. and Kim, B. and Ju, S. and Cho, Y.I.},
title={Automation of load balancing for Gantt planning using reinforcement learning},
journal={Engineering Applications of Artificial Intelligence},
year={2021},
volume={101},
doi={10.1016/j.engappai.2021.104226},
art_number={104226},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102976506&doi=10.1016%2fj.engappai.2021.104226&partnerID=40&md5=0281f9bb7130fded3b73dfefa241a326},
affiliation={Department of Naval Architecture and Ocean Engineering, Seoul National University, Seoul, South Korea},
abstract={Typically, in the shipbuilding industry, several vessels are built concurrently, and a production plan is established through a hierarchical planning process. This process largely comprises strategic planning (long-term) and master planning (mid-term) aspects. The portion that requires the most manual work of the planner is the load balancing in the master planning stage. The load balancing of master planning is an area where optimization studies using mixed integer programming, genetic algorithms, tabu search algorithms, and others have been actively conducted in the field of operational research. However, its practical application has not been successful due to the complexity and the curse of dimensionality, which is dependent on the manual work of the planner. Therefore, a new method that can facilitate the efficient action of optimal decisions is required, replacing conventional production planning methods based on the manual work of the planner. With the advent of the 4th industrial revolution in recent years, machine learning technology based on deep neural networks has been rapidly developing and applied to a wide range of engineering problems. This study introduces a methodology that can quickly improve the load balancing problem in shipyard master planning by using a deep neural network-based reinforcement learning algorithm among various machine learning techniques. Furthermore, we aim to verify the feasibility of the developed methodology using the ship block production data of an actual shipyard. © 2021 The Author(s)},
author_keywords={Deep neural networks;  Production planning;  Reinforcement learning;  Shipbuilding;  Workload balancing},
keywords={Balancing;  Deep neural networks;  Genetic algorithms;  Integer programming;  Learning algorithms;  Production control;  Reinforcement learning;  Shipbuilding;  Ships;  Shipyards;  Tabu search, Hierarchical planning;  Load-Balancing;  Master planning;  Neural-networks;  Planning process;  Production Planning;  Production plans;  Reinforcement learnings;  Shipbuilding industry;  Workload balancing, Planning},
funding_text 1={This research was supported by following research projects:},
references={Bae, H.C., Park, K.C., Cha, B.C., Moon, I.K., Scheduling of shipyard sub-assembly process using genetic algorithms (2007) IE Interfaces, 20, pp. 33-40; Basán, N.P., Achkar, V.G., Méndez, C.A., Garcia-del Valle, A., A heuristic simulation-based framework to improve the scheduling of blocks assembly and the production process in shipbuilding (2017) 2017 Winter Simulation Conference (WSC), pp. 3218-3229. , IEEE; Cho, K., Oh, J., Ryu, K., Choi, H., An integrated process planning and scheduling system for block assembly in shipbuilding (1998) CIRP Annal., 47, pp. 419-422; Clark, W., The Gantt Chart: A Working Tool of Management (1922), The Ronald Press Company New York; Hameed, M.S.A., Schwung, A., Reinforcement learning on job shop scheduling problems using graph networks (2020), eprint; Hinton, G.E., Osindero, S., Teh, Y.-W., A fast learning algorithm for deep belief nets (2006) Neural Comput., 18, pp. 1527-1554; Hu, S.C., Zhang, Z.Z., Wang, S., Kao, Y.G., Ito, T., A project scheduling problem with spatial resource constraints and a corresponding guided local search algorithm (2019) J. Oper. Res. Soc., 70, pp. 1349-1361; Hwang, I.-H., Noh, J.-K., Lee, K.-K., Shin, J.-G., Short-term scheduling optimization for subassembly line in ship production using simulated annealing (2010) J. Korea Soc. Simul., 19, pp. 73-82; Jeong, J.H., Woo, J.H., Park, J., Machine learning methodology for management of shipbuilding master data (2020) Int. J. Naval Archit. Ocean Eng., 12, pp. 428-439; Kim, B., Jeong, Y., Shin, J.G., Spatial arrangement using deep reinforcement learning to minimise rearrangement in ship block stockyards (2020) Int. J. Prod. Res., pp. 1-15; König, M., Beißert, U., Steinhauer, D., (2007), Bargstädt, H.-J. Constraint-based simulation of outfitting processes in shipbuilding and civil engineering. In: Proceedings of the 6th EUROSIM Congress on Modeling and Simulation. Citeseer; Kwon, B., Lee, G.M., Spatial scheduling for large assembly blocks in shipbuilding (2015) Comput. Ind. Eng., 89, pp. 203-212; Lee, J.K., Choi, H.R., Yang, O.R., Kim, H.D., Scheduling shipbuilding using a constraint directed graph search: DAS-ERECT (1994) Intell. Syst. Account. Finance Manag., 3, pp. 111-125; Lee, Y.G., Ju, S., Woo, J.H., Simulation-based planning system for shipbuilding (2020) Int. J. Comput. Integr. Manuf., pp. 1-16; Lee, S.S., Kim, J.W., Case study for development of maintenance system for equipment of LNG-FPSO topside (2014) J. Ocean Eng. Technol., 28, pp. 533-539; Lee, W.J., Kim, B.H., Ko, K., Shin, H., Simulation based multi-objective fab scheduling by using reinforcement learning (2019) 2019 Winter Simulation Conference (WSC), pp. 2236-2247. , IEEE; Liu, Z., Chua, D.K.H., Yeoh, K.W., Aggregate production planning for shipbuilding with variation-inventory trade-offs (2011) Int. J. Prod. Res., 49, pp. 6249-6272; Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., Kavukcuoglu, K., (2016), pp. 1928-1937. , Asynchronous methods for deep reinforcement learning. In: International Conference on Machine Learning; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., Playing atari with deep reinforcement learning (2013), arXiv preprint; Romero-Hdz, J., Saha, B.N., Tstutsumi, S., Fincato, R., Incorporating domain knowledge into reinforcement learning to expedite welding sequence optimization (2020) Eng. Appl. Artif. Intell., 91; Rose, C.D., Coenen, J.M., Comparing four metaheuristics for solving a constraint satisfaction problem for ship outfitting scheduling (2015) Int. J. Prod. Res., 53, pp. 5782-5796; Shang, Z.Y., Gu, J.A., Ding, W., Duodu, E.A., Spatial scheduling optimization algorithm for block assembly in shipbuilding (2017) Math Probl. Eng., 2017; Shi, D., Fan, W., Xiao, Y., Lin, T., Xing, C., Intelligent scheduling of discrete automated production line via deep reinforcement learning (2020) Int. J. Prod. Res., 58 (11); Shin, D.S., Park, B.C., Lim, C.O., Oh, S.J., Kim, G.Y., Shin, S.C., Pipe routing using reinforcement learning on initial design stage (2020) J. Soc. Nav. Archit. Korea, 57, pp. 191-197; Shin, H.J., Ru, J.P., An adaptive scheduling algorithm for manufacturing process with non-stationary rework probabilities (2010) J. Korea Acad.-Indust. Cooper. Soc., 11, pp. 4174-4181; Woo, S.B., Ryu, H.G., Hahn, H.S., Heuristic algorithms for resource leveling in pre-erection scheduling and erection scheduling of shipbuilding (2003) IE Interfaces, 16, pp. 332-343; Zhuo, L., Chua Kim Huat, D., Wee, K.H., Scheduling dynamic block assembly in shipbuilding through hybrid simulation and spatial optimisation (2012) Int. J. Prod. Res., 50, pp. 5986-6004},
correspondence_address1={Cho, Y.I.; Department of Naval Architecture and Ocean Engineering, South Korea; email: whduddlsi@snu.ac.kr},
publisher={Elsevier Ltd},
issn={09521976},
coden={EAAIE},
language={English},
abbrev_source_title={Eng Appl Artif Intell},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Thomas20212289,
author={Thomas, J. and Hernandez, M.P. and Parlikad, A.K. and Piechocki, R.},
title={Network Maintenance Planning Via Multi-Agent Reinforcement Learning*},
journal={Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
year={2021},
pages={2289-2295},
doi={10.1109/SMC52423.2021.9659150},
note={cited By 0; Conference of 2021 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2021 ; Conference Date: 17 October 2021 Through 20 October 2021;  Conference Code:176213},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124302826&doi=10.1109%2fSMC52423.2021.9659150&partnerID=40&md5=38a5970bf214457524d070a1217845d8},
affiliation={University of Bristol, Communications, Systems and Network Group, United Kingdom; University of Cambridge, Institute for Manufacturing, United Kingdom},
abstract={Within this work, the challenge of developing maintenance planning solutions for networked assets is considered. This is challenging due to the very nature of these systems which are often heterogeneous, distributed and have complex co-dependencies between the constituent components for effective operation. We develop a Multi-Agent Reinforcement Learning (MARL) solution for this domain and apply it to a simulated Radio Access Network (RAN) comprising of nine Base Stations (BS). Through empirical evaluation we show that our model outperforms fixed corrective and preventive maintenance policies in terms of network availability whilst generally utilizing less than or equal amounts of maintenance resource. © 2021 IEEE.},
keywords={Corrective maintenance;  Fertilizers;  Multi agent systems;  Preventive maintenance;  Reinforcement learning, Empirical evaluations;  Maintenance planning;  Maintenance resources;  Multi-agent reinforcement learning;  Network availability;  Network maintenances;  Preventive maintenance policies;  Radio access networks;  Reinforcement learning solution, Planning},
funding_details={EP/R004935/1},
funding_details={Engineering and Physical Sciences Research CouncilEngineering and Physical Sciences Research Council, EPSRC},
funding_text 1={*This work was supported by the Next Generation Convergent Digital Infrastructure Project funded by Engineering and Physical Sciences Research Council (EPSRC) and British Telecom (BT) under grant EP/R004935/1. 1Communications, Systems and Network Group, University of Bristol, [jt17591,r.j.piechocki]@bristol.ac.uk 2Institute for Manufacturing, University of Cambridge, [mep53, aknp2]@cam.ac.uk},
references={Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M.A., Playing atari with deep reinforcement learning (2013) CoRR, , abs/1312.5602; Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Hassabis, D., Mastering the game of go without human knowledge (2017) Nature, 550 (7676), pp. 354-359. , oct; Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., Mordatch, I., Multi-agent actor-critic for mixed cooperative-competitive environments (2017) Proceedings of the 31st International Conference on Neural Information Processing Systems, Ser. NIPS'17, pp. 6382-6393. , Red Hook, NY, USA: Curran Associates Inc; Papoudakis, G., Christianos, F., Rahman, A., Albrecht, S.V., Dealing with non-stationarity in multi-agent deep reinforcement learning (2019) Tech. Rep; Sutton, R., Barto, A., (2018) Reinforcement Learning-An Introduction, , 2nd ed. Cambridge, MIT Press; Foerster, J.N., Assael, Y.M., De Freitas, N., Whiteson, S., Learning to communicate with deep multi-agent reinforcement learning (2016) Proceedings of the 30th International Conference on Neural Information Processing Systems, Ser. NIPS'16, pp. 2145-2153. , Red Hook, NY, USA: Curran Associates Inc; Hernandez-Leal, P., Kartal, B., Taylor, M.E., A survey and critique of multiagent deep reinforcement learning (2019) Autonomous Agents and Multi-Agent Systems, 33 (6), pp. 750-797. , Oct; Chan, G.K., Asgarpoor, S., Optimum maintenance policy with markov processes (2006) Electric Power Systems Research, 76 (6-7), pp. 452-456; Srinivasan, R., Parlikad, A.K., Value of condition monitoring in infrastructure maintenance (2013) Computers & Industrial Engineering, 66 (2), pp. 233-241; Liang, Z., Parlikad, A.K., Predictive group maintenance for multi-system multi-component networks (2019) Reliability Engineering and System Safety, 195 (2020), p. 106704. , no, October; Wang, J., Zhu, X., Joint optimization of condition-based maintenance and inventory control for a k-out-of-n:f system of multi-state degrading components (2021) European Journal of Operational Research, 290 (2), pp. 514-529; Knowles, M., Baglee, D., Wermter, S., Reinforcement learning for scheduling of maintenance (2011) Res. and Dev. in Intelligent Syst. XXVII: Incorporating Applications and Innovations in Intel. Sys. XVIII-AI 2010, 30th Sgai Int. Conf. on Innovative Techniques and Applications of Artificial Intel, pp. 409-422; Barde, S.R., Yacout, S., Shin, H., Optimal preventive maintenance policy based on reinforcement learning of a fleet of military trucks (2019) Journal of Intelligent Manufacturing, 30 (1), pp. 147-161; Rocchetta, R., Bellani, L., Compare, M., Zio, E., Patelli, E., A reinforcement learning framework for optimal operation and maintenance of power grids (2019) Applied Energy, 241, pp. 291-301; Huang, J., Chang, Q., Arinez, J., Deep reinforcement learning based preventive maintenance policy for serial production lines (2020) Expert Systems with Applications, 160, p. 113701; Wang, X., Wang, H., Qi, C., Multi-agent reinforcement learning based maintenance policy for a resource constrained flow line system (2016) Journal of Intelligent Manufacturing, 27 (2), pp. 325-333; Kuhnle, A., Jakubik, J., Lanza, G., Reinforcement learning for opportunistic maintenance optimization (2019) Production Engineering, 13 (1), pp. 33-41; Andriotis, C.P., Papakonstantinou, K.G., Managing engineering systems with large state and action spaces through deep reinforcement learning (2019) Reliability Engineering and System Safety, 191, p. 106483. , no. April; Li, B., Zhou, Y., Multi-component maintenance optimization: An approach combining genetic algorithm and multiagent reinforcement learning (2020) 2020 Global Reliability and Prognostics and Health Management, PHM-Shanghai 2020; Shapley, L.S., Stochastic games (1953) Proceedings of the National Academy of Sciences, 39 (10), pp. 1-7; Buşoniu, L., Babuska, R., De Schutter, B., Multi-agent reinforcement learning: An overview (2010) Innovations in Multi-agent Systems and Applications, 1, pp. 183-221; Bellemare, M.G., Candido, S., Castro, P.S., Gong, J., MacHado, M.C., Moitra, S., Ponda, S.S., Wang, Z., Autonomous navigation of stratospheric balloons using reinforcement learning (2020) Nature, 588 (7836), pp. 77-82; Grieves, M., (2011) Virtually Perfect: Driving Innovative and Lean Products Through Product Lifecycle Management, 11; Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., Whiteson, S., Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning (2018) Proceedings of the 35th International Conference on Machine Learning, Ser. Proceedings of Machine Learning Research, 80, pp. 4295-4304. , J. Dy and A. Krause, Eds, PMLR, 10-15 Jul; Kipf, T.N., Welling, M., Semi-supervised classification with graph convolutional networks (2017) 5th International Conference on Learning Representations, Iclr 2017, , Toulon, France, April 24-26, 2017, Conference Track Proceedings; Tan, M., Multi-agent reinforcement learning: Independent vs. Cooperative agents (1993) Proceedings of the Tenth International Conference on Machine Learning, pp. 330-337. , Morgan Kaufmann; Biewald, L., (2020) Experiment Tracking with Weights and Biases, , https://www.wandb.com/, software wandb.com; Wang, M., Zheng, D., Ye, Z., Gan, Q., Li, M., Song, X., Zhou, J., Zhang, Z., (2019) Deep Graph Library: A Graph-centric, Highly-performant Package for Graph Neural Networks; Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., Whiteson, S., Counterfactual multi-agent policy gradients (2017) 32nd Aaai Conference on Artificial Intelligence, Aaai 2018, pp. 2974-2982. , may},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1062922X},
isbn={9781665442077},
coden={PICYE},
language={English},
abbrev_source_title={Conf. Proc. IEEE Int. Conf. Syst. Man Cybern.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ghaleb2021,
author={Ghaleb, M. and Namoura, H.A. and Taghipour, S.},
title={Reinforcement Learning-based Real-time Scheduling under Random Machine Breakdowns and Other Disturbances: A Case Study},
journal={Proceedings - Annual Reliability and Maintainability Symposium},
year={2021},
volume={2021-May},
doi={10.1109/RAMS48097.2021.9605791},
note={cited By 0; Conference of 67th Annual Reliability and Maintainability Symposium, RAMS 2021 ; Conference Date: 24 May 2021 Through 27 May 2021;  Conference Code:174610},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123057268&doi=10.1109%2fRAMS48097.2021.9605791&partnerID=40&md5=cabcf32b5523f1830948d098939b9654},
affiliation={Ryerson University; University of Alberta},
abstract={In practice, production lines are dynamic and subject to several disruptions, unforeseen events, and requirements. Examples of such disruptions and events include random machine breakdowns, new order arrivals, order cancellations, due date changes, and shortage of material. Production schedules are adapted to such events by conducting rescheduling continuously, using real-time information about the current status of work-in-progress, machines, and resources on a shop-floor. This level of connectivity and real-time information sharing is achieved with the help of advanced initiatives in manufacturing technologies and industrial informatics such as Industry 4.0. Industry 4.0, driven by many emerging technologies, such as cyber-physical systems (CPS), internet of things (IoT), and internet of services (IoS), delivers real-time actionable data for smart decision-making in manufacturing. Several optimization approaches have been proposed to take advantage of such technology by incorporating the use of real-time information in the optimization process. Recently, with the increasing power of new machine learning (ML) algorithms in solving real-world problems, several ML approaches have been introduced to production planning and scheduling.In this paper, to achieve the Industry 4.0 vision in production control, we apply a reinforcement learning (RL) approach to real-time scheduling (RTS). The proposed RL based RTS uses a multiple dispatching rules (MDRs) strategy to enhance the production performance. A case study of a smart manufacturing firm is considered to apply the proposed approach. The firm is located in Ontario (Canada) and specializes in thermoplastic injection molding of various components and assemblies.The production schedules on the shop floor are sensitive to the changes resulting from random breakdowns and their associated maintenance activities. The production managers are using the data from the continuous monitoring system to update production schedules. The updating process is conducted manually based on their knowledge and a single dispatching rule (SDR) strategy. We believe that the proposed RTS system will help the company utilize the installed Industry 4.0 concepts and achieve the Industry 4.0 vision in the production control.The performance of the proposed RTS system is compared to the current strategy applied in the company. Results show the efficiency of the proposed RTS system compared to the current strategy. © 2021 IEEE.},
author_keywords={Industry 4.0;  machine learning in manufacturing;  Real-time scheduling;  reinforcement learning},
keywords={Decision making;  Embedded systems;  Floors;  Information use;  Injection molding;  Internet of things;  Monitoring;  Production control;  Real time systems;  Reinforcement learning;  Scheduling;  Scheduling algorithms, 'current;  Case-studies;  Dispatching rules;  Machine breakdown;  Machine learning in manufacturing;  Production schedule;  Real time scheduling;  Real-time information;  Scheduling systems;  Shopfloors, Industry 4.0},
funding_details={Canada Research ChairsCanada Research Chairs},
funding_text 1={The funding for this research was provided by the Canada Research Chair (CRC) program.},
references={Ghaleb, M., Zolfagharinia, H., Taghipour, S., Real-time production scheduling in the industry-4.0 context: Addressing uncertainties in job arrivals and machine breakdowns (2020) Comput. Oper. Res, 123; Zheng, P., Smart manufacturing systems for industry 4.0: Conceptual framework, scenarios, and future perspectives (2018) Front. Mech. Eng, 13 (2), pp. 137-150; Rossit, D.A., Tohmé, F., Frutos, M., Industry 4.0: Smart scheduling (2018) Int. J. Prod. Res, pp. 1-12; Oztemel, E., Gursev, S., Literature review of industry 4.0 and related technologies (2020) J. Intell. Manuf, 31 (1), pp. 127-182; Cheng, Y., Zhang, Y., Ji, P., Xu, W., Zhou, Z., Tao, F., Cyber-physical integration for moving digital factories forward towards smart manufacturing: A survey (2018) Int. J. Adv. Manuf. Technol, 97 (1-4), pp. 1209-1221; Priore, P., Gómez, A., Pino, R., Rosillo, R., Dynamic scheduling of manufacturing systems using machine learning: An updated review (2014) Artif. Intell. Eng. Des. Anal. Manuf. Aiedam, 28 (1), pp. 83-97; Xu, J., Huang, E., Hsieh, L., Lee, L.H., Jia, Q.S., Chen, C.H., Simulation optimization in the era of industrial 4.0 and the industrial internet (2016) J. Simul, 10 (4), pp. 310-320; Yang, W., Takakuwa, S., Simulation-based dynamic shop floor scheduling for flexible manufacturing system in the indusrty 4.0 environment (2017) Winter Simulation Conference, 91, pp. 399-404; Zaayman, G., Innamorato, A., The application of simio scheduling in industry 4.0 (2018) Proc.-Winter Simul. Conf, pp. 4425-4434; Son, Y.J., Rodríguez-Rivera, H., Wysk, R.A., A MULTI-PASS SIMULATION-BASED, REAL-TIME Scheduling and Shop Floor Control System; Priore, P., Parreño, J., Pino, R., Gómez, A., Puente, J., Learning-based scheduling of flexible manufacturing systems using support vector machines (2010) Appl. Artif. Intell, 24 (3), pp. 194-209; Choi, H.S., Kim, J.S., Lee, D.H., Real-time scheduling for reentrant hybrid flow shops: A decision tree based mechanism and its application to a tft-lcd line (2011) Expert Syst. Appl, 38 (4), pp. 3514-3521; Shiue, Y.R., Guh, R., Lee, K., Development of machine learningbased real time scheduling systems: Using ensemble based on wrapper feature selection approach (2012) Int. J. Prod. Res, 50 (20), pp. 5887-5905; Shiue, Y.R., Lee, K.C., Su, C.T., Real-time scheduling for a smart factory using a reinforcement learning approach (2018) Comput. Ind. Eng, 125 (101), pp. 604-614; Zang, Z., Hybrid deep neural network scheduler for job-shop problem based on convolution two-dimensional transformation (2019) Comput. Intell. Neurosci, 2019 (2); Abidi, M.H., Alkhalefah, H., Mohammed, M.K., Umer, U., Qudeiri, J.E.A., Optimal scheduling of flexible manufacturing system using improved lion-based hybrid machine learning approach (2020) Ieee Access, 8, pp. 96088-96114; Waschneck, B., Optimization of global production scheduling with deep reinforcement learning (2018) Procedia Cirp, 72, pp. 1264-1269; Ishii, N., Talavage, J.J., A mixed dispatching rule approach in fms scheduling (1994) Int. J. Flex. Manuf. Syst, 6 (1), pp. 69-87; Vesanto, J., Alhoniemi, E., Clustering of the self-organizing map (2000) Ieee Trans. Neural Networks, 11 (3), pp. 586-600; Davies, D.L., Bouldin, D.W., (1979) A Cluster Separation Measure, (2), pp. 224-227},
sponsors={IEEE},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={0149144X},
isbn={9781728180175},
language={English},
abbrev_source_title={Proc. Annu. Reliab. Maintainability Symp.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Voß2021815,
author={Voß, T. and Bode, C. and Heger, J.},
title={Dynamic Adjustment of Lot Sizes with Reinforcement Learning [Dynamische Losgrößenoptimierung mit bestärkendem Lernen]},
journal={ZWF Zeitschrift fuer Wirtschaftlichen Fabrikbetrieb},
year={2021},
volume={116},
number={11},
pages={815-819},
doi={10.1515/zwf-2021-0195},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121027881&doi=10.1515%2fzwf-2021-0195&partnerID=40&md5=67508726fc13e6bda17411df4092e5e8},
affiliation={Leuphana Universität Lüneburg, Institut für Produkt- und Prozessinnovation (PPI), Universitätsallee 1, Luneburg, 21335, Germany},
abstract={Production planning and control has a great influence on the economic efficiency and logistical performance of a company. In this context, the article gives an insight into the use of simulation as a virtual model of a filling machine in the process industry. Furthermore, it shows the associated possibility of a reinforcement learning (RL) approach for dynamic lot sizing. The contribution indicates a possible implementation in an ERP system and shows how a decision support tool can support the planner to save up to 5 % costs. © 2021 Walter de Gruyter GmbH, Berlin/Boston, Germany.},
author_keywords={Dynamic Adjustment;  Lot Sizing;  Reinforcement Learning;  Simulation},
keywords={Enterprise resource planning;  Production control;  Reinforcement learning, Dynamic adjustment;  Economic efficiency;  Lot sizing;  Lot-size;  Performance;  Process industries;  Production planning and control;  Reinforcement learnings;  Simulation;  Virtual models, Decision support systems},
references={Welsch, A., Eitle, V., Buxmann, P., Maschinelles Lernen (2018) HMD Praxis der Wirtschaftsinformatik, 55 (2), pp. 366-382; (2019) Einsatz von Künstlicher Intelligenz in der Deutschen Wirtschaft: Stand der KI-Nutzung im Jahr, , Bundesministerium für Wirtschaft (Hrsg.) Berlin 2020; Azadnia, A., Saman, M., Wong, K., Sustainable Supplier Selection and Order Lot-sizing: An Integrated Multi-objective Decision-making Process (2015) International Journal of Production Research, 53 (2), pp. 383-408; Schuh, G., Reuter, C., Prote, J.-P., Brambring, F., Ays, J., Increasing Data Integrity for Improving Decision Making in Production Planning and Control (2017) CIRP Annals, 66 (1), pp. 425-428; Schmidt, M., Maier, J., Grothkopp, M., Eine bibliometrische Analyse: Produktionsplanung und -steuerung und maschinelles Lernen (2020) Wt Werkstattstechnik Online, 110 (4), pp. 220-225; Maier, J., Voß, T., Heger, J., Schmidt, M., Ameri, F., Stecke, K.E., Von Cieminski, G., Kiritsis, D., Simulation Based Optimization of Lot Sizes for Opposing Logistic Objectives (2019) Advances in Production Management Systems, pp. 171-179. , Springer International Publishing Cham; Şenyiǧit, E., Düǧenci, M., Aydin, M., Zeydan, M., Heuristic-based Neural Networks for Stochastic Dynamic Lot Sizing Problem (2013) Applied Soft Computing, 13 (3), pp. 1332-1339; Hachicha, W., A Simulation Metamodelling Based Neural Networks for Lot-sizing Problem in MTO Sector (2011) International Journal of Simulation Modelling, 10 (4), pp. 191-203; Wang, J., Li, X., Zhu, X., Intelligent Dynamic Control of Stochastic Economic Lot Scheduling by Agent-based Reinforcement Learning (2012) International Journal of Production Research, 50 (16), pp. 4381-4395; Voß, T., Rokoss, A., Maier, J., Schmidt, M., Heger, J., (2021) Outperformed by A Computer? - Comparing Human Decisions to Reinforcement Learning Agents, Assigning Lot Sizes in A Learning Factory.In: Proceedings of the Conference on Learning Factories (CLF), June 3; Münzberg, B., (2013) Multikriterielle Losgrößenbildung, , Zugl.: Dissertation Univ. Hannover PZH-Verlag, Garbsen 2013; Sutton, R.S., Barto, A.G., (2018) Reinforcement Learning: An Introduction, , MIT Press Cambridge, MA; pathmind.com; Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O., (2017) Proximal Policy Optimization Algorithms; Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W., Donahue, J., Razavi, A., Vinyals, O., Kavukcuoglu, K., (2017) Population Based Training of Neural Networks},
correspondence_address1={Voß, T.; Leuphana Universität Lüneburg, Universitätsallee 1, Germany; email: thomas.voss@leuphana.de},
publisher={Walter de Gruyter GmbH},
issn={09470085},
coden={ZZWFF},
language={German},
abbrev_source_title={ZWF},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rabbanian2021932,
author={Rabbanian, S.S. and Nemati, M. and Knapp, G.M.},
title={A deep reinforcement learning approach for maintenance planning},
journal={IISE Annual Conference and Expo 2021},
year={2021},
pages={932-937},
note={cited By 0; Conference of IISE Annual Conference and Expo 2021 ; Conference Date: 22 May 2021 Through 25 May 2021;  Conference Code:174181},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120951185&partnerID=40&md5=6c294f9feeb35ed9757f87bf55d30521},
affiliation={Department of Mechanical and Industrial Engineering, Louisiana State University, Baton Rouge, LA, United States},
abstract={Meeting customer requirements is the main goal of production units, which translates into maximizing system uptime. Even planned maintenance actions typically create downtime for the system, reducing production capacity, but can prevent even more costly unplanned failures. In this study, we focus on the decision of whether to postpone (to a future planning period) or not postpone planned maintenance actions over a finite planning horizon, based on sensor data acquired from Industrial IoT (IIoT) devices equipped with condition monitoring. The finite horizon requires a mechanism for learning how to integrate "look ahead" to optimize maintenance policy over the long run. Deep Reinforcement Learning (DRL) provides a mechanism for learning an optimal policy. There is a set of possible health states for equipment based on data gathered from IoT devices, demand and product inventory level. The agent should choose the best action based on the observed state which would be the best maintenance decision whether to plan maintenance or to postpone it to a later time. The main goal of Deep Reinforcement Learning in this study is to find the optimal maintenance policy which maximizes reward. The reward function is dependent on minimizing total cost over the long run and the deep Q network (DQN) algorithm is used to learn the optimal policy. We simulate failure for equipment in a small production system based on wear-type failure distributions with stationary stochastic demand with a six-week planning horizon. © 2021 IISE Annual Conference and Expo 2021. All rights reserved.},
author_keywords={Deep Q network;  Maintenance scheduling;  Production planning;  Reinforcement learning},
keywords={Condition monitoring;  Internet of things;  Maintenance;  Planning;  Production control;  Reinforcement learning;  Stochastic systems, Customer requirements;  Deep Q network;  Longest run;  Maintenance Action;  Maintenance planning;  Maintenance scheduling;  Optimal policies;  Planned maintenance;  Production Planning;  Reinforcement learning approach, Deep learning},
references={Wang, W., An overview of the recent advances in delay-time-based maintenance modelling (2012) Reliability Engineering & System Safety, 106, pp. 165-178; Huang, J., Chang, Q., Arinez, J., Deep reinforcement learning based preventive maintenance policy for serial production lines (2020) Expert Systems with Applications, 160, p. 113701; Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., Meger, D., Deep reinforcement learning that matters (2018) Proceedings of the AAAI Conference on Artificial Intelligence, 32 (1); Wei, S., Bao, Y., Li, H., Optimal policy for structure maintenance: A deep reinforcement learning framework (2020) Structural Safety, 83, p. 101906; Hubbs, C. D., Li, C., Sahinidis, N. V., Grossmann, I. E., Wassick, J. M., A deep reinforcement learning approach for chemical production scheduling (2020) Computers & Chemical Engineering, 141, p. 106982; Kuhnle, A., Jakubik, J., Lanza, G., Reinforcement learning for opportunistic maintenance optimization (2019) Production Engineering, 13 (1), pp. 33-41; Wocker, M., Betz, N. K., Feuersänger, C., Lindworsky, A., Deuse, J., Unsupervised Learning for Opportunistic Maintenance Optimization in Flexible Manufacturing Systems (2020) Procedia CIRP, 93, pp. 1025-1030; Yousefi, N., Tsianikas, S., Coit, D. W., Reinforcement learning for dynamic condition-based maintenance of a system with individually repairable components (2020) Quality Engineering, pp. 1-21; Paraschos, P. D., Koulinas, G. K., Koulouriotis, D. E., Reinforcement learning for combined production-maintenance and quality control of a manufacturing system with deterioration failures (2020) Journal of Manufacturing Systems, 56, pp. 470-483; Adsule, A., Kulkarni, M., Tewari, A., Reinforcement learning for optimal policy learning in condition-based maintenance (2020) IET Collaborative Intelligent Manufacturing, 2 (4), pp. 182-188; Chaharsooghi, S. K., Heydari, J., Zegordi, S. H., A reinforcement learning model for supply chain ordering management: An application to the beer game (2008) Decision Support Systems, 45 (4), pp. 949-959; Sutton, R. S., Barto, A. G., (2018) Reinforcement learning: An introduction, , MIT press; Mnih, V., Human-level control through deep reinforcement learning (2015) nature, 518 (7540), pp. 529-533; Li, Y., (2018) Deep reinforcement learning, , arXiv preprint arXiv:1810.06339; Jiang, R., Murthy, D., A study of Weibull shape parameter: Properties and significance (2011) Reliability Engineering & System Safety, 96 (12), pp. 1619-1626},
editor={Ghate A., Krishnaiyer K., Paynabar K.},
publisher={Institute of Industrial and Systems Engineers, IISE},
isbn={9781713838470},
language={English},
abbrev_source_title={IISE Annu. Conf. Expo},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lang2021793,
author={Lang, S. and Kuetgens, M. and Reichardt, P. and Reggelin, T.},
title={Modeling production scheduling problems as reinforcement learning environments based on discrete-event simulation and OpenAI gym},
journal={IFAC-PapersOnLine},
year={2021},
volume={54},
number={1},
pages={793-798},
doi={10.1016/j.ifacol.2021.08.093},
note={cited By 1; Conference of 17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021 ; Conference Date: 7 June 2021 Through 9 June 2021;  Conference Code:146678},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120714630&doi=10.1016%2fj.ifacol.2021.08.093&partnerID=40&md5=550c8d3f57461af413b2e491941691ae},
affiliation={Fraunhofer Institute for Factory Operation and Automation IFF, Magdeburg, 39106, Germany; Otto von Guericke University, Magdeburg, 39106, Germany},
abstract={Reinforcement learning (RL) is an emerging research topic in production and logistics, as it offers potentials to solve complex planning and control problems in real time. In recent years, many researchers investigated RL algorithms for solving production scheduling problems. However, most of the related articles reveal only little information about the process of developing and implementing RL applications. Against this background, we present a method for modeling production scheduling problems as RL environments. More specifically, we propose the application of Discrete-Event Simulation for modeling production scheduling problems as an interoperable environments and the Gym interface of the OpenAI foundation to allow a simple integration of pre-built RL algorithms from OpenAI Baselines and Stable Baselines. We support our explanations with a simple example of a job shop scheduling problem. © 2021 The Authors. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0)},
author_keywords={Artificial intelligence;  Deep learning;  Discrete event modeling and simulation;  Neural network;  OpenAI Gym;  Production planning and control;  Production scheduling;  Reinforcement learning},
keywords={Computer aided instruction;  Deep learning;  Discrete event simulation;  Job shop scheduling;  Production control, Deep learning;  Discrete event models;  Discrete-event simulations;  Model and simulation;  Neural-networks;  Openai gym;  Production planning and control;  Production Scheduling;  Production scheduling problems;  Reinforcement learning algorithms, Reinforcement learning},
references={Arulkumaran, K., Deisenroth, M.P., Brundage, M., Bharath, A.A., Deep reinforcement learning: A brief survey (2017) IEEE Signal Processing Magazine, 34, pp. 26-38; Banks, J., Carson, J.S., II, Nelson, B.L., Nicol, D.M., (2010) Discrete-Event System Simulation, , 5th ed., International version). Upper Saddle River, N.J., London: Pearson Education; Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., (2016) OpenAI Gym; Dahl, O.-J., Nygaard, K., Simula: An ALGOL-based simulation language (1966) Communications of the ACM, 9, pp. 671-678; Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., (2017) GitHub Repository: GitHub, , https://github.com/openai/baselines, Accessed 22.11.2020; François-Lavet, V., Henderson, P., Islam, R., Bellemare, M.G., Pineau, J., An introduction to deep reinforcement learning (2018) Foundations and Trends in Machine Learning, 11, pp. 219-354; Gershwin, S.B., The future of manufacturing systems engineering (2018) International Journal of Production Research, 56, pp. 224-237; Hill, A., Raffin, A., Ernestus, M., Gleave, A., Kanervisto, A., Traore, R., (2018) GitHub Repository: GitHub; Lang, S., Lanzerath, N., Reggelin, T., Müller, M., Behrendt, F., Integration of deep reinforcement learning and discrete-event simulation for real-time scheduling of a flexible job-shop production (2020) Proceedings of the 2020 Winter Simulation Conference (WSC'20), , K.-H. G. Bae, B. Feng, S. Kim, S. Lazarova-Molnar, Z. Zheng, & T. Roeder, et al. Eds, Piscataway, NJ, USA: IEEE; Lang, S., Reggelin, T., Behrendt, F., Nahhas, A., Evolving neural networks to solve a two-stage hybrid flow shop scheduling problem with family setup times (2020) Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS 2020), pp. 1298-1307; Lang, S., Schenk, M., Reggelin, T., Towards learning- And knowledge-based methods of artificial intelligence for short-term operative planning tasks in production and logistics: Research idea and framework (2019) IFAC-PapersOnLine, 52, pp. 2716-2721; Lin, C.-C., Deng, D.-J., Chih, Y.-L., Chiu, H.-T., Smart manufacturing scheduling with edge computing using multiclass deep Q network (2019) IEEE Transactions on Industrial Informatics, 15, pp. 4276-4284; Luo, S., Dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning (2020) Applied Soft Computing, 91. , article-nr; Matloff, N., (2008) Introduction to Discrete-Event Simulation and the SimPy Language, , http://heather.cs.ucdavis.edu/~matloff/156/PLN/DESimIntro.pdf, Accessed 24.11.2020; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Playing atari with deep reinforcement learning (2013) Technical Report, NIPS Deep Learning Workshop 2013; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Hassabis, D., Human-level control through deep reinforcement learning (2015) Nature, 518, pp. 529-533; Scholz-Reiter, B., de Beer, C., Freitag, M., Hamann, T., Rekersbrink, H., Tervo, J.T., Dynamik logistischer Systeme (2008) Beiträge Zu Einer Theorie Der Logistik, pp. 109-138. , Nyhuis Ed, Berlin, Heidelberg: Springer-Verlag Berlin Heidelberg; Schriber, T.J., Brunner, D.T., Smith, J.S., Inside discrete-event simulation software: How it works and why it matters (2017) Proceedings of the 2017 Winter Simulation Conference (WSC'17), pp. 735-749. , W. K. Chan, A. D'Ambrogio, G. Zacharewicz, N. Mustafee, G. Wainer, & E. H. Eds, Piscataway, NJ, USA: IEEE; Shiue, Y.-R., Lee, K.-C., Su, C.-T., Real-time scheduling for a smart factory using a reinforcement learning Approach (2018) Computers & Industrial Engineering, 125, pp. 604-614; Stricker, N., Kuhnle, A., Sturm, R., Friess, S., Reinforcement learning for adaptive order dispatching in the semiconductor industry (2018) CIRP Annals, 67, pp. 511-514; Sutton, R.S., Barto, A., (2018) Reinforcement Learning: An Introduction, , 2th ed.). Cambridge, MA, London: The MIT Press; van der Ham, R., SalAbim: Discrete event simulation and animation in python (2018) Journal of Open Source Software, 3, pp. 767-768; Wang, H., Sarker, B.R., Li, J., Li, J., Adaptive scheduling for assembly job shop with uncertain assembly times based on dual Q-learning (2020) International Journal of Production Research, pp. 1-17; Waschneck, B., Reichstaller, A., Belzner, L., Altenmüller, T., Bauernhansl, T., Knapp, A., Kyek, A., Optimization of global production scheduling with deep reinforcement learning (2018) Procedia CIRP, 72, pp. 1264-1269},
correspondence_address1={Lang, S.; Fraunhofer Institute for Factory Operation and Automation IFFGermany; email: sebastian.lang@iff.fraunhofer.de},
publisher={Elsevier B.V.},
issn={24058963},
language={English},
abbrev_source_title={IFAC-PapersOnLine},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Paraschos2021401,
author={Paraschos, P.D. and Koulinas, G.K. and Koulouriotis, D.E.},
title={Parametric and reinforcement learning control for degrading multi-stage systems},
journal={Procedia Manufacturing},
year={2021},
volume={55},
number={C},
pages={401-408},
doi={10.1016/j.promfg.2021.10.055},
note={cited By 1; Conference of 30th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM 2021 ; Conference Date: 7 September 2021 Through 10 September 2021;  Conference Code:146668},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120656839&doi=10.1016%2fj.promfg.2021.10.055&partnerID=40&md5=82604818be5f396d6e2da5420df964d5},
affiliation={Department of Production and Management Engineering, Democritus University of Thrace, 12 Vas. Sofias st., Xanthi, 67100, Greece},
abstract={This paper addresses the joint control problem in the context of a two-stage stochastic manufacturing/remanufacturing system, which involve both manufacturing and remanufacturing processes. Its operability is affected by frequent deterioration failures. Along with the condition of the system, the manufactured items are affected as well. Thus, the system obtains lesser revenues due to the low-quality products and the downtimes of the deteriorated system. For this purpose, the state and the condition of the systems and the manufactured products must be monitored dynamically so as to devise an optimal strategy for manufacturing, maintenance, and quality control. The present paper proposes a novel two-agent reinforcement learning framework that incorporates parametric production and maintenance activities. The aim is to improve the productivity of the system and keep the system operational with minimal maintenance activities so as to maximize the overall profitability. The performance of the presented approach is evaluated through experimental scenarios. © 2021 The Authors. Published by Elsevier Ltd.},
author_keywords={Maintenance;  Multi-agent reinforcement learning;  Parametric policies;  Production;  Remanufacturing},
keywords={Deterioration;  Multi agent systems;  Quality control;  Reinforcement learning;  Stochastic systems, Condition;  Control problems;  Joint control;  Maintenance activity;  Multi-agent reinforcement learning;  Multistage system;  Parametric policy;  Reinforcement learning control;  Remanufacturing;  Stochastics, Maintenance},
funding_details={European CommissionEuropean Commission, EC},
funding_details={European Social FundEuropean Social Fund, ESF},
funding_text 1={This research is co-financed by Greece and the European Union (European Social Fund - ESF) through the Operational},
references={Xanthopoulos, A.S., Chnitidis, G., Koulouriotis, D.E., Reinforcement learning-based adaptive production control of pull manufacturing systems (2019) J. Ind. Prod. Eng., 36, pp. 313-323; Deng, S., Yeh, T.H., Using least squares support vector machines for the airframe structures manufacturing cost estimation (2011) Int. J. Prod. Econ., 131, pp. 701-708; Ghaleb, M., Zolfagharinia, H., Taghipour, S., Real-time production scheduling in the Industry-4.0 context: Addressing uncertainties in job arrivals and machine breakdowns (2020) Comput. Oper. Res., 123, p. 105031; Sharp, M., Ak, R., Hedberg, T., A survey of the advancing use and development of machine learning in smart manufacturing (2018) J. Manuf. Syst., 48, pp. 170-179; Paraschos, P.D., Koulinas, G.K., Koulouriotis, D.E., Reinforcement learning for combined production-maintenance and quality control of a manufacturing system with deterioration failures (2020) J. Manuf. Syst., 56, pp. 470-483; Zhou, T., Tang, D., Zhu, H., Wang, L., Reinforcement learning with composite rewards for production scheduling in a smart factory (2021) IEEE Access, 9, pp. 752-766; Kim, Y.G., Lee, S., Son, J., Bae, H., Do, C.B., Multi-agent system and reinforcement learning approach for distributed intelligence in a flexible smart manufacturing system (2020) J. Manuf. Syst., 57, pp. 440-450; Koulinas, G., Paraschos, P., Koulouriotis, D., A Decision Trees-based knowledge mining approach for controlling a complex production system (2020) Procedia Manuf, pp. 1439-1445. , Elsevier B; Xanthopoulos, A.S., Kiatipis, A., Koulouriotis, D.E., Stieger, S., Reinforcement learning-based and parametric production-maintenance control policies for a deteriorating manufacturing system (2017) IEEE Access, 6, pp. 576-588; Mosayebi Omshi, E., Grall, A., Shemehsavar, S., A dynamic auto-adaptive predictive maintenance policy for degradation with unknown parameters (2020) Eur. J. Oper. Res., 282, pp. 81-92; Kuhnle, A., Jakubik, J., Lanza, G., Reinforcement learning for opportunistic maintenance optimization (2019) Prod. Eng., 13, pp. 33-41; Wang, X., Qi, C., Wang, H., Si, Q., Zhang, G., Resilience-driven maintenance scheduling methodology for multi-agent production line system (2015) Proc. 2015 27th Chinese Control Decis. Conf. CCDC 2015, pp. 614-619. , Institute of Electrical and Electronics Engineers Inc; Lai, X., Chen, Z., Bidanda, B., Optimal decision of an economic production quantity model for imperfect manufacturing under hybrid maintenance policy with shortages and partial backlogging (2019) Int. J. Prod. Res., 57, pp. 6061-6085; Zhou, Y.C., Sun, X.C., Robust optimal inventory and acquisition effort decisions in a hybrid manufacturing/remanufacturing system (2019) J. Ind. Prod. Eng., 36, pp. 335-350; Bahria, N., Harbaoui Dridi, I., Chelbi, A., Bouchriha, H., Joint design of control chart, production and maintenance policy for unreliable manufacturing systems J. Qual. Maint. Eng., 2020. , https://doi.org/10.1108/JQME-01-2020-0006; Rivera-Gómez, H., Gharbi, A., Kenné, J.-P., Montaño-Arango, O., Corona-Armenta, J.R., Joint optimization of production and maintenance strategies considering a dynamic sampling strategy for a deteriorating system (2020) Comput. Ind. Eng., 140, p. 106273; Lu, S., Pei, J., Liu, X., Pardalos, P.M., A hybrid DBH-VNS for high-end equipment production scheduling with machine failures and preventive maintenance activities (2021) J. Comput. Appl. Math., 384, p. 113195; Onyeocha, C.E., Wang, J., Khoury, J., Geraghty, J., A comparison of HK-CONWIP and BK-CONWIP control strategies in a multi-product manufacturing system (2015) Oper. Res. Perspect., 2, pp. 137-149; Malik, S., Kim, D., A hybrid scheduling mechanism based on agent cooperation mechanism and fair emergency first in smart factory (2020) IEEE Access, 8, pp. 227064-227075; Xanthopoulos, A.S., Koulouriotis, D.E., Gasteratos, A., Ioannidis, S., Efficient priority rules for dynamic sequencing with sequence-dependent setups (2016) Int. J. Ind. Eng. Comput., 7, pp. 367-384; Duri, C., Frein, Y., Di Mascolo, M., Comparison among three pull control policies: Kanban, base stock, and generalized kanban (2000) Ann. Oper. Res., 93, pp. 41-69; Dallery, Y., Liberopoulos, G., Extended kanban control system: Combining kanban and base stock (2000) IIE Trans, 32, pp. 369-386; Gosavi, A., Reinforcement learning for long-run average cost (2004) Eur. J. Oper. Res., 155, pp. 654-674; Schwartz, A., A reinforcement learning method for maximizing undiscounted rewards (1993) Proc. Tenth Int. Conf. Mach. Learn., pp. 298-305},
correspondence_address1={Paraschos, P.D.; Department of Production and Management Engineering, 12 Vas. Sofias st., Greece; email: pparasc@pme.duth.gr},
publisher={Elsevier B.V.},
issn={23519789},
language={English},
abbrev_source_title={Procedia Manuf.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Alves2021229,
author={Alves, J.C. and Silva, D.M. and Mateus, G.R.},
title={Applying and Comparing Policy Gradient Methods to Multi-echelon Supply Chains with Uncertain Demands and Lead Times},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12855 LNAI},
pages={229-239},
doi={10.1007/978-3-030-87897-9_21},
note={cited By 0; Conference of 20th International Conference on Artificial Intelligence and Soft Computing, ICAISC 2021 ; Conference Date: 21 June 2021 Through 23 June 2021;  Conference Code:266689},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117700566&doi=10.1007%2f978-3-030-87897-9_21&partnerID=40&md5=f7d50bdb3d7e3cba79a2cdd0085fe9a5},
affiliation={Federal University of Lavras, Lavras, Brazil; Federal Institute of Minas Gerais, Formiga, Brazil; Federal University of Minas Gerais, Belo Horizonte, Brazil},
abstract={In the present work, we have applied and compared Deep Reinforcement Learning techniques to solve a problem usually addressed with Operations Research tools. State-of-the-art Policy Gradient methods are used in a production planning and product distribution problem, considering a four-echelon supply chain with uncertain lead times, to minimize total operating costs while meeting uncertain seasonal demands. Two-phases experiments are conducted regarding eight different scenarios. Firstly, A2C, DDPG, PPO, SAC, and TD3 are used considering fixed training budgets. In the second phase, the best two algorithms from the first phase are tuned considering different stopping criteria. Results show that PPO and SAC perform better for the problem addressed. They achieve comparable learning behavior, final performance, and sample complexity, while PPO is faster in terms of wall-clock time. © 2021, Springer Nature Switzerland AG.},
author_keywords={Deep reinforcement learning;  Policy gradient methods;  Supply chain;  Uncertainty},
keywords={Budget control;  Gradient methods;  Operating costs;  Operations research;  Production control;  Reinforcement learning;  Supply chains, Leadtime;  Multi echelon supply chains;  Operation research;  Policy gradient methods;  Production Planning;  Reinforcement learning techniques;  Research tools;  State of the art;  Uncertain demand;  Uncertainty, Deep learning},
funding_details={Conselho Nacional de Desenvolvimento Científico e TecnológicoConselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq},
funding_details={Fundação de Amparo à Pesquisa do Estado de Minas GeraisFundação de Amparo à Pesquisa do Estado de Minas Gerais, FAPEMIG},
funding_text 1={This research has been supported by the following Brazilian institutions: Minas Gerais Research Funding Foundation (FAPEMIG) and National Council for Scientific and Technological Development (CNPq).},
references={Alves, J.C., Mateus, G.R., (2021) Multi-Echelon Supply Chains with Uncertain Seasonal Demands and Lead Times Using Deep Reinforcement Learning, , Submitted; Bergstra, J., Bardenet, R., Bengio, Y., Kégl, B., Algorithms for hyper-parameter optimization (2011) Proceedings of the 24Th International Conference on Neural Information Processing Systems. NIPS 2011, pp. 2546-2554. , Red Hook, NY, USA, pp., Curran Associates Inc; Colas, C., Sigaud, O., Oudeyer, P.Y., A Hitchhiker’s guide to statistical comparisons of reinforcement learning algorithms (2019) ICLR Worskhop on Reproducibility, , https://hal.archives-ouvertes.fr/hal-02369859, Nouvelle-Orléans, United States, May; Fujimoto, S., van Hoof, H., Meger, D., Addressing function approximation error in actor-critic methods (2018) Dy, J., Krause, A. (Eds.) Proceedings of the 35Th International Conference on Machine Learning. Proceedings of Machine Learning Research, 10–15 Jul 2018, Vol. 80, Pp. 1587–1596. PMLR, , http://proceedings.mlr.press/v80/fujimoto18a.html; Geevers, K., (2020) Deep Reinforcement Learning in Inventory Management, , http://essay.utwente.nl/85432/, Master’s thesis, December; Gijsbrechts, J., Boute, R.N., Van Mieghem, J.A., Zhang, D.: Can deep reinforcement learning improve inventory management? performance on dual sourcing, lost sales and multi-echelon problems. SSRN (2020). https://doi.org/10.2139/ssrn. 3302881; Haarnoja, T., Zhou, A., Abbeel, P., Levine, S., Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor (2018) Dy, J., Krause, A. (Eds.) Proceedings of the 35Th International Conference on Machine Learning. Proceedings of Machine Learning Research, 10–15 Jul 2018, Vol. 80, Pp. 1861–1870. PMLR, , http://proceedings.mlr.press/v80/haarnoja18b.html; Hachaïchi, Y., Chemingui, Y., Affes, M.: A policy gradient based reinforcement learning method for supply chain management. In: 2020 4th International Conference on Advanced Systems and Emergent Technologies (IC ASET), pp. 135–140 (2020). https://doi.org/10.1109/IC ASET49463.2020.9318258; Hutse, V., (2019) Reinforcement Learning for Inventory Optimisation in Multi-Echelon Supply Chains, , http://lib.ugent.be/catalog/rug01, Master in business engineering; Kemmer, L., von Kleist, H., de Rochebouët, D., Tziortziotis, N., Read, J., Reinforcement learning for supply chain optimization (2018) European Workshop on Reinforcement Learning, 14. , https://ewrl.files.wordpress.com/2018/09/ewrl142018paper44.pdf, vol; Lillicrap, T.P., (2015) Continuous Control with Deep Reinforcement Learning. Arxiv Preprint Arxiv, 1509, p. 02971; Mnih, V., et al.: Asynchronous methods for deep reinforcement learning. In: Balcan, M.F., Weinberger, K.Q. (eds.) Proceedings of The 33rd International Conference on Machine Learning. Proceedings of Machine Learning Research, 20–22 Jun 2016, New York, USA, vol. 48, pp. 1928–1937. PMLR (2016). http://proceedings.mlr. press/v48/mniha16.html; Mnih, V., Human-level control through deep reinforcement learning (2015) Nature, 518 (7540), pp. 529-533. , https://doi.org/10.1038/nature14236; Oroojlooyjadid, A., (2019) Applications of Machine Learning in Supply Chains, , https://preserve.lehigh.edu/etd/4364, Ph.D. thesis; Peng, Z., Zhang, Y., Feng, Y., Zhang, T., Wu, Z., Su, H.: Deep reinforcement learning approach for capacitated supply chain optimization under demand uncertainty. In: 2019 Chinese Automation Congress (CAC), pp. 3512–3517 (2019). https://doi. org/10.1109/CAC48633.2019.8997498; Perez, H.D., Hubbs, C.D., Li, C., Grossmann, I.E., Algorithmic approaches to inventory management optimization (2021) Processes, 9 (1). , https://doi.org/10.3390/pr9010102; Raffin, A., Hill, A., Ernestus, M., Gleave, A., Kanervisto, A., Dormann, N., (2019) Stable Baselines3, , https://github.com/DLR-RM/stable-baselines3; Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O., (2017) Proximal Policy Optimization Algorithms. Arxiv Preprint Arxiv, 1707, p. 06347; Silver, D., Mastering the game of go with deep neural networks and tree search (2016) Nature, 529 (7587), pp. 484-489. , https://doi.org/10.1038/nature16961; Sutton, R., Barto, A., Reinforcement learning, second edition: An introduction (2018) Adaptive Computation and Machine Learning Series, , https://mitpress.mit.edu/books/reinforcement-learning-second-edition, MIT Press},
correspondence_address1={Alves, J.C.; Federal University of LavrasBrazil; email: juliocesar.alves@ufla.br},
editor={Rutkowski L., Scherer R., Korytkowski M., Pedrycz W., Tadeusiewicz R., Zurada J.M.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030878962},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Panzer2021,
author={Panzer, M. and Bender, B.},
title={Deep reinforcement learning in production systems: a systematic literature review},
journal={International Journal of Production Research},
year={2021},
doi={10.1080/00207543.2021.1973138},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115160505&doi=10.1080%2f00207543.2021.1973138&partnerID=40&md5=acc371f7ea1952bb93faf1266fbc934e},
affiliation={Chair of Business Informatics, Processes and Systems, University of Potsdam, Potsdam, Germany},
abstract={Shortening product development cycles and fully customisable products pose major challenges for production systems. These not only have to cope with an increased product diversity but also enable high throughputs and provide a high adaptability and robustness to process variations and unforeseen incidents. To overcome these challenges, deep Reinforcement Learning (RL) has been increasingly applied for the optimisation of production systems. Unlike other machine learning methods, deep RL operates on recently collected sensor-data in direct interaction with its environment and enables real-time responses to system changes. Although deep RL is already being deployed in production systems, a systematic review of the results has not yet been established. The main contribution of this paper is to provide researchers and practitioners an overview of applications and to motivate further implementations and research of deep RL supported production systems. Findings reveal that deep RL is applied in a variety of production domains, contributing to data-driven and flexible processes. In most applications, conventional methods were outperformed and implementation efforts or dependence on human experience were reduced. Nevertheless, future research must focus more on transferring the findings to real-world systems to analyse safety aspects and demonstrate reliability under prevailing conditions. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={Machine learning;  manufacturing processes;  production control;  production planning;  reinforcement learning;  systematic literature review},
keywords={Learning systems;  Reinforcement learning, Conventional methods;  Direct interactions;  Machine learning methods;  Product development cycle;  Production system;  Real time response;  Systematic literature review;  Systematic Review, Deep learning},
references={Altenmüller, T., Stüker, T., Waschneck, B., Kuhnle, A., Lanza, G., Reinforcement Learning for An Intelligent and Autonomous Production Control of Complex Job-Shops Under Time Constraints (2020) Production Engineering, 14 (3), pp. 319-328. , http://doi.org/10.1007/s11740-020-00967-8; Andersen, R.E., Madsen, S., Barlo, A.B.K., Johansen, S.B., Nør, M., Andersen, R.S., Bøgh, S., Self-Learning Processes in Smart Factories: Deep Reinforcement Learning for Process Control of Robot Brine Injection (2019) Procedia Manufacturing, 38, pp. 171-177. , http://doi.org/10.1016/j.promfg.2020.01.023; Antônio Márcio Tavares, T., Felipe Scavarda, L., José Scavarda, A., Conducting Systematic Literature Review in Operations Management (2016) Production Planning & Control, 27 (5), pp. 408-420. , http://doi.org/10.1080/09537287.2015.1129464; Arinez, J.F., Chang, Q., Gao, R.X., Xu, C., Zhang, J., Artificial Intelligence in Advanced Manufacturing: Current Status and Future Outlook (2020) Journal of Manufacturing Science and Engineering, 142 (11). , http://doi.org/10.1115/1.4047855; Baer, S., Bakakeu, J., Meyes, R., Meisen, T., (2019), September. Multi-Agent Reinforcement Learning for Job Shop Scheduling Flexible Manufacturing Systems. 2019 Second International Conference on Artificial Intelligence for Industries (AI4I), Laguna Hills, CA: IEEE, 22–25; Baer, S., Turner, D., Mohanty, P., Samsonov, V., Bakakeu, R., Meisen, T., (2020), Multi Agent Deep Q-Network Approach for Online Job Shop Scheduling Flexible Manufacturing. 2020 International Conference on Manufacturing System and Multiple Machines, Tokyo, Japan, 1–9; Bakakeu, J., Baer, S., Bauer, J., Klos, H.-H., Peschke, J., Fehrle, A., Eberlein, W., An Artificial Intelligence Approach for Online Optimization of Flexible Manufacturing Systems (2018) Applied Mechanics and Materials, 882, pp. 96-108. , http://doi.org/10.4028/www.scientific.net/AMM.882.96; Bakakeu, J., Kisskalt, D., Franke, J., Baer, S., Klos, H.-H., Peschke, J., (2020), August 30–September 2. Multi-Agent Reinforcement Learning for the Energy Optimization of Cyber-Physical Production Systems. 2020 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE), London, ON, Canada. IEEE, 2–8; Bellman, R., (1957) Dynamic Programming, 1. , Princeton: Princeton University Press, vols; Beltran-Hernandez, C.C., Petit, D., Ramirez-Alpizar, I.G., Harada, K., Variable Compliance Control for Robotic Peg-in-Hole Assembly: A Deep-Reinforcement-Learning Approach (2020) Applied Sciences, 10 (19), p. 6923. , http://doi.org/10.3390/app10196923; Beltran-Hernandez, C.C., Petit, D., Ramirez-Alpizar, I.G., Nishi, T., Kikuchi, S., Matsubara, T., Harada, K., Learning Force Control for Contact-Rich Manipulation Tasks With Rigid Position-Controlled Robots (2020) IEEE Robotics and Automation Letters, 5 (4), pp. 5709-5716. , http://doi.org/10.1109/LRA.2020.3010739; Brito, T., Queiroz, J., Piardi, L., Fernandes, L.A., Lima, J., Leitão, P., A Machine Learning Approach for Collaborative Robot Smart Manufacturing Inspection for Quality Control Systems (2020) Procedia Manufacturing, 51, pp. 11-18. , http://doi.org/10.1016/j.promfg.2020.10.003; Cao, D., Hu, W., Zhao, J., Zhang, G., Zhang, B., Liu, Z., Chen, Z., Blaabjerg, F., Reinforcement Learning and Its Applications in Modern Power and Energy Systems: A Review (2020) Journal of Modern Power Systems and Clean Energy, 8 (6), pp. 1029-1042. , http://doi.org/10.35833/MPCE.2020.000552; Chen, N., Luo, S., Dai, J., Luo, B., Gui, W., Optimal Control of Iron-Removal Systems Based on Off-Policy Reinforcement Learning (2020) IEEE Access, 8, pp. 149730-149740. , http://doi.org/10.1109/ACCESS.2020.3015801; Chen, B., Wan, J., Yanting, L., Imran, M., Li, D., Guizani, N., Improving Cognitive Ability of Edge Intelligent IIoT Through Machine Learning (2019) IEEE Network, 33 (5), pp. 61-67. , http://doi.org/10.1109/MNET.001.1800505; Chien, C.-F., Lin, Y.-S., Lin, S.-K., Deep Reinforcement Learning for Selecting Demand Forecast Models to Empower Industry 3.5 and An Empirical Study for a Semiconductor Component Distributor (2020) International Journal of Production Research, 58 (9), pp. 2784-2804. , http://doi.org/10.1080/00207543.2020.1733125; Cooper, H.M., Organizing Knowledge Syntheses: A Taxonomy of Literature Reviews (1988) Knowledge in Society, 1, pp. 104-126; Dai, W., Mo, Z., Luo, C., Jiang, J., Zhang, H., Miao, Q., Fault Diagnosis of Rotating Machinery Based on Deep Reinforcement Learning and Reciprocal of Smoothness Index (2020) IEEE Sensors Journal, 20 (15), pp. 8307-8315. , http://doi.org/10.1109/JSEN.2020.2970747; Ding, Y., Ma, L., Ma, J., Suo, M., Tao, L., Cheng, Y., Lu, C., Intelligent Fault Diagnosis for Rotating Machinery Using Deep Q-Network Based Health State Classification: A Deep Reinforcement Learning Approach (2019) Advanced Engineering Informatics, 42. , http://doi.org/10.1016/j.aei.2019.100977; Dong, T., Xue, F., Xiao, C., Li, J., Task Scheduling Based on Deep Reinforcement Learning in a Cloud Manufacturing Environment (2020) Concurrency and Computation: Practice and Experience, 32 (11), p. e5654. , http://doi.org/10.1002/cpe.5654; Dornheim, J., Link, N., Gumbsch, P., Model-Free Adaptive Optimal Control of Episodic Fixed-horizon Manufacturing Processes Using Reinforcement Learning (2020) International Journal of Control, Automation and Systems, 18 (6), pp. 1593-1604. , http://doi.org/10.1007/s12555-019-0120-7; Durach, C.F., Kembro, J., Wieland, A., A New Paradigm for Systematic Literature Reviews in Supply Chain Management (2017) Journal of Supply Chain Management, 53 (4), pp. 67-85. , http://doi.org/10.1111/jscm.12145; Epureanu, B.I., Li, X., Nassehi, A., Koren, Y., Self-Repair of Smart Manufacturing Systems by Deep Reinforcement Learning (2020) CIRP Annals, 69 (1), pp. 421-424. , http://doi.org/10.1016/j.cirp.2020.04.008; Feldkamp, N., Bergmann, S., Strassburger, S., (2020), Simulation-Based Deep Reinforcement Learning for Modular Production Systems. Proceedings of the 2020 Winter Simulation Conference, 1596–1607. IEEE; Gabel, T., Riedmiller, M., Adaptive Reactive Job-Shop Scheduling with Reinforcement Learning Agents (2007) International Journal of Information Technology and Intelligent Computing, 24 (4), pp. 14-18; Ge, Y., Zhu, F., Ling, X., Liu, Q., Safe Q-Learning Method Based on Constrained Markov Decision Processes (2019) IEEE Access, 7, pp. 165007-165017. , http://doi.org/10.1109/ACCESS.2019.2952651; Greenhalgh, T., Peacock, R., Effectiveness and Efficiency of Search Methods in Systematic Reviews of Complex Evidence: Audit of Primary Sources (2005) BMJ (Clinical Research Ed.), 331 (7524), pp. 1064-1065. , http://doi.org/10.1136/bmj.38636.593461.68; Günther, J., Pilarski, P.M., Helfrich, G., Shen, H., Diepold, K., Intelligent Laser Welding Through Representation, Prediction, and Control Learning: An Architecture with Deep Neural Networks and Reinforcement Learning (2016) Mechatronics, 34, pp. 1-11. , http://doi.org/10.1016/j.mechatronics.2015.09.004; Guo, L., Wang, H., Zhang, J., (2019), August 10–12. Data-Driven Grinding Control Using Reinforcement Learning. 2019 IEEE 21st International Conference on High Performance Computing and Communications, Zhangjiajie, China. IEEE; Guo, F., Zhou, X., Liu, J., Zhang, Y., Li, D., Zhou, H., A Reinforcement Learning Decision Model for Online Process Parameters Optimization From Offline Data in Injection Molding (2019) Applied Soft Computing, 85. , http://doi.org/10.1016/j.asoc.2019.105828; Han, B.-A., Yang, J.-J., Research on Adaptive Job Shop Scheduling Problems Based on Dueling Double DQN (2020) IEEE Access, 8, pp. 186474-186495. , http://doi.org/10.1109/ACCESS.2020.3029868; He, Z., Tran, K.-P., Thomassey, S., Zeng, X., Xu, J., Yi, C., A Deep Reinforcement Learning Based Multi-Criteria Decision Support System for Optimizing Textile Chemical Process (2020) Computers in Industry, 125. , http://doi.org/10.1016/j.compind.2020.103373; Heger, J., Voβ, T., (2020), Dynamically Changing Sequencing Rules Wirth Reinforcement Learning a Job Shop System with Stochastic Influences. Proceedings of the 2020 Winter Simulation Conference 1608–1618. Orlando, FL: IEEE; Hildebrand, M., Andersen, R.S., Bøgh, S., Deep Reinforcement Learning for Robot Batching Optimization and Flow Control (2020) Procedia Manufacturing, 51, pp. 1462-1468. , http://doi.org/10.1016/j.promfg.2020.10.203; Hoppe, S., Lou, Z., Hennes, D., Toussaint, M., Planning Approximate Exploration Trajectories for Model-Free Reinforcement Learning in Contact-Rich Manipulation (2019) IEEE Robotics and Automation Letters, 4 (4), pp. 4042-4047. , http://doi.org/10.1109/LRA.2019.2928212; Hu, H., Jia, X., He, Q., Fu, S., Liu, K., Deep Reinforcement Learning Based AGVs Real-Time Scheduling with Mixed Rule for Flexible Shop Floor in Industry 4.0 (2020) Computers & Industrial Engineering, 149. , http://doi.10.1016/j.cie.2020.106749; Hu, L., Liu, Z., Hu, W., Wang, Y., Tan, J., Wu, F., Petri-Net-Based Dynamic Scheduling of Flexible Manufacturing System Via Deep Reinforcement Learning with Graph Convolutional Network (2020) Journal of Manufacturing Systems, 55, pp. 1-14. , http://doi.org/10.1016/j.jmsy.2020.02.004; Hu, W., Sun, Z., Zhang, Y., Li, Y., Joint Manufacturing and Onsite Microgrid System Control Using Markov Decision Process and Neural Network Integrated Reinforcement Learning (2019) Procedia Manufacturing, 39, pp. 1242-1249. , http://doi.org/10.1016/j.promfg.2020.01.345; Huang, J., Chang, Q., Arinez, J., Deep Reinforcement Learning Based Preventive Maintenance Policy for Serial Production Lines (2020) Expert Systems with Applications, 160. , http://doi.org/10.1016/j.eswa.2020.113701; Huang, X., Hong, S.H., Yu, M., Ding, Y., Jiang, J., Demand Response Management for Industrial Facilities: A Deep Reinforcement Learning Approach (2019) IEEE Access, 7, pp. 82194-82205. , http://doi.org/10.1109/ACCESS.2019.2924030; Hubbs, C.D., Li, C., Sahinidis, N.V., Grossmann, I.E., Wassick, J.M., A Deep Reinforcement Learning Approach for Chemical Production Scheduling (2020) Computers & Chemical Engineering, 141. , http://doi.10.1016/j.compchemeng.2020.106982; Inoue, T., Magistris, G.D., Munawar, A., Yokoya, T., Tachibana, R., (2017), Deep Reinforcement Learning for High Precision Assembly Tasks. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Vancouver, BC, September 24–28; Jiang, Y., Fan, J., Chai, T., Lewis, F.L., Dual-Rate Operational Optimal Control for Flotation Industrial Process With Unknown Operational Model (2019) IEEE Transactions on Industrial Electronics, 66 (6), pp. 4587-4599. , http://doi.org/10.1109/TIE.2018.2856198; Jiang, Y., Fan, J., Chai, T., Li, J., Lewis, F.L., Data-Driven Flotation Industrial Process Operational Optimal Control Based on Reinforcement Learning (2018) IEEE Transactions on Industrial Informatics, 14 (5), pp. 1974-1989. , http://doi.org/10.1109/TII.2017.2761852; Jin, Z., Hongming Gao, H.L., An Intelligent Weld Control Strategy Based on Reinforcement Learning Approach (2019) The International Journal of Advanced Manufacturing Technology, 100 (9-12), pp. 2163-2175. , http://doi.org/10.1007/s00170-018-2864-2; Kagermann, H., Wahlster, W., Helbig, J., (2013), Recommendations for Implementing the Strategic Initiative INDUSTRIE 4.0Securing the Future of German Manufacturing Industry,. AcatechNational Academy of Science and Engineering; Kang, Z., Catal, C., Tekinerdogan, B., Machine Learning Applications in Production Lines: A Systematic Literature Review (2020) Computers & Industrial Engineering, 149. , http://doi.org/10.1016/j.cie.2020.106773; Khan, A.-M., Jaowad Khan, R., Tooshil, A., Sikder, N., Mahmud, M.A.P., Kouzani, A.Z., Nahid, A.-A., A Systematic Review on Reinforcement Learning-Based Robotics Within the Last Decade (2020) IEEE Access, 8, pp. 176598-176623. , http://doi.10.1109/ACCESS.2020.3027152; Kim, Y.-L., Ahn, K.-H., Song, J.-B., Reinforcement Learning Based on Movement Primitives For Contact Tasks (2020) Robotics and Computer-Integrated Manufacturing, 62. , http://doi.org/10.1016/j.rcim.2019.101863; Kim, J.-B., Choi, H.-B., Hwang, G.-Y., Kim, K., Hong, Y.-G., Han, Y.-H., Sortation Control Using Multi-Agent Deep Reinforcement Learning in N-Grid Sortation System (2020) Sensors, 20 (12), p. 3401. , http://doi.10.3390/s20123401; Kuhnle, A., (2020), https://github.com/AndreasKuhnle/SimRLFab, SimRLFab: Simulation and Reinforcement Learning Framework for Production Planning and Control of Complex Job Shop Manufacturing Systems,. GitHub (accessed 20 March 2021; Kuhnle, A., Jakubik, J., Lanza, G., Reinforcement Learning for Opportunistic Maintenance Optimization (2019) Production Engineering, 13 (1), pp. 33-41. , http://doi.org/10.1007/s11740-018-0855-7; Kuhnle, A., Kaiser, J.-P., Theiβ, F., Stricker, N., Lanza, G., Designing An Adaptive Production Control System Using Reinforcement Learning (2020) Journal of Intelligent Manufacturing, 32, pp. 855-876. , http://doi.org/10.1007/s10845-020-01612-y; Kumar, A., Dimitrakopoulos, R., Maulen, M., Adaptive Self-Learning Mechanisms for Updating Short-Term Production Decisions in An Industrial Mining Complex (2020) Journal of Intelligent Manufacturing, 31 (7), pp. 1795-1811. , http://doi.org/10.1007/s10845-020-01562-5; Lämmle, A., König, T., El-Shamouty, M., Huber, M.F., Skill-Based Programming of Force-controlled Assembly Tasks Using Deep Reinforcement Learning (2020) Procedia CIRP, 93, pp. 1061-1066. , http://doi.org/10.1016/j.procir.2020.04.153; Lange, S., Riedmiller, M., Voigtlander, A., (2012), Autonomous Reinforcement Learning on Raw Visual Input Data A Real World Application. The 2012 International Joint Conference on Neural Networks (IJCNN), Brisbane, Australia, Juni 10–15; Lee, S., Cho, Y., Lee, Y.H., Injection Mold Production Sustainable Scheduling Using Deep Reinforcement Learning (2020) Sustainability, 12 (20), p. 8718. , http://doi.org/10.3390/su12208718; Lee, J.-H., Kim, H.-J., Reinforcement Learning for Robotic Flow Shop Scheduling with Processing Time Variations (2021) International Journal of Production Research, pp. 1-23. , https://doi.org/10.1080/00207543.2021.1887533; Lee, J.H., Shin, J., Realff, M.J., Machine Learning: Overview of the Recent Progresses and Implications for the Process Systems Engineering Field (2018) Computers & Chemical Engineering, 114, pp. 111-121. , http://doi.org/10.1016/j.compchemeng.2017.10.008; Lei, L., Tan, Y., Zheng, K., Liu, S., Zhang, K., Shen, X., Deep Reinforcement Learning for Autonomous Internet of Things: Model, Applications and Challenges (2020) IEEE Communications Surveys & Tutorials, 22 (3), pp. 1722-1760. , http://doi.org/10.1109/COMST.2020.2988367; Leng, J., Jin, C., Vogl, A., Liu, H., Deep Reinforcement Learning for A Color-Batching Resequencing Problem (2020) Journal of Manufacturing Systems, 56, pp. 175-187. , http://doi.org/10.1016/j.jmsy.2020.06.001; Leng, J., Ruan, G., Song, Y., Liu, Q., Fu, Y., Ding, K., Chen, X., A Loosely-Coupled Deep Reinforcement Learning Approach for Order Acceptance Decision of Mass-Individualized Printed Circuit Board Manufacturing in Industry 4.0 (2021) Journal of Cleaner Production, 280. , http://doi.org/10.1016/j.jclepro.2020.124405; Li, J., Ding, J., Chai, T., Lewis, F.L., Nonzero-Sum Game Reinforcement Learning for Performance Optimization in Large-Scale Industrial Processes (2020) IEEE Transactions on Cybernetics, 50 (9), pp. 4132-4145. , http://doi.org/10.1109/TCYB.2019.2950262; Li, F., Jiang, Q., Quan, W., Cai, S., Song, R., Li, Y., Manipulation Skill Acquisition for Robotic Assembly Based on Multi-Modal Information Description (2019) IEEE Access, 8, pp. 6282-6294. , http://doi.org/10.1109/ACCESS.2019.2934174; Li, F., Jiang, Q., Zhang, S., Wei, M., Song, R., Robot Skill Acquisition in Assembly Process Using Deep Reinforcement Learning (2019) Neurocomputing, 345, pp. 92-102. , http://doi.org/10.1016/j.neucom.2019.01.087; Li, B., Zhang, H., Ye, P., Wang, J., Trajectory Smoothing Method Using Reinforcement Learning for Computer Numerical Control Machine Tools (2020) Robotics and Computer-Integrated Manufacturing, 61. , http://doi.org/10.1016/j.rcim.2019.101847; Liang, H., Wen, X., Liu, Y., Zhang, H., Zhang, L., Wang, L., Logistics-Involved QoS-Aware Service Composition in Cloud Manufacturing With Deep Reinforcement Learning (2021) Robotics and Computer-Integrated Manufacturing, 67. , http://doi.org/10.1016/j.rcim.2020.101991; Liao, Y., Deschamps, F., Freitas Rocha Loures, E.D., Felipe Pierin Ramos, L., Past, Present and Future of Industry 4.0–A Systematic Literature Review and Research Agenda Proposal (2017) International Journal of Production Research, 55 (12), pp. 3609-3629. , http://doi.org/10.1080/00207543.2017.1308576; Liao, H., Zhang, W., Dong, X., Poczos, B., Shimada, K., Burak Kara, L., A Deep Reinforcement Learning Approach for Global Routing (2020) Journal of Mechanical Design, 142 (6). , http://doi.org/10.1115/1.4045044; Light, R.J., Pillemer, D.B., (1984) Summing Up: The Science of Reviewing Research, , Cambridge: Harvard University Press; Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., Wierstra, D., (2016), Continuous Control with Deep Reinforcement Learning. Proceedings of 4th International Conference on Learning Representations; Lin, C.-C., Deng, D.-J., Chih, Y.-L., Chiu, H.-T., Smart Manufacturing Scheduling With Edge Computing Using Multiclass Deep Q Network (2019) IEEE Transactions on Industrial Informatics, 15 (7), pp. 4276-4284. , http://doi.org/10.1109/TII.2019.2908210; Liu, C.-L., Chang, C.-C., Tseng, C.-J., Actor-Critic Deep Reinforcement Learning for Solving Job Shop Scheduling Problems (2020) IEEE Access, 8, pp. 71752-71762. , http://doi.org/10.1109/ACCESS.2020.2987820; Liu, Y., Chen, Y., Jiang, T., Dynamic Selective Maintenance Optimization for Multi-State Systems Over a Finite Horizon: A Deep Reinforcement Learning Approach (2020) European Journal of Operational Research, 283 (1), pp. 166-181. , http://doi.org/10.1016/j.ejor.2019.10.049; Liu, X., Xu, H., Liao, W., Yu, W., (2019), Reinforcement Learning for Cyber-Physical Systems. 2019 IEEE International Conference on Industrial Internet (ICII), Orlando, FL, USA, November 11–12; Liu, X., Yu, W., Liang, F., Griffith, D., Golmie, N., On Deep Reinforcement Learning Security for Industrial Internet of Things (2021) Computer Communications, 168, pp. 20-32. , http://doi.org/10.1016/j.comcom.2020.12.013; Lohmer, J., Lasch, R., Production Planning and Scheduling in Multi-Factory Production Networks: A Systematic Literature Review (2020) International Journal of Production Research, 59 (7), pp. 2028-2054. , http://doi.org/10.1080/00207543.2020.1797207; Lu, X., Kiumarsi, B., Chai, T., Lewis, F.L., Data-Driven Optimal Control of Operational Indices for A Class of Industrial Processes (2016) IET Control Theory & Applications, 10 (12), pp. 1348-1356. , http://doi.org/10.1049/iet-cta.2015.0798; Lu, R., Li, Y.-C., Li, Y., Jiang, J., Ding, Y., Multi-Agent Deep Reinforcement Learning Based Demand Response for Discrete Manufacturing Systems Energy Management (2020) Applied Energy, 276. , http://doi.org/10.1016/j.apenergy.2020.115473; Luo, S., Dynamic Scheduling for Flexible Job Shop with New Job Insertions by Deep Reinforcement Learning (2020) Applied Soft Computing, 91. , http://doi.org/10.1016/j.asoc.2020.106208; Luo, J., Solowjow, E., Wen, C., Aparicio Ojea, J., Agogino, A.M., (2018), Deep Reinforcement Learning for Robotic Assembly of Mixed Deformable and Rigid Objects. 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Madrid, Spain, October 1–5; Luo, J., Solowjow, E., Wen, C., Aparicio Ojea, J., Agogino, A.M., Tamar, A., Abbeel, P., (2019), Reinforcement Learning on Variable Impedance Controller for High-Precision Robotic Assembly. 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20–24; Luong, N.C., Thai Hoang, D., Gong, S., Niyato, D., Wang, P., Liang, Y.-C., Kim, D.I., Applications of Deep Reinforcement Learning in Communications and Networking: A Survey (2019) IEEE Communications Surveys Tutorials, 21 (4), pp. 3133-3174. , http://doi.org/10.1109/COMST.2019.2916583; Ma, Y., Zhu, W., Benton, M.G., Romagnoli, J., Continuous Control of A Polymerization System with Deep Reinforcement Learning (2019) Journal of Process Control, 75, pp. 40-47. , http://doi.org/10.1016/j.jprocont.2018.11.004; Mahadevan, S., Theocharous, G., (1998), Optimizing Production Manufacturing Using Reinforcement Learning. Proceedings of the Eleventh International FLAIRS Conference, 372–377; Malus, A., Kozjek, D., Vrabič, R., Real-Time Order Dispatching for A Fleet of Autonomous Mobile Robots Using Multi-Agent Reinforcement Learning (2020) CIRP Annals, 69 (1), pp. 397-400. , http://doi.org/10.1016/j.cirp.2020.04.001; Marc-André, D., Fohlmeister, S., Cooperative Multi-Agent System for Production Control Using Reinforcement Learning (2020) CIRP Annals, 69 (1), pp. 389-392. , http://doi.org/10.1016/j.cirp.2020.04.005; Masinelli, G., Le-Quang, T., Zanoli, S., Wasmer, K., Shevchik, S.A., Adaptive Laser Welding Control: A Reinforcement Learning Approach (2020) IEEE Access, 8, pp. 103803-103814. , http://doi.org/10.1109/ACCESS.2020.2998052; Mazgualdi, C.E., Masrour, T., Hassani, I.E., Khdoudi, A., (2021), A Deep Reinforcement Learning (DRL) Decision Model for Heating Process Parameters Identification Automotive Glass Manufacturing. Artificial Intelligence and Industrial Applications,  1193, 77–87. Cham: Springer International Publishing; Miljković, Z., Mitić, M., Lazarević, M., Babić, B., Neural Network Reinforcement Learning for Visual Control of Robot Manipulators (2013) Expert Systems with Applications, 40 (5), pp. 1721-1736. , http://doi.org/10.1016/j.eswa.2012.09.010; Mishra, M., Nayak, J., Naik, B., Abraham, A., Deep Learning in Electrical Utility Industry: A Comprehensive Review of A Decade of Research (2020) Engineering Applications of Artificial Intelligence, 96. , http://doi.org/10.1016/j.engappai.2020.104000; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., (2013), Playing Atari with Deep Reinforcement Learning; Mohammed, M.Q., Chung, K.L., Chyi Chua, S., Review of Deep Reinforcement Learning-Based Object Grasping: Techniques, Open Challenges, and Recommendations (2020) IEEE Access, 8, pp. 178450-178481. , http://doi.org/10.1109/ACCESS.2020.3027923; Mosavi, A., Faghan, Y., Ghamisi, P., Duan, P., Faizollahzadeh Ardabili, S., Salwana, E., Band, S.S., Comprehensive Review of Deep Reinforcement Learning Methods and Applications in Economics (2020) Mathematics, 8 (10), p. 1640. , http://doi.org/10.3390/math8101640; Naeem, M., Rizvi, S.T.H., Coronato, A., A Gentle Introduction to Reinforcement Learning and Its Application in Different Fields (2020) IEEE Access, 8, pp. 209320-209344. , http://doi.org/10.1109/ACCESS.2020.3038605; Nguyen, H., La, H., (2019), Review of Deep Reinforcement Learning for Robot Manipulation. 2019 Third IEEE International Conference on Robotic Computing (IRC), Naples, Italy, February 25–27; Noel, M.M., Pandian, B.J., Control of a Nonlinear Liquid Level System Using a New Artificial Neural Network Based Reinforcement Learning Approach (2014) Applied Soft Computing, 23, pp. 444-451. , http://doi.org/10.1016/j.asoc.2014.06.037; Oh, T.-H., Han, J.-S., Kim, Y.-S., Yang, D.-Y., Lee, S.-H., ‘Dan’ Cho, D.-I., Deep RL Based Notch Filter Design Method for Complex Industrial Servo Systems (2020) International Journal of Control, Automation and Systems, 18 (12), pp. 2983-2992. , http://doi.org/10.1007/s12555-020-0153-y; Palombarini, J.A., Martinez, E.C., (2018), Automatic Generation of Rescheduling Knowledge Socio-Technical Manufacturing Systems using Deep Reinforcement Learning. 2018 IEEE Biennial Congress of Argentina (ARGENCON), San Miguel de Tucumán, Argentina, June 6–8; Palombarini, J.A., Martínez, E.C., Closed-loop Rescheduling Using Deep Reinforcement Learning (2019) IFAC-PapersOnLine, 52 (1), pp. 231-236. , http://doi.org/10.1016/j.ifacol.2019.06.067; Pandian, B.J., Noel, M.M., Tracking Control of a Continuous Stirred Tank Reactor Using Direct and Tuned Reinforcement Learning Based Controllers (2018) Chemical Product and Process Modeling, 13 (3), pp. 1-10. , https://doi.org/10.1515/cppm-2017-0040; Park, J., Chun, J., Hun Kim, S., Kim, Y., Park, J., Learning to Schedule Job-Shop Problems: Representation and Policy Learning Using Graph Neural Network and Reinforcement Learning (2021) International Journal of Production Research, 59 (11), pp. 3360-3377. , http://doi.org/10.1080/00207543.2020.1870013; Park, I.-B., Huh, J., Kim, J., Park, J., A Reinforcement Learning Approach to Robust Scheduling of Semiconductor Manufacturing Facilities (2020) IEEE Transactions on Automation Science and Engineering, 17 (3), pp. 1420-1431. , http://doi.org/10.1109/TASE.2019.2956762; Park, J., Lee, S., Lee, J., Um, J., GadgetArm–Automatic Grasp Generation and Manipulation of 4-DOF Robot Arm for Arbitrary Objects Through Reinforcement Learning (2020) Sensors, 20 (21), p. 6183. , http://doi.org/10.3390/s20216183; Peres, R.S., Jia, X., Lee, J., Sun, K., Walter Colombo, A., Barata, J., Industrial Artificial Intelligence in Industry 4.0–Systematic Review, Challenges and Outlook (2020) IEEE Access, 8, pp. 220121-220139. , http://doi.org/10.1109/ACCESS.2020.3042874; Petticrew, M., Roberts, H., (2006) Systematic Reviews in the Social Sciences, , Oxford: Blackwell Publishing Ltd; Powell, B.K.M., Machalek, D., Quah, T., Real-Time Optimization Using Reinforcement Learning (2020) Computers & Chemical Engineering, 143. , http://doi.org/10.1016/j.compchemeng.2020.107077; Quah, T., Machalek, D., Powell, K.M., Comparing Reinforcement Learning Methods for Real-Time Optimization of a Chemical Process (2020) Processes, 8 (11), p. 1497. , http://doi.org/10.3390/pr8111497; Riedmiller, S., Riedmiller, M., A Neural Reinforcement Learning Approach to Learn Local Dispatching Policies in Production Scheduling (1999) Proceedings of the 16th International Joint Conference on Artificial Intelligence, 2, pp. 764-769; Rossit, D.A., Tohmé, F., Frutos, M., Industry 4.0: Smart Scheduling (2019) International Journal of Production Research, 57 (12), pp. 3802-3813. , http://doi.org/10.1080/00207543.2018.1504248; Rummukainen, H., Nurminen, J.K., Practical Reinforcement Learning -Experiences in Lot Scheduling Application (2019) IFAC-PapersOnLine, 52 (13), pp. 1415-1420. , http://doi.org/10.1016/j.ifacol.2019.11.397; Samsonov, V., Enslin, C., Köpken, H.-G., Baer, S., Lütticke, D., (2020), http://doi.org/10.5220/0009354105060514, Using Reinforcement Learning for Optimization of a Workpiece Clamping Position a Machine Tool. Proceedings of the 22nd International Conference on Enterprise Information Systems, 506–514; Schaul, T., Quan, J., Antonoglou, I., Silver, D., (2016), Prioritized Experience Replay. International Conference on Learning Representations, San Juan, Puerto Rico, May; Scheiderer, C., Thun, T., Idzik, C., Felipe Posada-Moreno, A., Krämer, A., Lohmar, J., Hirt, G., Meisen, T., Simulation-as-a-Service for Reinforcement Learning Applications by Example of Heavy Plate Rolling Processes (2020) Procedia Manufacturing, 51, pp. 897-903. , http://doi.org/10.1016/j.promfg.2020.10.126; Scheiderer, C., Thun, T., Meisen, T., Bézier Curve Based Continuous and Smooth Motion Planning for Self-Learning Industrial Robots (2019) Procedia Manufacturing, 38, pp. 423-430. , http://doi.org/10.1016/j.promfg.2020.01.054; Schmidt, A., Schellroth, F., Riedel, O., (2020), http://doi.org/10.1145/3378184.3378198, Control Architecture for Embedding Reinforcement Learning Frameworks on Industrial Control Hardware. Proceedings of the 3rd International Conference on Applications of Intelligent Systems, Las Palmas de Gran Canaria Spain, January 7–12; Schoettler, G., Nair, A., Luo, J., Bahl, S., Aparicio Ojea, J., Solowjow, E., Levine, S., (2020), http://doi.org/10.1109/IROS45743.2020.9341714, Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards. 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Las Vegas, Nevada, USA, October 25-29; Serin, G., Sener, B., Ozbayoglu, M., Ozgur Unver, H., Review of Tool Condition Monitoring in Machining and Opportunities for Deep Learning (2020) The International Journal of Advanced Manufacturing Technology, 109 (3-4), pp. 953-974. , http://doi.org/10.1007/s00170-020-05449-w; Sewak, M., (2019), Policy-Based Reinforcement Learning Approaches: Stochastic Policy Gradient and the REINFORCE Algorithm. Deep Reinforcement Learning, 127–140. Singapore: Springer Singapore; Sewak, M., (2019) Deep Reinforcement Learning: Frontiers of Artificial Intelligence, , 1st ed, Singapore: Springer Singapore; Shi, D., Fan, W., Xiao, Y., Lin, T., Xing, C., Intelligent Scheduling of Discrete Automated Production Line Via Deep Reinforcement Learning (2020) International Journal of Production Research, 58 (11), pp. 3362-3380. , http://doi.org/10.1080/00207543.2020.1717008; Shyalika, C., Silva, T., Karunananda, A., Reinforcement Learning in Dynamic Task Scheduling: A Review (2020) SN Computer Science, 1 (6), p. 306. , http://doi.org/10.1007/s42979-020-00326-5; Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., (2017), Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm. [cs]; Skordilis, E., Moghaddass, R., A Deep Reinforcement Learning Approach for Real-Time Sensor-Driven Decision Making and Predictive Analytics (2020) Computers & Industrial Engineering, 147, pp. 106-600. , http://doi.org/10.1016/j.cie.2020.106600; Spielberg, S., Gopaluni, R.B., Loewen, P.D., (2017), Deep Reinforcement Learning Approaches for Process Control. 2017 6th International Symposium on Advanced Control of Industrial Processes, Taipei, Taiwan, May 28–31; Spielberg, S., Tulsyan, A., Lawrence, N.P., Loewen, P.D., Gopaluni, R.B., Toward Self-Driving Processes: A Deep Reinforcement Learning Approach to Control (2019) AIChE Journal, 65 (10), pp. 16-689. , http://doi.org/10.1002/aic.16689; Stricker, N., Kuhnle, A., Sturm, R., Friess, S., Reinforcement Learning for Adaptive Order Dispatching in The Semiconductor Industry (2018) CIRP Annals, 67 (1), pp. 511-514. , http://doi.org/10.1016/j.cirp.2018.04.041; Sutton, R.S., Barto, A.G., (2017) Reinforcement Learning: An Introduction, , 2nd ed, Cambridge: The MIT Press; Szarski, M., Chauhan, S., Composite Temperature Profile and Tooling Optimization Via Deep Reinforcement Learning (2021) Composites Part A: Applied Science and Manufacturing, 142, pp. 106-235. , http://doi.org/10.1016/j.compositesa.2020.106235; Tewari, A., Liu, K.-H., Papageorgiou, D., Information-Theoretic Sensor Planning for Large-Scale Production Surveillance Via Deep Reinforcement Learning (2020) Computers & Chemical Engineering, 141, pp. 106-988. , http://doi.org/10.1016/j.compchemeng.2020.106988; Tomé, S., Azevedo, A., Production Flow Control Through the Use of Reinforcement Learning (2019) Procedia Manufacturing, 38, pp. 194-202. , http://doi.org/10.1016/j.promfg.2020.01.026; Tranfield, D., Denyer, D., Smart, P., Towards a Methodology for Developing Evidence-Informed Management Knowledge by Means of Systematic Review (2003) British Journal of Management, 14 (3), pp. 207-222. , http://doi.org/10.1111/1467-8551.00375; Tsai, Y.-T., Lee, C.-H., Liu, T.-Y., Chang, T.-J., Wang, C.-S., Pawar, S.J., Huang, P.-H., Huang, J.-H., Utilization of A Reinforcement Learning Algorithm for the Accurate Alignment of a Robotic Arm in a Complete Soft Fabric Shoe Tongues Automation Process (2020) Journal of Manufacturing Systems, 56, pp. 501-513. , http://doi.org/10.1016/j.jmsy.2020.07.001; van Hasselt, H., Guez, A., Silver, D., (2016), http://doi.org/10.5555/3016100.3016191, Deep Reinforcement Learning with Double Q-learning. Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 2094–2100; vom Brocke, J., Simons, A., Niehaves, B., Riemer, K., Plattfaut, R., Cleven, A., (2009), Reconstructing the Giant: On the Importance of Rigour Documenting the Literature Search Process. Proceedings of the 17th European Conference on Information Systems (ECIS). Verona: Università di Verona, 2206–2217; Wang, K., Kang, B., Shao, J., Feng, J., (2020), Improving Generalization Reinforcement Learning with Mixture Regularization. 34th Conference on Neural Information Processing Systems, Vancouver, Canada, December 6–12; Wang, J.P., Kang Shi, Y., Sheng Zhang, W., Thomas, I., Hui Duan, S., Multitask Policy Adversarial Learning for Human-Level Control With Large State Spaces (2019) IEEE Transactions on Industrial Informatics, 15 (4), pp. 2395-2404. , http://doi.org/10.1109/TII.2018.2881266; Wang, H.-N., Liu, N., Zhang, Y.-Y., Feng, D.-W., Huang, F., Li, D.-S., Zhang, Y.-M., Deep Reinforcement Learning: A Survey (2020) Frontiers of Information Technology & Electronic Engineering, 21 (12), pp. 1726-1744. , http://doi.org/10.1631/FITEE.1900533; Wang, H., Sarker, B.R., Li, J., Li, J., Adaptive Scheduling for Assembly Job Shop with Uncertain Assembly Times Based on Dual Q-Learning (2020) International Journal of Production Research, pp. 1-17. , https://doi.org/10.1080/00207543.2020.1794075; Wang, F., Zhou, X., Wang, J., Zhang, X., He, Z., Song, B., Joining Force of Human Muscular Task Planning With Robot Robust and Delicate Manipulation for Programming by Demonstration (2020) IEEE/ASME Transactions on Mechatronics, 25 (5), pp. 2574-2584. , http://doi.org/10.1109/TMECH.2020.2997799; Waschneck, B., Reichstaller, A., Belzner, L., Altenmüller, T., Bauernhansl, T., Knapp, A., Kyek, A., Optimization of Global Production Scheduling with Deep Reinforcement Learning (2018) Procedia CIRP, 72, pp. 1264-1269. , http://doi.org/10.1016/j.procir.2018.03.212; Waschneck, B., Reichstaller, A., Belzner, L., Altenmuller, T., Bauernhansl, T., Knapp, A., Kyk, A., (2018), Deep Reinforcement Learning for Semiconductor Production Scheduling. 2018 29th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC), Saratoga Springs, NY, USA, April 30May 3; Wasmer, K., Le-Quang, T., Meylan, B., Shevchik, S., In Situ Quality Monitoring in AM Using Acoustic Emission: A Reinforcement Learning Approach (2019) Journal of Materials Engineering and Performance, 28 (2), pp. 666-672. , http://doi.org/10.1007/s11665-018-3690-2; Watanabe, K., Inada, S., Search Algorithm of the Assembly Sequence of Products by Using Past Learning Results (2020) International Journal of Production Economics, 226, pp. 107-615. , http://doi.org/10.1016/j.ijpe.2020.107615; Watkins, C.J.C.H., Dayan, P., Q-learning (1992) Machine Learning, 8 (3-4), pp. 279-292. , http://doi.org/10.1007/BF00992698; Webster, J., Watson, R.T., Analyzing the Past to Prepare for the Future: Writing a Literature Review (2002) MIS Quarterly, 26 (2), pp. 13-23. , –,. Publisher: Management Information Systems Research Center, University of Minnesota; Wu, W., Huang, Z., Zeng, J., Fan, K., A Fast Decision-Making Method for Process Planning with Dynamic Machining Resources Via Deep Reinforcement Learning (2021) Journal of Manufacturing Systems, 58, pp. 392-411. , http://doi.org/10.1016/j.jmsy.2020.12.015; Wu, C.-X., Liao, M.-H., Karatas, M., Chen, S.-Y., Zheng, Y.-J., Real-Time Neural Network Scheduling of Emergency Medical Mask Production During COVID-19 (2020) Applied Soft Computing, 97, pp. 106-790. , http://doi.org/10.1016/j.asoc.2020.106790; Xanthopoulos, A.S., Kiatipis, A., Koulouriotis, D.E., Stieger, S., Reinforcement Learning-Based and Parametric Production-Maintenance Control Policies for a Deteriorating Manufacturing System (2018) IEEE Access, 6, pp. 576-588. , http://doi.org/10.1080/00207543.2021.1887533; Xia, K., Sacco, C., Kirkpatrick, M., Saidy, C., Nguyen, L., Kircaliali, A., Harik, R., A Digital Twin to Train Deep Reinforcement Learning Agent for Smart Manufacturing Plants: Environment, Interfaces and Intelligence (2020) Journal of Manufacturing Systems, 58, pp. 210-230. , http://doi.org/10.1016/j.jmsy.2020.06.012; Xie, S., Zhang, T., Rose, O., (2019), Online Single Machine Scheduling Based on Simulation and Reinforcement Learning. 18. ASIM Fachtagung Simulation Production und Logistik, Chemnitz, September 19–20; Xiong, H., Diao, X., Safety Robustness of Reinforcement Learning Policies: A View From Robust Control (2021) Neurocomputing, 422, pp. 12-21. , http://doi.org/10.1016/j.neucom.2020.09.055; Xu, J., Hou, Z., Wang, W., Xu, B., Zhang, K., Chen, K., Feedback Deep Deterministic Policy Gradient With Fuzzy Reward for Robotic Multiple Peg-in-Hole Assembly Tasks (2019) IEEE Transactions on Industrial Informatics, 15 (3), pp. 1658-1667. , http://doi.org/10.1109/TII.2018.2868859; Xu, X., Xie, H., Shi, J., (2020), Iterative Learning Control (ILC) Guided Reinforcement Learning Control (RLC) Scheme for Batch Processes. 2020 IEEE 9th Data Driven Control and Learning Systems Conference (DDCLS), Liuzhou, Juni 19–21; Xu, L.D., Xu, E.L., Li, L., Industry 4.0: State of The Art and Future Trends (2018) International Journal of Production Research, 56 (8), pp. 2941-2962. , http://doi.org/10.1080/00207543.2018.1444806; Yang, H., Alphones, A., Zhong, W.-D., Chen, C., Xie, X., Learning-Based Energy-Efficient Resource Management by Heterogeneous RF/VLC for Ultra-Reliable Low-Latency Industrial IoT Networks (2020) IEEE Transactions on Industrial Informatics, 16 (8), pp. 5565-5576. , http://doi.org/10.1109/TII.2019.2933867; Yoo, H., Kim, B., Woo Kim, J., Lee, J.H., Reinforcement Learning Based Optimal Control of Batch Processes Using Monte-Carlo Deep Deterministic Policy Gradient with Phase Segmentation (2021) Computers & Chemical Engineering, 144, pp. 107-133. , http://doi.org/10.1016/j.compchemeng.2020.107133; Yu, J., Guo, P., Run-to-Run Control of Chemical Mechanical Polishing Process Based on Deep Reinforcement Learning (2020) IEEE Transactions on Semiconductor Manufacturing, 33 (3), pp. 454-465. , http://doi.org/10.1109/TSM.2020.3002896; Yu, T., Huang, J., Chang, Q., Mastering the Working Sequence in Human–Robot Collaborative Assembly Based on Reinforcement Learning (2020) IEEE Access, 8, pp. 163868-163877. , http://doi.org/10.1109/ACCESS.2020.3021904; Zhang, W., Dietterich, T.G., (1995), A Reinforcement Learning Approach to Job-Shop Scheduling. Proceedings of the 14th International Joint Conference on Artificial Intelligence, 2, 1114–1120; Zhang, N., Si, W., Deep Reinforcement Learning for Condition-Based Maintenance Planning of Multi-Component Systems Under Dependent Competing Risks (2020) Reliability Engineering & System Safety, 203, pp. 107-094. , http://doi.org/10.1016/j.ress.2020.107094; Zhang, T., Zhou, F., Zhao, J., Wang, W., (2020), Deep Reinforcement Learning for Secondary Energy Scheduling Steel Industry. 2020 2nd International Conference on Industrial Artificial Intelligence (IAI), Shenyang, China, July 24–26; Zhao, M., Guo, X., Zhang, X., Fang, Y., Ou, Y., ASPW-DRL: Assembly Sequence Planning for Workpieces Via a Deep Reinforcement Learning Approach (2019) Assembly Automation, 40 (1), pp. 65-75. , http://doi.org/10.1108/AA-11-2018-0211; Zhao, X., Zhao, H., Chen, P., Ding, H., Model Accelerated Reinforcement Learning for High Precision Robotic Assembly (2020) International Journal of Intelligent Robotics and Applications, 4 (2), pp. 202-216. , http://doi.org/10.1007/s41315-020-00138-z; Zheng, S., Gupta, C., Serita, S., (2020), http://doi.org/10.1007/978-3-030-46133-1_39, Manufacturing Dispatching Using Reinforcement and Transfer Learning. Proceedings of Joint European Conference on Machine Learning and Knowledge Discovery Databases, 655–671; Zhou, Z., Li, X., Zare, R.N., Optimizing Chemical Reactions with Deep Reinforcement Learning (2017) ACS Central Science, 3 (12), pp. 1337-1344. , http://doi.org/10.1021/acscentsci.7b00492; Zhou, T., Tang, D., Zhu, H., Wang, L., Reinforcement Learning With Composite Rewards for Production Scheduling in a Smart Factory (2021) IEEE Access, 9, pp. 752-766. , http://doi.org/10.1109/ACCESS.2020.3046784; Zhou, Y., Xing, T., Song, Y., Li, Y., Zhu, X., Li, G., Ding, S., Digital-Twin-Driven Geometric Optimization of Centrifugal Impeller with Free-Form Blades for Five-Axis Flank Milling (2020) Journal of Manufacturing Systems, 58, pp. 22-35. , http://doi.org/10.1016/j.jmsy.2020.06.019; Zhou, L., Zhang, L., Horn, B.K.P., Deep Reinforcement Learning-Based Dynamic Scheduling in Smart Manufacturing (2020) Procedia CIRP, 93, pp. 383-388. , http://doi.org/10.1016/j.procir.2020.05.163; Zhu, K., Ji, N., Dong Li, X., Hybrid Heuristic Algorithm Based On Improved Rules Reinforcement Learning for 2D Strip Packing Problem (2020) IEEE Access, 8, pp. 226784-226796. , http://doi.org/10.1109/ACCESS.2020.3045905; Zhu, H., Li, M., Tang, Y., Sun, Y., A Deep-Reinforcement-Learning-Based Optimization Approach for Real-Time Scheduling in Cloud Manufacturing (2020) IEEE Access, 8, pp. 9987-9997. , http://doi.org/10.1109/ACCESS.2020.2964955; Zimmerling, C., Poppe, C., Kärger, L., Estimating Optimum Process Parameters in Textile Draping of Variable Part Geometries–A Reinforcement Learning Approach (2020) Procedia Manufacturing, 47, pp. 847-854. , http://doi.org/10.1016/j.promfg.2020.04.263; Zou, Y., Lan, R., An End-to-End Calibration Method for Welding Robot Laser Vision Systems With Deep Reinforcement Learning (2020) IEEE Transactions on Instrumentation and Measurement, 69 (7), pp. 4270-4280. , http://doi.org/10.1109/TIM.2019.2942533},
correspondence_address1={Panzer, M.; Chair of Business Informatics, Karl-Marx-Street 67, Germany; email: marcel.panzer@wi.uni-potsdam.de},
publisher={Taylor and Francis Ltd.},
issn={00207543},
coden={IJPRB},
language={English},
abbrev_source_title={Int J Prod Res},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kuhnle2021,
author={Kuhnle, A. and May, M.C. and Schäfer, L. and Lanza, G.},
title={Explainable reinforcement learning in production control of job shop manufacturing system},
journal={International Journal of Production Research},
year={2021},
doi={10.1080/00207543.2021.1972179},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114823465&doi=10.1080%2f00207543.2021.1972179&partnerID=40&md5=f836036910f46286139c7f4c1e3ed39e},
affiliation={WBK Institute of Production Science, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={Manufacturing in the age of Industry 4.0 can be characterised by a high product variety and complex material flows. The increasing individualisation of products requires adaptive production planning and control systems. Research in the area of Machine Learning demonstrates the applicability and potential of Reinforcement Learning (RL) systems for the control of complex manufacturing. However, a major disadvantage of RL-methods is that they are usually considered as ‘black box’ models. For this reason, this paper investigates methods of explainable reinforcement learning in production control. Based on a comprehensive literature review an approach to increase the plausibility of RL-based control strategies is presented. The approach combines the advantages of high prediction accuracy (e.g. neural networks) and high explainability (e.g. decision trees). In doing so, understandable control strategies such as heuristics can be generated, and an advanced RL-system can be designed including specific domain expertise. The results are demonstrated based on a real-world system, taken from semiconductor manufacturing, which is investigated in a simulated approach. © 2021 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={explainability;  production control;  reinforcement learning;  semiconductor manufacturing;  simulation},
keywords={Complex networks;  Decision trees;  Industrial research;  Job shop scheduling;  Learning systems;  Production control;  Reinforcement learning;  Semiconductor device manufacture, Complex manufacturing;  Complex materials;  Control strategies;  Job shop manufacturing;  Literature reviews;  Prediction accuracy;  Production planning and control;  Semiconductor manufacturing, Adaptive control systems},
references={Arel, I., Liu, C., Urbanik, T., Kohls, A.G., Reinforcement Learning-Based Multi-Agent System for Network Traffic Signal Control (2010) IET Intelligent Transport Systems, 4 (2), p. 128; Aydin, M.E., Öztemel, E., Dynamic Job-Shop Scheduling using Reinforcement Learning Agents (2000) Robotics and Autonomous Systems, 33 (2-3), pp. 169-178; Bach, S., Binder, A., Montavon, G., Klauschen, F., Müller, K.-R., Samek, W., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation (2015) PloS One, 10 (7); Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., Mordatch, I., (2019), http://arxiv.org/pdf/1909.07528v2, Emergent Tool Use from Multi-Agent Autocurricula; Blazewicz, J., Karel Lenstra, J., Rinnooy Kan, A.H.G., Scheduling Subject to Resource Constraints: Classification and Complexity (1983) Discrete Applied Mathematics, 5 (1), pp. 11-24; Brittain, M., Wei, P., (2019), Autonomous Air Traffic Controller: A Deep Multi-Agent Reinforcement Learning Approach. Preprint; Ciosek, K., Vuong, Q., Loftin, R., Hofmann, K., (2019), http://arxiv.org/pdf/1910.12807v1, Better Exploration with Optimistic Actor-Critic. NeurIPS 2019; Colledani, M., Tolio, T., Fischer, A., Iung, B., Lanza, G., Schmitt, R., Váncza, J., Design and Management of Manufacturing Systems for Production Quality (2014) CIRP Annals, 63 (2), pp. 773-796; Dewey, D., (2014), https://www.aaai.org/ocs/index.php/SSS/SSS14/paper/viewPaper/7704, Reinforcement Learning and the Reward Engineering Principle. 2014 AAAI Spring Symposium Series, Palo Alto, 24.03. - 26.03.2014, edited by Carol M. Hamilton, 13–16. Palo Alto: AAAI Press; Dulac-Arnold, G., Mankowitz, D., Hester, T., (2019), Challenges of Real-World Reinforcement Learning. Preprint; Ennen, P., Reuter, S., Vossen, R., Jeschke, S., Automated Production Ramp-Up Through Self-Learning Systems (2016) Procedia CIRP, 51, pp. 57-62; Gabel, T., Riedmiller, M., Adaptive Reactive Job-Shop Scheduling with Reinforcement Learning Agents (2008) International Journal of Information Technology and Intelligent Computing, 24 (4), pp. 14-18; Gabel, T., Riedmiller, M., Distributed Policy Search Reinforcement Learning for Job-Shop Scheduling Tasks (2012) International Journal of Production Research, 50 (1), pp. 41-61. , http://www.tandfonline.com/doi/abs/10.1080/00207543.2011.571443; Ganin, Y., Kulkarni, T., Babuschkin, I., Ali Eslami, S.M., Vinyals, O., (2018), http://arxiv.org/pdf/1804.01118v1, Synthesizing Programs for Images using Reinforced Adversarial Learning; Gläscher, J., Daw, N., Dayan, P., O'Doherty, J.P., States Versus Rewards: Dissociable Neural Prediction Error Signals Underlying Model-Based and Model-Free Reinforcement Learning (2010) Neuron, 66 (4), pp. 585-595; Graham, R.L., Lawler, E.L., Lenstra, J.K., Rinnooy Kan, A.H.G., Optimization and Approximation in Deterministic Sequencing and Scheduling: A Survey (1979) Annals of Discrete Mathematics, 5 (C), pp. 287-326; Graves, S.C., A Review of Production Scheduling (1981) Operations Research, 29 (4), pp. 646-675. , https://doi.org/10.1287/opre.29.4.646; Greschke, P., Schönemann, M., Thiede, S., Herrmann, C., Matrix Structures for High Volumes and Flexibility in Production Systems (2014) Procedia CIRP, 17, pp. 160-165; Greydanus, S., Koul, A., Dodge, J., Fern, A., (2017), http://arxiv.org/pdf/1711.00138v5, Visualizing and Understanding Atari Agents; Hausknecht, M., Stone, P., (2015), http://arxiv.org/pdf/1507.06527v4, Deep Recurrent Q-Learning for Partially Observable MDPs; He, S., Pugeault, N., (2018), Deep Saliency: What is Learnt by a Deep Network about Saliency? Preprint; Hsu, C.-H., Chang, S.-H., Liang, J.-H., Chou, H.-P., Liu, C.-H., Chang, S.-C., Pan, J.-Y., Juan, D.-C., (2018), Monas: Multi-Objective Neural Architecture Search using Reinforcement Learning. Preprint; Jaques, N., Lazaridou, A., Hughes, E., Gulcehre, C., Ortega, P.A., Strouse, D.J., Leibo, J.Z., de Freitas, N., (2018), http://arxiv.org/pdf/1810.08647v4, Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning; Jaunet, T., Vuillemot, R., Wolf, C., (2019), http://arxiv.org/pdf/1909.02982v1, DRLViz: Understanding Decisions and Memory Deep Reinforcement Learning; Kober, J., Peters, J., (2014), eds., Learning Motor Skills,. Springer Tracts Advanced Robotics. Cham: Springer International Publishing; Kuhnle, A., (2020), Adaptive Order Dispatching based on Reinforcement Learning: Application a Complex Job Shop the Semiconductor Industry. PhD diss., Karslruhe Institute of Technology (KIT), Karlsruhe; Kuhnle, A., Kaiser, J.-P., Theiβ, F., Stricker, N., Lanza, G., Designing an Adaptive Production Control System Using Reinforcement Learning (2021) Journal of Intelligent Manufacturing, 32, pp. 855-876; Kuhnle, A., Schaarschmidt, M., Fricke, K., (2017), https://github.com/tensorforce/tensorforce, Tensorforce: A TensorFlow Library for Applied Reinforcement Learning; Kuhnle, A., Schäfer, L., Stricker, N., Lanza, G., Design, Implementation and Evaluation of Reinforcement Learning for an Adaptive Order Dispatching in Job Shop Manufacturing Systems (2019) Procedia CIRP, 81, pp. 234-239; Küpper, D., Sieben, C., Kuhlmann, K., Lim, Y., Ahmad, J., (2018) Will Flexible-Cell Manufacturing Revolutionize Carmaking?, , https://www.bcg.com/de-de/publications/2018/flexible-cell-manufacturing-revolutionize-carmaking.aspx, Cologne: Boston Consulting Group; Lasi, H., Fettke, P., Kemper, H.-G., Feld, T., Hoffmann, M., Industry 4.0 (2014) Business & Information Systems Engineering, 6 (4), pp. 239-242; Law, A.M., (2014) Simulation Modeling and Analysis, , 5th ed, New York: McGraw-Hill Education -- Europe; Levine, S., Finn, C., Darrell, T., Abbeel, P., (2015), http://arxiv.org/pdf/1504.00702v5, End-to-End Training of Deep Visuomotor Policies; Li, S., Egorov, M., Kochenderfer, M., (2019), http://arxiv.org/pdf/1912.10146v1, Optimizing Collision Avoidance Dense Airspace using Deep Reinforcement Learning; Li, J., Monroe, W., Jurafsky, D., (2017), http://arxiv.org/pdf/1612.08220v3, Understanding Neural Networks through Representation Erasure; Little, J.D.C., A Proof for the Queuing Formula (1961) Operations Research, 9 (3), pp. 383-387. , http://pubsonline.informs.org/doi/abs/10.1287/opre.9.3.383; Madumal, P., Miller, T., Sonenberg, L., Vetere, F., (2019), http://arxiv.org/pdf/1905.10958v2, Explainable Reinforcement Learning through a Causal Lens; Mahadevan, S., Theocharous, G., (1998), Optimizing Production Manufacturing using Reinforcement Learning. Proceedings of the 11th International Florida Artificial Intelligence Research Society Conference, Sanibel Island, 18.05. - 20.05.1998, edited by D. J. Cook, 372–377. Palo Alto: AAAI Press; Mao, H., Alizadeh, M., Menache, I., Kandula, S., (2016), Resource Management with Deep Reinforcement Learning. Proceedings of the 15th ACM Workshop on Hot Topics Networks - HotNets 16, edited by Bryan Ford, Alex C. Snoeren, and Ellen Zegura, 50–56. New York, NY: ACM Press; Mavrin, B., Zhang, S., Yao, H., Kong, L., Wu, K., Yu, Y., (2019), http://arxiv.org/pdf/1905.06125v1, Distributional Reinforcement Learning for Efficient Exploration. ICML; May, M.C., Kiefer, L., Kuhnle, A., Stricker, N., Lanza, G., Decentralized Multi-Agent Production Control through Economic Model Bidding for Matrix Production Systems (2021) Procedia CIRP, 96, pp. 3-8; May, M.C., Overbeck, L., Wurster, M., Kuhnle, A., Lanza, G., Foresighted Digital Twin for Situational Agent Selection in Production Control (2021) Procedia CIRP, 99, pp. 27-32; May, M.C., Schmidt, S., Kuhnle, A., Stricker, N., Lanza, G., Product Generation Module: Automated Production Planning for Optimized Workload and Increased Efficiency in Matrix Production Systems (2021) Procedia CIRP, 96, pp. 45-50; Mayer, S., Gankin, D., Arnet, C., Endisch, C., (2019), Adaptive Production Control with Negotiating Agents Modular Assembly Systems. 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC), 120–127. IEEE; Mendonca, R., Gupta, A., Kralev, R., Abbeel, P., Levine, S., Finn, C., (2019), http://arxiv.org/pdf/1904.00956v1, Guided Meta-Policy Search; Mes, M., Van Der Heijden, M., Van Harten, A., Comparison of Agent-Based Scheduling to Look-Ahead Heuristics for Real-Time Transportation Problems (2007) European Journal of Operational Research, 181 (1), pp. 59-75; Meurer, A., Smith, C., Paprocki, M., Čertík, O., Kirpichev, S., Rocklin, M., Kumar, A., SymPy: Symbolic Computing in Python (2017) PeerJ Computer Science, 3, p. e103. , https://doi.org/10.7717/peerj-cs.103, et al; Minguillon, F.E., Lanza, G., Coupling of Centralized and Decentralized Scheduling for Robust Production in Agile Production Systems (2019) Procedia CIRP, 79, pp. 385-390; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., (2013) Playing Atari with Deep Reinforcement Learning, 1312 (5602), pp. 1-9. , http://arxiv.org/abs/1312.5602; Mönch, L., Fowler, J.W., Mason, S.J., (2013) Production Planning and Control for Wafer Fabrication Facilities: Modeling, Analysis, and Systems, , New York: Springer, and,. 10, 978–1; Morch, N.J.S., Kjems, U., Kai Hansen, L., Svarer, C., Law, I., Lautrup, B., Strother, S., Rehm, K., (1995), Visualization of Neural Networks using Saliency Maps. Proceedings of ICNN95-International Conference on Neural Networks, 4, 2085–2090. IEEE; Ng, A., Harada, D., Russell, S., (1999), http://luthuli.cs.uiuc.edu/daf/courses/games/AIpapers/ng99policy.pdf, Policy Invariance under Rreward Transformations: Theory and Application to Reward Shaping. Proceedings of the Sixteenth International Conference on Machine Learning, Bled, 27.06. - 30.06.1999, edited by Ivan Bratko and S. Dzeroski, 278–287. San Francisco: Morgan Kaufmann Publishers Inc; Ou, X., Chang, Q., Chakraborty, N., Simulation Study on Reward Function of Reinforcement Learning in Gantry Work Cell Scheduling (2019) Journal of Manufacturing Systems, 50, pp. 1-8. , https://linkinghub.elsevier.com/retrieve/pii/S0278612518304503, –,. :10.1016/j.jmsy.2018.11.005; Rakelly, K., Zhou, A., Quillen, D., Finn, C., Levine, S., (2019), http://arxiv.org/pdf/1903.08254v1, Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables; Ribeiro, M.T., Singh, S., Guestrin, C., (2016), Why Should I Trust You? Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD 16, edited by Balaji Krishnapuram, Mohak Shah, Alex Smola, Charu Aggarwal, Dou Shen, and Rajeev Rastogi, 1135–1144. New York, NY: ACM Press; Riedmiller, S., Riedmiller, M., (1999), A Neural Reinforcement Learning Approach to Learn Local Dispatching Policies Production Scheduling. Proceedings of the 16th International Joint Conference on Artificial Intelligence, Stockholm, 31.07. - 06.08.1999, edited by Thomas Dean, 764–769. San Francisco: Morgan Kaufmann Publishers Inc; Russell, S.J., Norvig, P., (2016), Artificial Intelligence: A Modern Approach,. 3rd ed. Always Learning. Boston: Pearson; Samek, W., Montavon, G., Vedaldi, A., Kai Hansen, L., Müller, K.-R., (2019), eds., Explainable AI: Interpreting, Explaining and Visualizing Deep Learning,. Lecture Notes Computer Science. Cham: Springer International Publishing; Schönemann, M., Herrmann, C., Greschke, P., Thiede, S., Simulation of Matrix-structured Manufacturing Systems (2015) Journal of Manufacturing Systems, 37 (1), pp. 104-112; Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P., (2015), Trust Region Policy Optimization. International Conference on Machine Learning, 1889–1897; Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O., (2017), Proximal Policy Optimization Algorithms. Preprint; Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D., (2017), http://arxiv.org/pdf/1610.02391v4, Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. 2017 IEEE International Conference on Computer Vision (ICCV), 618–626; Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go Through Self-play (2018) Science (New York, N.Y.), 362 (6419), pp. 1140-1144. , et al; Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Mastering the Game of Go Without Human Knowledge (2017) Nature, 550 (7676), pp. 354-359. , http://www.nature.com/articles/nature24270, et al.,. :10.1038/nature24270; Simonyan, K., Vedaldi, A., Zisserman, A., (2013), Deep Inside Convolutional Nnetworks: Visualising Image Classification Models and Saliency Maps. Preprint; Stricker, N., Kuhnle, A., Sturm, R., Friess, S., CIRP Annals -- Manufacturing Technology Reinforcement Learning for Adaptive Order Dispatching in the Semiconductor Industry (2018) CIRP Annals -- Manufacturing Technology, 67 (1), pp. 511-514; Sutton, R.S., Barto, A.G., (2018) Reinforcement Learning: An Introduction, , 2nd ed., Cambridge, MA: MIT Press; Tedrake, R., Zhang, T.W., Seung, H.S., (2004), Stochastic Policy Gradient Reinforcement Learning on a Simple 3D Biped. 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. 04CH37566), 2849–2854. IEEE; Vinyals, O., Babuschkin, I., Czarnecki, W.M., Mathieu, M., Dudzik, A., Chung, J., Choi, D.H., Grandmaster Level in StarCraft II Using Multi-agent Reinforcement Learning (2019) Nature, 575 (7782), pp. 350-354. , et al; Wang, J., Li, X., Zhu, X., Intelligent Dynamic Control of Stochastic Economic Lot Scheduling by Agent-Based Reinforcement Learning (2012) International Journal of Production Research, 50 (16), pp. 4381-4395; Waschneck, B., Altenmüller, T., Bauernhansl, T., Knapp, A., Kyek, A., Reichstaller, A., Belzner, L., (2018), https://ieeexplore.ieee.org/document/8373191/, et al.,. Deep Reinforcement Learning for Semiconductor Production Scheduling. 2018 29th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC 2018), Saratoga Springs, NY, 30.04. - 03.05.2018, edited by A. Roussy and R. Van Roijen, 301–306. Piscataway: IEEE; Waschneck, B., Altenmüller, T., Bauernhansl, T., Kyek, A., (2016), Production Scheduling Complex Job Shops from an Industrie 4.0 Perspective: A Review and Challenges the Semiconductor Industry. Graz; Wiendahl, H.-P., ElMaraghy, H.A., Nyhuis, P., Zäh, M.F., Wiendahl, H.-H., Duffie, N., Brieke, M., Changeable Manufacturing-Classification, Design and Operation (2007) CIRP Annals, 56 (2), pp. 783-809; Wu, J., Xu, X., Zhang, P., Liu, C., A Novel Multi-Agent Reinforcement Learning Approach for Job Scheduling in Grid Computing (2011) Future Generation Computer Systems, 27 (5), pp. 430-439; Yao, M., (2019), https://www.topbots.com/top-ai-reinforcement-learning-research-papers-2019/, Breakthrough Research Reinforcement Learning From 2019. Accessed May 5 2020; Zhang, W., Dietterich, T.G., (1995), A Reinforcement Learning Approach to Jjob-Shop Scheduling. IJCAI, 95, 1114–1120. Citeseer; Zhang, Z., Zheng, L., Hou, F., Li, N., Semiconductor Final Test Scheduling with Sarsa(λ, K) Algorithm (2011) European Journal of Operational Research, 215 (2), pp. 446-458},
correspondence_address1={Kuhnle, A.; wbk Institute of Production Science, Kaiserstr. 12, Germany; email: andreas.kuhnle@kit.edu},
publisher={Taylor and Francis Ltd.},
issn={00207543},
coden={IJPRB},
language={English},
abbrev_source_title={Int J Prod Res},
document_type={Article},
source={Scopus},
}

@ARTICLE{Heger2021,
author={Heger, J. and Voss, T.},
title={Dynamically adjusting the k-values of the ATCS rule in a flexible flow shop scenario with reinforcement learning},
journal={International Journal of Production Research},
year={2021},
doi={10.1080/00207543.2021.1943762},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109310848&doi=10.1080%2f00207543.2021.1943762&partnerID=40&md5=65347fbe00dc524847b91da715ae9e49},
affiliation={Institute of Product and Process Innovation, Leuphana University Lueneburg, Lueneburg, Germany},
abstract={Given the fact that finding the optimal sequence in a flexible flow shop is usually an NP-hard problem, priority-based sequencing rules are applied in many real-world scenarios. In this contribution, an innovative reinforcement learning approach is used as a hyper-heuristic to dynamically adjust the k-values of the ATCS sequencing rule in a complex manufacturing scenario. For different product mixes as well as different utilisation levels, the reinforcement learning approach is trained and compared to the k-values found with an extensive simulation study. This contribution presents a human comprehensible hyper-heuristic, which is able to adjust the k-values to internal and external stimuli and can reduce the mean tardiness up to 5%. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={dynamic adjustment;  production planning and control;  reinforcement learning;  Sequencing rules;  simulation study},
keywords={Heuristic methods;  Machine shop practice;  NP-hard, Complex manufacturing;  Extensive simulations;  External stimulus;  Flexible flow shop;  Optimal sequence;  Real-world scenario;  Reinforcement learning approach;  Sequencing rules, Reinforcement learning},
references={Adams, J., Balas, E., Zawack, D., The Shifting Bottleneck Procedure for Job Shop Scheduling (1988) Management Science, 34 (3), pp. 391-401. , http://www.jstor.org/stable/2632051; Baker, K.R., The Effects of Input Control in a Simple Scheduling Model (1984) Journal of Operations Management, 4 (2), pp. 99-112; Branke, J., Hildebrandt, T., Scholz-Reiter, B., Hyper-Heuristic Evolution of Dispatching Rules: A Comparison of Rule Representations (2015) Evolutionary Computation, 23 (2), pp. 249-277; Branke, J., Nguyen, S., Pickardt, C.W., Zhang, M., Automated Design of Production Scheduling Heuristics: A Review (2016) IEEE Transactions on Evolutionary Computation, 20 (1), pp. 110-124; Chen, J.Y., Pfund, M.E., Fowler, J.W., Montgomery, D.C., Callarman, T.E., Robust Scaling Parameters for Composite Dispatching Rules (2010) IIE Transactions, 42 (11), pp. 842-853; Chen, S., Fang, S., Tang, R., A Reinforcement Learning Based Approach for Multi-projects Scheduling in Cloud Manufacturing (2019) International Journal of Production Research, 57 (10), pp. 3080-3098; Chen, Y., (2013) 2013 International Conference on Engineering, Management Science and Innovation (ICEMSI), , Loss Function Based Robust Scaling Parameters for Composite Dispatching Rule ATCS. Macao, China, 1–3. IEEE; Fazlollahtabar, H., Saidi-Mehrabad, M., Masehian, E., Mathematical Model for Deadlock Resolution in Multiple AGV Scheduling and Routing Network: A Case Study (2015) Industrial Robot: An International Journal, 42 (3), pp. 252-263; Gabel, T., (2009), Multi-agent Reinforcement Learning Approaches for Distributed Job-Shop Scheduling Problems. PhD diss., Universität Osnabrück, Osnabrück; Gabel, T., Riedmiller, M., Adaptive Reactive Job-shop Scheduling with Reinforcement Learning Agents (2008) International Journal of Information Technology and Intelligent Computing, 24 (4), pp. 14-18; Gallay, O., Korpela, K., Tapio, N., Nurminen, J.K., (2017) Proceedings of the Hamburg International Conference of Logistics (HICL), , https://doi.org/10.15480/882.1473, A Peer-to-peer Platform for Decentralized Logistics. Hamburg, Germany, 19–34. epubli; Graham, R.L., Lawler, E.L., Karel Lenstra, J., Rinnooy, A., Kan, H.G., Optimization and Approximation in Deterministic Sequencing and Scheduling: A Survey (1979) Annals of Discrete Mathematics, 5, pp. 287-326; Heger, J., (2014) Dynamische Regelselektion in Der Reihenfolgeplanung, , Wiesbaden: Springer Fachmedien Wiesbaden; Heger, J., Branke, J., Hildebrandt, T., Scholz-Reiter, B., Dynamic Adjustment of Dispatching Rule Parameters in Flow Shops with Sequence-Dependent Set-up Times (2016) International Journal of Production Research, 54 (22), pp. 6812-6824; Heger, J., Voβ, T., (2020) Proceedings of the Winter Simulation Conference, , https://doi.org/10.1109/WSC48552.2020.9383903, Dynamically Changing Sequencing Rules with Reinforcement Learning a Job Shop System with Stochastic Influences. edited by K.-H. Bae, B. Feng, S. Kim, S. Lazarova-Molnar, Z. Zheng, T. Roeder, and R. Thiesing, 1608–1618. Orlando, FL: IEEE; Holland, J.H., (1984) Adaptive Control of Ill-Defined Systems, , Genetic Algorithms and Adaptation. edited by Oliver G. Selfridge, Edwina L. Rissland, and Michael A. Arbib, 317–333. Boston, MA: Springer US; Jun, S., Lee, S., Learning Dispatching Rules for Single Machine Scheduling with Dynamic Arrivals Based on Decision Trees and Feature Construction (2021) International Journal of Production Research, 59 (9), pp. 2838-2856; Kardos, C., Laflamme, C., Gallina, V., Sihn, W., Dynamic Scheduling in a Job-shop Production System with Reinforcement Learning (2021) Procedia CIRP, 97, pp. 104-109; Knust, S., (1999) Shop scheduling problems with transportation: Diss, , Osnabrück: Universität Osnabrück, Fachbereich Mathematik/Informatik; Kuhnle, A., (2020) Adaptive Order Dispatching Based on Reinforcement Learning: Application in a Complex Job Shop in the Semiconductor IndustryForschungsberichte aus dem wbk, Institut für Produktionstechnik, Karlsruher Institut für Technologie (KIT), , Band 241 of Forschungsberichte aus dem wbk, Institut für Produktionstechnik, Karlsruher Institut für Technologie (KIT). Düren: Shaker Verlag GmbH; Kuhnle, A., Kaiser, J.-P., Theiβ, F., Stricker, N., Lanza, G., Designing An Adaptive Production Control System Using Reinforcement Learning (2020) Journal of Intelligent Manufacturing, 32, pp. 855-876; Kuhnle, A., Röhrig, N., Lanza, G., Autonomous Order Dispatching in the Semiconductor Industry Using Reinforcement Learning (2019) Procedia CIRP, 79, pp. 391-396; Kumar, R., Simulation of Manufacturing System at Different Part Mix Ratio and Routing Flexibility (2016) Global Journal of Enterprise Information System, 8 (1), pp. 10-14; Lee, Y.H., Bhaskaran, K., Pinedo, M., A Heuristic to Minimize the Total Weighted Tardiness with Sequence-dependent Setups (1997) IIE Transactions, 29 (1), pp. 45-52; May, M.C., Kiefer, L., Kuhnle, A., Stricker, N., Lanza, G., Decentralized Multi-Agent Production Control Through Economic Model Bidding for Matrix Production Systems (2021) Procedia CIRP, 96, pp. 3-8; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Human-level Control Through Deep Reinforcement Learning (2015) Nature, 518 (7540), p. 529. , et al; Mönch, L., Zimmermann, J., Otto, P., Machine Learning Techniques for Scheduling Jobs with Incompatible Families and Unequal Ready Times on Parallel Batch Machines (2006) Engineering Applications of Artificial Intelligence, 19 (3), pp. 235-245; Nguyen, S., Mei, Y., Zhang, M., Genetic Programming for Production Scheduling: a Survey with a Unified Framework (2017) Complex & Intelligent Systems, 3 (1), pp. 41-66; Oukil, A., El-Bouri, A., Ranking Dispatching Rules in Multi-objective Dynamic Flow Shop Scheduling: a Multi-faceted Perspective (2021) International Journal of Production Research, 59 (2), pp. 388-411; Panwalkar, S.S., Iskander, W., A Survey of Scheduling Rules (1977) Operations Research, 25 (1), pp. 45-61. , http://www.jstor.org/stable/169546; Park, Y., Kim, S., Lee, Y.-H., Scheduling Jobs on Parallel Machines Applying Neural Network and Heuristic Rules (2000) Computers & Industrial Engineering, 38 (1), pp. 189-202; Pickardt, C.W., Hildebrandt, T., Branke, J., Heger, J., Scholz-Reiter, B., Evolutionary Generation of Dispatching Rule Sets for Complex Dynamic Scheduling Problems (2013) International Journal of Production Economics, 145 (1), pp. 67-77; Poppenborg, J., Knust, S., Hertzberg, J., Online Scheduling of Flexible Job-shops with Blocking and Transportation (2012) European Journal of Industrial Engineering (EJIE), 6 (4), pp. 497-518; Qu, T., Lei, S.P., Wang, Z.Z., Nie, D.X., Chen, X., Huang, G.Q., IoT-based Real-time Production Logistics Synchronization System Under Smart Cloud Manufacturing (2016) The International Journal of Advanced Manufacturing Technology, 84 (1-4), pp. 147-164; Riley, M., Mei, Y., Zhang, M., (2016) 2016 IEEE Congress on Evolutionary Computation (CEC), , Improving Job Shop Dispatching Rules Via Terminal Weighting and Adaptive Mutation Genetic Programming. 3362–3369. Piscataway, NJ: Institute of Electrical and Electronics Engineers; Scholz-Reiter, B., Heger, J., Meinecke, C., Bergmann, J., Materialklassifizierung Unter Einbeziehung Von Bedarfsprognosen (2010) Productivity Management, 15 (1), pp. 57-60. , http://fox.leuphana.de/portal/de/publications/materialklassifizierung-unter-einbeziehung-von-bedarfsprognosen(61e557ff-32c6-4f9e-9731-45e91103203b).html; Sharma, P., Jain, A., Effect of Routing Flexibility and Sequencing Rules on Performance of Stochastic Flexible Job Shop Manufacturing System with Setup Times: Simulation Approach (2016) Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture, 231 (2), pp. 329-345; Shi, L., Guo, G., Song, X., Multi-Agent Based Dynamic Scheduling Optimisation of the Sustainable Hybrid Flow Shop in a Ubiquitous Environment (2021) International Journal of Production Research, 59 (2), pp. 576-597; Shiue, Y.-R., Lee, K.-C., Su, C.-T., Real-time Scheduling for a Smart Factory Using a Reinforcement Learning Approach (2018) Computers & Industrial Engineering, 125, pp. 604-614; Shiue, Y.-R., Lee, K.-C., Su, C.-T., A Reinforcement Learning Approach to Dynamic Scheduling in a Product-Mix Flexibility Environment (2020) IEEE Access, 8, pp. 106542-106553; Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Mastering the Game of Go Without Human Knowledge (2017) Nature, 550 (7676), p. 354. , et al; Stricker, N., Kuhnle, A., Sturm, R., Friess, S., Reinforcement Learning for Adaptive Order Dispatching in the Semiconductor Industry (2018) CIRP Annals, 67 (1), pp. 511-514; Vepsalainen, A.P.J., Morton, T.E., Priority Rules for Job Shops with Weighted Tardiness Costs (1987) Management Science, 33 (8), pp. 1035-1047; Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Sasha Vezhnevets, A., Yeo, M., Makhzani, A., al, E., (2017), http://arXiv:1708.04782, Starcraft II: A New Challenge for Reinforcement Learning. preprint; Waschneck, B., Reichstaller, A., Belzner, L., Altenmüller, T., Bauernhansl, T., Knapp, A., Kyek, A., Optimization of Global Production Scheduling with Deep Reinforcement Learning (2018) Procedia CIRP, 72 (1), pp. 1264-1269; Wuest, T., Weimer, D., Irgens, C., Thoben, K.-D., Machine Learning in Manufacturing: Advantages, Challenges, and Applications (2016) Production & Manufacturing Research, 4 (1), pp. 23-45; Zhang, F., Mei, Y., Nguyen, S., Zhang, M., (2020) Evolutionary Computation in Combinatorial OptimizationLecture Notes in Computer Science, , Genetic Programming with Adaptive Search Based on the Frequency of Features for Dynamic Flexible Job Shop Scheduling. edited by Luís Paquete and Christine Zarges, 12102 of Lecture Notes Computer Science, 214–230. Cham: Springer International Publishing; Zhang, F., Mei, Y., Zhang, M., (2018) AI 2018LNCS sublibrary. SL 7, Artificial intelligence, , Genetic Programming with Multi-tree Representation for Dynamic Flexible Job Shop Scheduling. edited by Tanja. Mitrovic, Bing. Xue, and Xiaodong. Li, 11320 of LNCS sublibrary. SL 7, Artificial intelligence, 472–484. Cham: Springer; Zhang, F., Mei, Y., Zhang, M., (2019) 2019 IEEE Congress on Evolutionary Computation (CEC), , https://doi.org/10.1109/CEC.2019.8790030, Can Stochastic Dispatching Rules Evolved by Genetic Programming Hyper-heuristics Help Dynamic Flexible Job Shop Scheduling? Wellington, New Zealand, 41–48. IEEE; Zhang, F., Mei, Y., Zhang, M., (2019) GECCO 2019: Proceedings of the Genetic and Evolutionary Computation Conference, , https://doi.org/10.1145/3321707.3321790, A Two-Stage Genetic Programming Hyper-Heuristic Approach with Feature Selection for Dynamic Flexible Job Shop Scheduling. 347–355. New York, NY: Association for Computing Machinery; Zhao, M., Liang Gao, X.L., Wang, L., Xiao, M., (2019) 2019 IEEE 15th International Conference on Automation Science and Engineering (CASE), , An improved Q-Learning Based Rescheduling Method for Flexible Job-Shops with Machine Failures. 331–337. Piscataway, NJ: Institute of Electrical and Electronics Engineers; Zheng, S., Gupta, C., Serita, S., (2019), http://arXiv:1910.02035, Manufacturing Dispatching Using Reinforcement and Transfer Learning. preprint},
correspondence_address1={Voss, T.; Institute of Product and Process Innovation, Universitaetsallee 1, Germany; email: thomas.voss@leuphana.de},
publisher={Taylor and Francis Ltd.},
issn={00207543},
coden={IJPRB},
language={English},
abbrev_source_title={Int J Prod Res},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2022429,
author={Zhang, H. and Djurdjanovic, D.},
title={Integrated production and maintenance planning under uncertain demand with concurrent learning of yield rate},
journal={Flexible Services and Manufacturing Journal},
year={2022},
volume={34},
number={2},
pages={429-450},
doi={10.1007/s10696-021-09417-8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105463074&doi=10.1007%2fs10696-021-09417-8&partnerID=40&md5=54c5f330fd680379e6ba20abe9e7345c},
affiliation={Program of Operations Research and Industrial Engineering, University of Texas at Austin, 204 E. Dean Keeton St., Austin, TX  78712, United States; Department of Mechanical Engineering, University of Texas at Austin, 204 E. Dean Keeton St., Austin, TX  78712, United States},
abstract={Strong interactions between decisions in the maintenance and production scheduling domains, and their impacts on the equipment yield rates necessitate maintenance and production decisions being optimized concurrently, with considerations of yield dependencies on the equipment conditions and production rates. This paper proposes an integrated decision-making policy for production and maintenance operations on a single machine under uncertain demand, with concurrent considerations and learning of yield dependencies on the equipment conditions and production rates. This policy is obtained through a two-stage stochastic programming model, which considers the variable demand, machine degradation, and maintenance times. This model incorporates outsourcing decisions and operational decisions regarding reworking, scraping of imperfect products to ensure the demand is adequately met. A closed-form reinforcement learning method is utilized to learn yield dependencies. Simulations confirm the necessity of yield learning and show the proposed method outperforms the traditional, fragmented approaches where the effects of production rates and machine conditions on the resulting yield rates are not considered. The two-stage stochastic setting is demonstrated by comparing with the traditional one-stage deterministic approach, where decisions are made based on the expected demand and production performance, with scrapping, reworking, and outsourcing decisions established before the demand and production performance are observed. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Integrated decision-making;  Learning of yield rate;  Maintenance scheduling;  Production planning;  Two-stage stochastic programming},
keywords={Decision making;  Maintenance;  Outsourcing;  Production control;  Reinforcement learning;  Stochastic models;  Stochastic programming;  Stochastic systems, Deterministic approach;  Integrated decision makings;  Maintenance operations;  Outsourcing decisions;  Production performance;  Production Scheduling;  Reinforcement learning method;  Two-stage stochastic programming, Learning systems},
funding_details={Department of Mechanical Engineering, University of Texas at AustinDepartment of Mechanical Engineering, University of Texas at Austin},
funding_text 1={This study was funded by Department of Mechanical Engineering, University of Texas at Austin.},
references={Aktekin, T., Ekin, T., Stochastic call center staffing with uncertain arrival, service and abandonment rates: a Bayesian perspective (2016) Nav Res Logist (NRL), 63 (6), pp. 460-478; Alaswad, S., Xiang, Y., A review on condition-based maintenance optimization models for stochastically deteriorating system (2017) Reliab Eng Syst Saf, 157, pp. 54-63; Sahin AZ (eds) (2014) Maintenance in manufacturing environment: an overview Integrated maintenance planning in manufacturing systems, pp. 5-23. , In:,., Springer, Cham; Aramon Bajestani, M., Banjevic, D., Beck, J.C., Integrated maintenance planning and production scheduling with Markovian deteriorating machine conditions (2014) Int J Prod Res, 52 (24), pp. 7377-7400; Batun, S., Maillart, L.M., Reassessing tradeoffs inherent to simultaneous maintenance and production planning (2012) Prod Oper Manag, 21 (2), pp. 396-403; Bearda, T., Mertens, P.W., Beaudoin, S.P., Overview of wafer contamination and defectivity (2018) Handbook of silicon wafer cleaning technology, pp. 87-149. , 3, Elsevier, Amsterdam; Birge, J.R., Louveaux, F., (2011) Introduction to stochastic programming, , Springer, New York; Joint maintenance and production operations decision making in flexible manufacturing systems (Doctoral dissertation) (2016) Retrieved from University of Texas Libraries (OCLC Number, , 979556469; Christensen, R., Johnson, W., Branscum, A., Hanson, T.E., (2011) Bayesian ideas and data analysis: an introduction for scientists and statisticians, , CRC Press, Boca Raton; Derman, C., (1970) Finite state Markovian decision processes, , (,)., (, 04; T57. 83, D47; Djurdjanovic, D., Mears, L., Niaki, F.A., Haq, A.U., Li, L., State of the art review on process, system, and operations control in modern manufacturing (2018) J Manuf Sci Eng, 140 (6), p. 061010; Ekin, T., Integrated maintenance and production planning with endogenous uncertain yield (2018) Reliab Eng Syst Saf, 179, pp. 52-61; Ekin, T., Polson, N.G., Soyer, R., Augmented Markov chain Monte Carlo simulation for two-stage stochastic programs with recourse (2014) Decis Anal, 11 (4), pp. 250-264; Ekin, T., Polson, N.G., Soyer, R., Augmented nested sampling for stochastic programs with recourse and endogenous uncertainty (2017) Nav Res Logist (NRL), 64 (8), pp. 613-627; Homem-de-Mello, T., Bayraksan, G., Monte Carlo sampling-based methods for stochastic optimization (2014) Surv Oper Res Manag Sci, 19 (1), pp. 56-85; Iravani, S.M., Duenyas, I., Integrated maintenance and production control of a deteriorating production system (2002) IIE Trans, 34 (5), pp. 423-435; Khouja, M., Mehrez, A., Economic production lot size model with variable production rate and imperfect quality (1994) J Oper Res Soc, 45 (12), pp. 1405-1417; Lattimore, T., Szepesvári, C., (2020) Bandit algorithms, , Cambridge University Press, Cambridge; Russo, D., Van Roy, B., Kazerouni, A., Osband, I., (2017) A Tutorial on Thompson Sampling. arXiv preprint arXiv, 1707, p. 02038. , &; Shapiro, A., Dentcheva, D., Ruszczyński, A., Lectures on stochastic programming: modeling and theory (2014) Society for Industrial and Applied Mathematics; Sloan, T.W., A periodic review production and maintenance model with random demand, deteriorating equipment, and binomial yield (2004) J Oper Res Soc, 55 (6), pp. 647-656; Sloan, T., Simultaneous determination of production and maintenance schedules using in-line equipment condition and yield information (2008) Nav Res Logist (NRL), 55 (2), pp. 116-129; Sloan, T.W., Shanthikumar, J.G., Combined production and maintenance scheduling for a multiple-product, single-machine production system (2000) Prod Oper Manag, 9 (4), pp. 379-399; Sloan, T.W., Shanthikumar, J.G., Using in-line equipment condition and yield information for maintenance scheduling and dispatching in semiconductor wafer fabs (2002) IIE Trans, 34 (2), pp. 191-209; Sutton, R.S., Barto, A.G., (2018) Reinforcement learning: an introduction, , MIT press, Cambridge; Terwiesch, C., Bohn, R.E., Learning and process improvement during production ramp-up (2001) Int J Prod Econ, 70 (1), pp. 1-19; Terwiesch, C., Xu, Y., The copy-exactly ramp-up strategy: trading-off learning with process change (2004) IEEE Trans Eng Manage, 51 (1), pp. 70-84; Thompson, W.R., On the likelihood that one unknown probability exceeds another in view of the evidence of two samples (1933) Biometrika, 25 (3-4), pp. 285-294; Thompson, W.R., On the theory of apportionment (1935) Am J Math, 57 (2), pp. 450-456; Wang, H., A survey of maintenance policies of deteriorating systems (2002) Eur J Oper Res, 139 (3), pp. 469-489; Yano, C.A., Lee, H.L., Lot sizing with random yields: a review (1995) Oper Res, 43 (2), pp. 311-334},
correspondence_address1={Zhang, H.; Program of Operations Research and Industrial Engineering, 204 E. Dean Keeton St., United States; email: huidong.zhang@utexas.edu},
publisher={Springer},
issn={19366582},
language={English},
abbrev_source_title={Flexible Serv Manuf J},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yang2021,
author={Yang, Y. and Yao, L.},
title={Optimization Method of Power Equipment Maintenance Plan Decision-Making Based on Deep Reinforcement Learning},
journal={Mathematical Problems in Engineering},
year={2021},
volume={2021},
doi={10.1155/2021/9372803},
art_number={9372803},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103677302&doi=10.1155%2f2021%2f9372803&partnerID=40&md5=103a0acbce6b5ed171ca645af5aa4957},
affiliation={School of Engineering, Fujian Jiangxia University, Fuzhou, 350108, China; School of Mechanical Engineering and Automation, Fuzhou University, Fuzhou, 350116, China},
abstract={The safe and reliable operation of power grid equipment is the basis for ensuring the safe operation of the power system. At present, the traditional periodical maintenance has exposed the abuses such as deficient maintenance and excess maintenance. Based on a multiagent deep reinforcement learning decision-making optimization algorithm, a method for decision-making and optimization of power grid equipment maintenance plans is proposed. In this paper, an optimization model of power grid equipment maintenance plan that takes into account the reliability and economics of power grid operation is constructed with maintenance constraints and power grid safety constraints as its constraints. The deep distributed recurrent Q-networks multiagent deep reinforcement learning is adopted to solve the optimization model. The deep distributed recurrent Q-networks multiagent deep reinforcement learning uses the high-dimensional feature extraction capabilities of deep learning and decision-making capabilities of reinforcement learning to solve the multiobjective decision-making problem of power grid maintenance planning. Through case analysis, the comparative results show that the proposed algorithm has better optimization and decision-making ability, as well as lower maintenance cost. Accordingly, the algorithm can realize the optimal decision of power grid equipment maintenance plan. The expected value of power shortage and maintenance cost obtained by the proposed method is $71.75$ $MW·H$ and $496000$ $yuan$. © 2021 Yanhua Yang and Ligang Yao.},
keywords={Cost benefit analysis;  Costs;  Decision making;  Electric power system economics;  Electric power transmission networks;  Learning systems;  Maintenance;  Multi agent systems;  Optimization;  Planning;  Recurrent neural networks;  Reinforcement learning, Decision-making optimization;  Extraction capability;  High dimensional feature;  Lower maintenance costs;  Multi-objective decision-making problems;  Optimization and decision makings;  Optimization modeling;  Power grid operations, Electric power system planning},
references={Rocchetta, R., Bellani, L., Compare, M., Zio, E., Patelli, E., A reinforcement learning framework for optimal operation and maintenance of power grids (2019) Applied Energy, 241 (5), pp. 291-301. , 2-s2.0-85062607186; Fattahi, M., Mahootchi, M., Mosadegh, H., Fallahi, F., A new approach for maintenance scheduling of generating units in electrical power systems based on their operational hours (2014) Computers & Operations Research, 50 (10), pp. 61-79. , 2-s2.0-84901009028; Lindner, B.G., Brits, R., Van Vuuren, J.H., Bekker, J., Tradeoffs between levelling the reserve margin and minimising production cost in generator maintenance scheduling for regulated power systems (2018) International Journal of Electrical Power & Energy Systems, 101 (10), pp. 458-471. , 2-s2.0-85045272520; Dai, C., Wang, Z., Dai, Y., Economic optimization method of transmission equipment maintenance scheduling based on declining index of transmission loss (2017) Northeast Electric Power Technology, 38 (12), pp. 4-9; Ran, L.I., Wang, F., Li, Z., Maintenance decision making optimization based on risk assessment for distribution system (2013) Electric Power Automation Equipment, 33 (11), pp. 1-8; Yang, X., Yin, Y., Kou, X., Research on the maintenance strategy of UHV receiving end power grid transmission equipment based on risk assessment (2017) Electric Power Engineering Technology, 36 (2), pp. 72-75; Li, E., Kang, C., Li, Y., Optimization model of distribution network maintenance plan based on equipment condition evaluation and grid loss risk (2018) High Voltage Engineering, 44 (11), pp. 3751-3759; Xu, B., Xu, S., Zhang, Y., (2019) Determining Optimal Inspection Rates of Power Equipment Considering Opportunistic Maintenance Strategy; Tian, H., Shuai, M., Li, K., Optimization study of line planning for high speed railway based on an improved multi-objective differential evolution algorithm (2019) IEEE Access, 7 (7), pp. 137731-137743; Wang, Y., Liu, H., Zheng, W., Multi-objective workflow scheduling with deep- Q -network-based multi-agent reinforcement learning (2019) IEEE Access, 7 (7), pp. 39974-39982. , 2-s2.0-85064241175; Raza, M.Q., Khosravi, A., A review on artificial intelligence based load demand forecasting techniques for smart grid and buildings (2015) Renewable and Sustainable Energy Reviews, 50 (10), pp. 1352-1372. , 2-s2.0-84935845022; Bakar, N.A., Desa, M.K.M., (2017) Optimal Placement of Tcsc in Transmission Network Using Sensitivity Based Method for Multi-objective Optimization: 2017 IEEE Conference on Energy Conversion; Li, Y., Ni, Z., Zhao, T., (2019) Coordinated Stochastic Scheduling for Improving Wind Power Adsorption in Electric Vehicles-wind Integrated Power Systems by Multi-objective Optimization Approach; Twaha, S., Ramli, M.A.M., A review of optimization approaches for hybrid distributed energy generation systems: Off-grid and grid-connected systems (2018) Sustainable Cities and Society, 41 (8), pp. 320-331. , 2-s2.0-85048482843; Abu-Mouti, F.S., El-Hawary, M.E., (2012) Overview of Artificial Bee Colony (ABC) Algorithm and Its Applications; Zhang, D., Han, X., Han, X., Deng, C., Review on the research and practice of deep learning and reinforcement learning in smart grids (2018) CSEE Journal of Power and Energy Systems, 4 (3), pp. 362-370; Zhang, Z., Zhang, D., Qiu, R.C., Deep reinforcement learning for power system Applications: An overview (2020) CSEE Journal of Power and Energy Systems, 6 (1), pp. 213-225; Aznar, F., Pujol, M., Rizo, R., Obtaining fault tolerance avoidance behavior using deep reinforcement learning (2019) Neuro Computing, 345 (6), pp. 77-91; Liu, J., Gao, F., Luo, X., Survey of deep reinforcement learning based on value function and policy gradient (2019) Chinese Journal of Computers, 42 (6), pp. 1406-1438; Wan, L., Lan, X., Zhang, H., A review of deep reinforcement learning theory and application (2019) Pattern Recognition and Artificial Intelligence, 32 (1), pp. 67-81; Liu, Q., Zhai, J., Zhang, Z., A survey on deep reinforcement learning (2018) Chinese Journal of Computers, 41 (1), pp. 1-27; Glavic, M., Fonteneau, R., Erns, D., Reinforcement learning for electric power system decision and control: Past considerations and perspectives (2017) IFAC-papers OnLine, 50 (7), pp. 6918-6927. , 2-s2.0-85031817629; Li, H., Cai, R., Liu, N., Lin, X., Wang, Y., Deep reinforcement learning: Algorithm, applications, and ultra-low-power implementation (2018) Nano Communication Networks, 16 (6), pp. 81-90. , 2-s2.0-85042882006; Cui, X., Wei, L.I., Li, B., Review on transmission equipment maintenance planning decision-making (2015) Power System and Clean Energy, 31 (12), pp. 18-26; Zhang, Y., Cai, P., Pan, C., Zhang, S., Multi-agent deep reinforcement learning-based cooperative spectrum sensing with upper confidence bound exploration (2019) IEEE Access, 7, pp. 118898-118906; Jia, W.U., Chen, S., Chen, X., Reinforcement learning for model selection and hyperparameter optimization (2020) Journal of University of Electronic Science and Technology of China (Social Sciences Edition), 49 (2), pp. 255-261; Hossein, A.-H., Hamed, M.-R., Power systems big data analytics: An assessment of paradigm shift barriers and prospects (2018) Energy Reports, 4 (11), pp. 91-100},
correspondence_address1={Yang, Y.; School of Engineering, China; email: mayspring@163.com},
publisher={Hindawi Limited},
issn={1024123X},
language={English},
abbrev_source_title={Math. Probl. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tanimoto202146788,
author={Tanimoto, A.},
title={Combinatorial Q-Learning for Condition-Based Infrastructure Maintenance},
journal={IEEE Access},
year={2021},
volume={9},
pages={46788-46799},
doi={10.1109/ACCESS.2021.3059244},
art_number={9354750},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100913193&doi=10.1109%2fACCESS.2021.3059244&partnerID=40&md5=0bc66aa06fa7ed24a886ca227d24481b},
affiliation={NEC, Kanagawa, Japan},
abstract={Infrastructure maintenance planning is a large-scale optimization problem of planning when and on which components to carry out maintenance so as to keep the whole infrastructure in good condition with minimal maintenance cost. Recent advances in condition monitoring techniques have enabled timely maintenance in response to the condition of each part regardless of age. In addition to the condition, the spatial structure is also important for cost-efficiency in infrastructure maintenance since traveling costs and/or setup costs can be saved by simultaneous maintenance of neighboring components, which is called economic dependency. This optimization problem naively has a high computational complexity of O(2nH), where n is the number of components and H is the planning horizon, and the predictive modeling of degradation is also a big issue. To solve this problem efficiently at scale, our proposed method utilizes two kinds of dynamic programming for temporal and spatial scalability and consequently enjoys O(n) complexity at each time step. For temporal scalability, we utilize a direct modeling approach for the action value of maintenance instead of modeling degradation, namely, Q-learning. For spatial scalability, we exploit locality in economic dependency by means of a reasonable approximation of the Q-function. A typical baseline approach is to divide the whole infrastructure into fixed groups of neighboring components beforehand and determine if maintenance should be performed for all the components in each group at each time step. In contrast, our scalable method enables fully combinatorial optimization for each component at each time step. We demonstrate the advantage of our method in a simulated environment, and the resulting maintenance history intuitively illustrates the benefit of our dynamic grouping approach. We also show that our method has a kind of interpretability in the optimization at each time step. © 2013 IEEE.},
author_keywords={artificial intelligence;  combinatorial optimization;  decision support systems;  dynamic programming;  Predictive maintenance;  reinforcement learning},
keywords={Combinatorial optimization;  Condition monitoring;  Dynamic programming;  Maintenance;  Predictive analytics;  Reinforcement learning;  Scalability, Infrastructure maintenance;  Large-scale optimization;  Monitoring techniques;  Number of components;  Optimization problems;  Simulated environment;  Temporal and spatial;  Temporal scalability, Costs},
references={Papadakis, I.S., Kleindorfer, P.R., Optimizing infrastructure network maintenance when benefits are interdependent (2005) OR Spectr., 27 (1), pp. 63-84. , Jan; Jardine, A.K., Tsang, A.H., (2005) Maintenance, Replacement, and Reliability: Theory and Applications, , Boca Raton, FL, USA: CRC Press; Dekker, R., Wildeman, R.E., Van Der-Duyn-Schouten, F.A., A review of multi-component maintenance models with economic dependence (1997) Math. Methods Oper. Res., 45 (3), pp. 411-435. , Oct; Nicolai, R.P., Dekker, R., Optimal maintenance of multi-component systems: A review (2008) Complex System Maintenance Handbook, pp. 263-286. , London, U.K.: Springer; Chambon, S., Moliard, J.-M., Automatic road pavement assessment with image processing: Review and comparison (2011) Int. J. Geophys., 2011, pp. 1-20. , Jun; Kim, S., Pakzad, S., Culler, D., Demmel, J., Fenves, G., Glaser, S., Turon, M., Health monitoring of civil infrastructures using wireless sensor networks (2007) Proc. 6th Int. Conf. Inf. Process. Sensor Netw. (IPSN), pp. 254-263; Li, H.-N., Li, D.-S., Song, G.-B., Recent applications of fiber optic sensors to health monitoring in civil engineering (2004) Eng. Struct., 26 (11), pp. 1647-1657. , Sep; Inaudi, D., Glisic, B., Long-range pipeline monitoring by distributed fiber optic sensing (2010) J. Pressure Vessel Technol., 132 (1). , Feb; Bousdekis, A., Magoutas, B., Apostolou, D., Mentzas, G., Review, analysis and synthesis of prognostic-based decision support methods for condition based maintenance (2018) J. Intell. Manuf., 29 (6), pp. 1303-1316. , Aug; Tian, Z., Liao, H., Condition based maintenance optimization for multicomponent systems using proportional hazards model (2011) Rel. Eng. Syst. Saf., 96 (5), pp. 581-589. , May; Nguyen, K.-A., Do, P., Grall, A., Multi-level predictive maintenance for multi-component systems (2015) Rel. Eng. Syst. Saf., 144, pp. 83-94. , Dec; Van Horenbeek, A., Pintelon, L., A dynamic predictive maintenance policy for complex multi-component systems (2013) Rel. Eng. Syst. Saf., 120, pp. 39-50. , Dec; Watkins, C.J.C.H., Dayan, P., Q-learning (1992) Mach. Learn., 8 (3-4), pp. 279-292; Peng, Y., Dong, M., Zuo, M.J., Current status of machine prognostics in condition-based maintenance: A review (2010) Int. J. Adv. Manuf. Technol., 50 (1-4), pp. 297-313. , Sep; Andriotis, C.P., Papakonstantinou, K.G., (2020) Deep Reinforcement Learning Driven Inspection and Maintenance Planning under Incomplete Information and Constraints, , http://arxiv.org/abs/2007.01380, [Online]; Su, Z., Jamshidi, A., Núñez, A., Baldi, S., De Schutter, B., Integrated condition-based track maintenance planning and crew scheduling of railway networks (2019) Transp. Res. C, Emerg. Technol., 105, pp. 359-384. , Aug; Su, Z., Jamshidi, A., Núñez, A., Baldi, S., De Schutter, B., Multilevel condition-based maintenance planning for railway infrastructures-A scenario-based chance-constrained approach (2017) Transp. Res. C, Emerg. Technol., 84, pp. 92-123. , Nov; Verbert, K., De Schutter, B., Babuška, R., Timely condition-based maintenance planning for multi-component systems (2017) Rel. Eng. Syst. Saf., 159, pp. 310-321. , Mar; Liu, J., Sun, L., Chen, W., Xiong, H., Rebalancing bike sharing systems: A multi-source data smart optimization (2016) Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, pp. 1005-1014. , Aug; Singh, S.P., Sutton, R.S., Reinforcement learning with replacing eligibility traces (1996) Mach. Learn., 22 (1-3), pp. 123-158; Sutton, R.S., Barto, A.G., (2018) Reinforcement Learning: An Introduction, , Cambridge, MA, USA: MIT Press; Aissani, N., Beldjilali, B., Trentesaux, D., Dynamic scheduling of maintenance tasks in the petroleum industry: A reinforcement approach (2009) Eng. Appl. Artif. Intell., 22 (7), pp. 1089-1103. , Oct; Barde, S.R.A., Yacout, S., Shin, H., Optimal preventive maintenance policy based on reinforcement learning of a fieet of military trucks (2019) J. Intell. Manuf., 30 (1), pp. 147-161. , Jan; Compare, M., Bellani, L., Cobelli, E., Zio, E., Reinforcement learningbased flow management of gas turbine parts under stochastic failures (2018) Int. J. Adv. Manuf. Technol., 99 (9-12), pp. 2981-2992. , Dec; Xanthopoulos, A.S., Kiatipis, A., Koulouriotis, D.E., Stieger, S., Reinforcement learning-based and parametric production-maintenance control policies for a deteriorating manufacturing system (2018) IEEE Access, 6, pp. 576-588; Kuhnle, A., Jakubik, J., Lanza, G., Reinforcement learning for opportunistic maintenance optimization (2019) Prod. Eng., 13 (1), pp. 33-41. , Feb; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., (2013) Playing Atari with Deep Reinforcement Learning, , http://arxiv.org/abs/1312.5602, [Online]; Yao, L., Dong, Q., Jiang, J., Ni, F., Deep reinforcement learning for long-term pavement maintenance planning (2020) Comput.-Aided Civil Infrastruct. Eng., 35 (11), pp. 1230-1245. , Nov; Wei, S., Bao, Y., Li, H., Optimal policy for structure maintenance: A deep reinforcement learning framework (2020) Struct. Saf., 83. , Mar; Zhang, N., Si, W., Deep reinforcement learning for condition-based maintenance planning of multi-component systems under dependent competing risks (2020) Rel. Eng. Syst. Saf., 203. , Nov; Liu, Y., Chen, Y., Jiang, T., Dynamic selective maintenance optimization for multi-state systems over a finite horizon: A deep reinforcement learning approach (2020) Eur. J. Oper. Res., 283 (1), pp. 166-181. , May; Riedmiller, M., Neural fitted Q iteration-first experiences with a data efficient neural reinforcement learning method (2005) Proc. Eur. Conf. Mach. Learn., pp. 317-328. , Berlin, Germany: Springer; Arulkumaran, K., Deisenroth, M.P., Brundage, M., Bharath, A.A., Deep reinforcement learning: A brief survey (2017) IEEE Signal Process. Mag., 34 (6), pp. 26-38. , Nov; Zheng, H., Yang, Z., Liu, W., Liang, J., Li, Y., Improving deep neural networks using softplus units (2015) Proc. Int. Joint Conf. Neural Netw. (IJCNN), pp. 1-4. , Jul; Getoor, L., Taskar, B., (2007) Introduction to Statistical Relational Learning, , Cambridge, MA, USA: MIT Press; Srivastava, N.K., Mondal, S., Development of predictive maintenance model for N-component repairable system using NHPP models and system availability concept (2016) Global Bus. Rev., 17 (1), pp. 105-115. , Feb; Zhou, R.R., Serban, N., Gebraeel, N., Degradation modeling applied to residual lifetime prediction using functional data analysis (2011) Ann. Appl. Statist., 5 (2 B), pp. 1586-1610. , Jun; Famurewa, S.M., Xin, T., Rantatalo, M., Kumar, U., Optimisation of maintenance track possession time: A tamping case study (2015) Proc. Inst. Mech. Eng. F, J. Rail Rapid Transit, 229 (1), pp. 12-22. , Jan; Adlinge, S.S., Gupta, A., Pavement deterioration and its causes (2013) Int. J. Innov. Res. Develop., 2 (4), pp. 437-450; Hong, H.P., Wang, S.S., Stochastic modeling of pavement performance (2003) Int. J. Pavement Eng., 4 (4), pp. 235-243. , Dec; Peng, C.-Y., Tseng, S.-T., Statistical lifetime inference with skewwiener linear degradation models (2013) IEEE Trans. Rel., 62 (2), pp. 338-350. , Jun; Snoek, S., Larochelle, H., Adams, R., Practical Bayesian optimization of machine learning algorithms (2012) Proc. Adv. Neural Inf. Process. Syst., 25, pp. 2951-2959; Shahriari, B., Swersky, K., Wang, Z., Adams, R.P., De Freitas, N., Taking the human out of the loop: A review of Bayesian optimization (2016) Proc. IEEE, 104 (1), pp. 148-175. , Jan; Gu, S., Lillicrap, T., Sutskever, I., Levine, S., Continuous deep Q-learning with model-based acceleration (2016) Proc. Int. Conf. Mach. Learn., pp. 2829-2838},
correspondence_address1={Tanimoto, A.; NECJapan; email: a.tanimoto@nec.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chatterjee2021435,
author={Chatterjee, J. and Dethlefs, N.},
title={Deep reinforcement learning for maintenance planning of offshore vessel transfer},
journal={Developments in Renewable Energies Offshore - Proceedings the 4th International Conference on Renewable Energies Offshore, RENEW 2020},
year={2021},
pages={435-443},
note={cited By 0; Conference of 4th International Conference on Renewable Energies Offshore, RENEW 2020 ; Conference Date: 12 October 2020 Through 15 October 2020;  Conference Code:172350},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100502505&partnerID=40&md5=b3c522396944488c002d0d5fd8b29cb4},
affiliation={Department of Computer Science and Technology, Big Data Analytics Research Group, University of Hull, Hull, United Kingdom},
abstract={Offshore wind farm operators need to make short-term decisions on planning vessel transfers to turbines for preventive or corrective maintenance. These decisions can play a pivotal role in ensuring maintenance actions are carried out in a timely and cost-effective manner. The present optimization of offshore vessel transfer uses mathematical models rather than learning decisions from historical data. In this paper, we design a simulated environment for an offshore wind farm based on Supervisory Control & Acquisition (SCADA) data and alarm logs of historical faults in an operational turbine. Firstly, we utilise a state-of-art decision tree model to predict fault types using SCADA features, and provide explainable decisions. Next, we apply deep reinforcement learning to automatically learn maintenance priorities corresponding to different fault types for ensuring prioritized vessel transfers for critical conditions, and deciding on optimal vessel fleet size. This can lead to significant savings in maintenance costs for the offshore wind industry. © 2021 Taylor & Francis Group, London.},
keywords={Cost effectiveness;  Costs;  Decision trees;  Electric utilities;  Offshore oil well production;  Offshore wind farms;  Preventive maintenance;  Reinforcement learning, Corrective maintenance;  Cost effective;  Fault types;  Historical data;  Maintenance Action;  Maintenance planning;  Offshore vessels;  Optimisations;  Simulated environment;  Supervisory control, Deep learning},
references={Abdallah, I., Dertimanis, V., Mylonas, H., Tatsis, K., Chatzi, E., Dervilis, N., Worden, K., Maguire, E., Fault diagnosis of wind turbine structures using decision tree learning algorithms with big data (2018) Proceedings of the European Safety and Reliability Conference, pp. 3053-3061. , (June) Trondheim, Norway; Allahviranloo, M., Chow, J. Y., Recker, W. W., Selective vehicle routing problems under uncertainty without recourse (2014) Transportation Research Part E: Logistics and Transportation Review, 62 (C), pp. 68-88; Borchersen, A., Larsen, J., Stoustrup, J., Fault analysis of wind turbines based on error messages and work orders (2012) I F A C Workshop Series, 47 (3), pp. 4316-4321. , (November). 10th European Workshop on Advanced Control and Diagnosis 2012; Conference date: 08-11-2012 Through 09-11–2012; Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., Zaremba, W., (2016) Openai gym; Chatterjee, J., Dethlefs, N., Deep learning with knowledge transfer for explainable anomaly prediction in wind turbines (2020) Wind Energy n/a(n/a), pp. 1-18. , April); Chatterjee, J., Dethlefs, N., A dual transformer model for intelligent decision support for maintenance of wind turbines (to appear) (2020) International Joint Conference on Neural Networks (IJCNN), , July) Glasgow (UK); Chawla, N. V., Bowyer, K. W., Hall, L. O., Kegelmeyer, W. P., Smote: Synthetic minority over-sampling technique (2002) Journal of Artificial Intelligence Research, 16, p. 321â357; Chen, C.-Y., Yao, W., Liu, Z., Zhu, Z., Zhao, Y.-G., Chi, T., Measuring impacts of urban environmental elements on housing prices based on multisource data—a case study of shanghai, china (2020) ISPRS International Journal of Geo-Information, 9, p. 106. , (02); Chen, J., Development of offshore wind power in china (2011) Renewable and Sustainable Energy Reviews, 15 (9), pp. 5013-5020; Chen, T., Guestrin, C., XGBoost: A scalable tree boosting system (2016) Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD’16, pp. 785-794. , New York, NY, USA, ACM; Dalgic, Y., Lazakis, I., Turan, O., Investigation of optimum crew transfer vessel fleet for offshore wind farm maintenance operations (2015) Wind Engineering, 39, pp. 31-52. , (02); Dawid, R., Mcmillan, D., Revie, M., Decision support tool for offshore wind farm vessel routing under uncertainty (2018) Energies, 11, p. 2190. , (08); Elosegui, U., Egaña, I., Ulazia, A., Berastegi, G., Pitch angle misalignment correction based on benchmarking and laser scanner measurement in wind farms (2018) Energies, 11, p. 3357. , (12); Giebel, G., Hasager, C., (2016) An Overview of Offshore Wind Farm Design, pp. 337-346. , (08). MARE-WINT. Springer, Cham; Halvorsen-Weare, E., Gundegjerde, C., Halvorsen, I., Hvattum, L. M., Nonås, L., Vessel fleet analysis for maintenance operations at offshore wind farms (2013) Energy Procedia, 35, pp. 167-176. , (08); Hansen, K. S., Larsen, G. C., Wind shear extremes at possible offshore wind turbine locations (2003) Wind Engineering, 27 (5), pp. 339-349; Irawan, C. A., Ouelhadj, D., Jones, D., Ståhane, M., Sperstad, I. B., Optimisation of maintenance routing and scheduling for offshore wind farms (2017) European Journal of Operational Research, 256 (1), pp. 76-89; Jang, B., Kim, M., Harerimana, G., Kim, J., Q-learning algorithms: A comprehensive classification and applications (2019) IEEE Access PP, pp. 1-1. , (09); Jang, B., Kim, M., Harerimana, G., Kim, J. W., Q-learning algorithms: A comprehensive classification and applications (2019) IEEE Access, 7, pp. 133653-133667; Jin, X., Cheng, F., Peng, Y., Qiao, W., Qu, L., Drivetrain gearbox fault diagnosis: Vibration- and current-based approaches (2018) IEEE Industry Applications Magazine, 24 (6), pp. 56-66; Kasim, M. F., Playing the game of congklak with reinforcement learning (2016) 2016 8th International Conference on Information Technology and Electrical Engineering (ICITEE), pp. 1-5; Li, M., Fu, X., Li, D., Diabetes prediction based on xgboost algorithm (2020) IOP Conference Series: Materials Science and Engineering, 768, p. 072093. , (03); Lin, C.-J., Jhang, J.-Y., Lin, H.-Y., Lee, C.-L., Young, K.-Y., Using a reinforcement q-learning-based deep neural network for playing video games (2019) Electronics, 8, p. 1128. , (10); Liu, R., Zou, J., The effects of memory replay in reinforcement learning (2018) 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 478-485. , (10); Liu, Y., Luo, Y., Zhong, Y., Chen, X., Liu, Q., Peng, J., Sequence modeling of temporal credit assignment for episodic reinforcement learning (2019), pp. 1-15. , CoRR abs 1905.13420; Lundberg, S. M., Lee, S.-I., A unified approach to interpreting model predictions (2017) Advances in Neural Information Processing Systems, 30, pp. 4765-4774. , I. Guyon, U. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds), Curran Associates, Inc; Milborrow, D., (2019) Wind Energy Development, pp. 3-22. , (10). The Age of Wind Energy, Springer; Mousavi, S., Schukat, M., Howley, E., Deep reinforcement learning: An overview (2018) Lecture Notes in Networks and Systems, pp. 426-440. , (06); Ohnishi, S., Uchibe, E., Yamaguchi, Y., Nakanishi, K., Yasui, Y., Ishii, S., Constrained deep q-learning gradually approaching ordinary q-learning (2019) Frontiers in Neurorobotics, 13, p. 103. , (12); System performance, availability and reliability trend analysis (2019) ORE Catapult, 1 (1), pp. 1-26. , ORE Catapult; Poulsen, T., Hasager, C., How expensive is expensive enough? opportunities for cost reductions in offshore wind energy logistics (2016) Energies, 9 (6), p. 437. , (July); Qiu, Y., Feng, Y., Tavner, P., Richardson, P., Erdos, G., Chen, B., Wind turbine scada alarm analysis for improving reliability (2012) Wind Energy, 15, pp. 951-966. , (11); Si, Y., Qian, L., Mao, B., Zhang, D., A data-driven approach for fault detection of offshore wind turbines using random forests (2017) IECON 2017-43rd Annual Conference of the IEEE Industrial Electronics Society, pp. 3149-3154. , (October) Beijing, China; (2018) Wpd offshore goes for service vessels, , Siemens; Sutton, R. S., Barto, A. G., (2018) Reinforcement Learning: An Introduction, , Cambridge, MA, USA: A Bradford Book; Van Seijen, H., Van Hasselt, H., Whiteson, S., Wiering, M., A theoretical and empirical analysis of expected sarsa (2009) IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, pp. 177-184. , (05); Wen, F., Wang, X., Xu, X., Hierarchical sarsa learning based route guidance algorithm (2019) Journal of Advanced Transportation, 2019, pp. 1-12. , (06); Zhao, Y., Li, D., Dong, A., Kang, D., Lv, Q., Shang, L., Fault prediction and diagnosis of wind turbine generators using scada data (2017) Energies, 108, p. 1210},
editor={Soares S.C.},
publisher={CRC Press/Balkema},
isbn={9780367681319},
language={English},
abbrev_source_title={Dev. Renew. Energies Offshore - Proc. Int. Conf. Renew. Energies Offshore, RENEW},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang20203052,
author={Zhang, Q. and Yin, G.G. and Wang, L.Y.},
title={Two-time scale reinforcement learning and applications to production planning},
journal={IET Control Theory and Applications},
year={2020},
volume={14},
number={19},
pages={3052-3061},
doi={10.1049/iet-cta.2020.0049},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102345304&doi=10.1049%2fiet-cta.2020.0049&partnerID=40&md5=cdd13bf2589f7ec4b5b60c17286a4473},
affiliation={Department of Mathematics, University of Georgia, Athens, GA  30602, United States; Department of Mathematics, University of Connecticut, Storrs, CT  06269-1009, United States; Department of Electrical and Computer Engineering, Wayne State University, Detroit, MI  48202, United States},
abstract={This study is concerned with reinforcement learning enhanced by two-time scale approximations. Many systems arising in applications are large and complex. To treat these problems, it is often beneficial, and sometimes necessary, to reduce the dimensionality and aggregate states that are ‘close’ to each other. In this study, the authors propose a two-time scale reinforcement learning method for such an aggregation process. In particular, they present how to classify states that are ‘close’ and demonstrate the effectiveness of the authors' state aggregation based two-time scale methods. Thus the problem can be considered as using learning for identifying the system. A production planning problem with failure-prone machines is used throughout this study to illustrate the main ideas, key steps and results. Monte Carlo simulations are used to generate the random environment. © The Institution of Engineering and Technology 2020.},
keywords={Learning systems;  Monte Carlo methods;  Planning;  Production control;  Time measurement, Aggregate state;  Aggregation process;  Failure prone machines;  Production Planning;  Random environment;  Reinforcement learning method;  State aggregation;  Two time scale, Reinforcement learning},
funding_details={Army Research OfficeArmy Research Office, ARO, W911NF-19-1-0176},
funding_text 1={This research was supported in part by the Army Research Office under grant W911NF-19-1-0176.},
references={Han, J., Jentzen, A.E.W., Solving high-dimensional partial differential equations using deep learning (2018) Proc. Natl. Acad. Sci, 115 (34), pp. 8505-8510; Bai, E.-W., Li, K., Zhao, W., Variable selection in nonlinear nonparametric system identification (2016) Sci. Sin.: Math, 46 (10), pp. 1383-1400; Guo, J., Zhao, Y., Identification for Wiener-Hammerstein systems under quantized inputs and quantized output observations (2019) Asian J. Control, pp. 1-10. , https://doi.org/10.1002/asjc.2237; Mu, B., Bai, E.-W., Zheng, W.X., A globally consistent nonlinear least squares estimator for identification of nonlinear rational systems (2017) Automatica, 77, pp. 322-335; Mu, B., Chen, T., Ljung, L., On asymptotic properties of hyperparameter estimators for kernel-based regularization methods (2018) Automatica, 94, pp. 381-395; Yin, G., Zhang, Q., (2013) ‘, , Springer, New York, 2nd edn; Yin, G., Zhang, H., Zhang, Q., (2013) Two-Time-Scale Markovian Systems and Applications (Sci, , Press, Beijing; Simon, H.A., O, A., Aggregation of variables in dynamic systems (1961) Econometrica, 29, pp. 111-138; O'malley, R.E., Jr., (1991) Singular Perturbation Methods for Ordinary Differential Equations, , Springer-Verlag, New York; Sethi, S.P., Zhang, Q., (1994) Hierarchical Decision Making in Stochastic Manufacturing Systems, , Birkhäuser, Boston; Bensoussan, A., Liu, R., Sethi, S., Optimality of an (S,S) policy with compound Poisson and diffusion demands: A quasi-variational inequalities approach (2005) SIAM J. Control Optim, 44, pp. 1650-1676; Costa, O.L.V., Dufour, F., (2013) Continuous Average Control of Piecewise Deterministic Markov Processes, Springer Briefs in Math, , Springer, New York; Usuga Cadavid, J.P., Lamouri, S., Grabot, B., Machine learning in production planning and control: A review of empirical literature (2019) Ifacpapersonline, 52, pp. 385-390; Usuga Cadavid, J.P., Lamouri, S., Grabot, B., Machine learning applied in production planning and control: A state-of-the-art in the era of industry 4.0 (2020) J. Intell. Manuf, 31, pp. 1531-1558; Kushner, H.J., Yin, G., (1997) Stochastic Approximation Algorithms and Applications, , Springer, New York; Akella, R., Kumar, P.R., Optimal control of production rate in a failure prone manufacturing system (1986) IEEE Tran. Autom. Control, 31, pp. 116-126; Zhang, Q., Yin, G., On nearly optimal controls of hybrid LQG problems (1999) IEEE Trans. Autom. Control, 44 (12), pp. 2271-2282; Zhang, Q., Yin, G., Liu, R.H., A near-optimal selling rule for a two-timescale market model (2005) Multiscale Model. Simul.: A SIAM J, 4, pp. 172-193; Tran, K., Yin, G., Optimal harvesting strategies for stochastic ecosystems (2017) IET Control Theory Appl, 11 (15), pp. 2521-2530; Phillips, R.G., Kokotovic, P.V., A singular perturbation approach to modeling and control of Markov chains (1981) IEEE Trans. Autom. Control, 26, pp. 1087-1094; Tsitsiklis, J.N., Asynchronous stochastic approximation and Q-learning (1994) Mach. Learn, 16, pp. 185-202; Watkins, C.I.C.H., Dayan, P., Q-learning (1992) Mach. Learn, 8, pp. 279-292; Yin, G., Xu, C., Wang, L.Y., Q-learning algorithms with random truncation bounds and applications to effective parallel computing (2008) J. Optim. Theory Appl, 137, pp. 435-451; Berkovitz, L., (1974) Optimal Control Theory, , Springer-Verlag, Berlin and New York; Perthame, B., Perturbed dynamical systems with an attracting singularity and weak viscosity limits in Hamilton-Jacobi equations (1987) Tech. Rep, 18. , Ecole Normale Supérieure; Kushner, H.J., (1984) Approximation and Weak Convergence Methods for Random Processes, with Applications to Stochastic Systems Theory, , MIT Press, Cambridge, MA},
correspondence_address1={Zhang, Q.; Department of Mathematics, United States; email: qz@uga.edu},
publisher={John Wiley and Sons Inc},
issn={17518644},
language={English},
abbrev_source_title={IET Control Theory Appl.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gros20203032,
author={Gros, T.P. and Gros, J. and Wolf, V.},
title={Real-Time Decision Making for a Car Manufacturing Process Using Deep Reinforcement Learning},
journal={Proceedings - Winter Simulation Conference},
year={2020},
volume={2020-December},
pages={3032-3044},
doi={10.1109/WSC48552.2020.9383884},
art_number={9383884},
note={cited By 0; Conference of 2020 Winter Simulation Conference, WSC 2020 ; Conference Date: 14 December 2020 Through 18 December 2020;  Conference Code:168205},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103878644&doi=10.1109%2fWSC48552.2020.9383884&partnerID=40&md5=f72e7fbc9e02999eb0818ae27a5f7dc8},
affiliation={Saarland Informatics Campus Saarland University, Saarbrücken, 66123, Germany},
abstract={Computer simulations of manufacturing processes are in widespread use for optimizing production planning and order processing. If unforeseeable events are common, real-time decisions are necessary to maximize the performance of the manufacturing process. Pre-trained AI-based decision support offers promising opportunities for such time-critical production processes. Here, we explore the effectiveness of deep reinforcement learning for real-time decision making in a car manufacturing process. We combine a simulation model of a central production part, the line buffer, with deep reinforcement learning algorithms, in particular with deep Q-Learning and Monte Carlo tree search. We simulate two different versions of the buffer, a single-agent and a multi-agent one, to generate large amounts of data and train neural networks to represent near-optimal strategies. Our results show that deep reinforcement learning performs extremely well and the resulting strategies provide near-optimal decisions in real-time, while alternative approaches are either slow or give strategies of poor quality. © 2020 IEEE.},
keywords={Decision making;  Decision support systems;  Learning algorithms;  Manufacture;  Monte Carlo methods;  Multi agent systems;  Production control;  Reinforcement learning, Car manufacturing;  Decision supports;  Large amounts of data;  Manufacturing process;  Monte-Carlo tree searches;  Production Planning;  Real time decision-making;  Real time decisions, Deep learning},
funding_details={European Regional Development FundEuropean Regional Development Fund, ERDF},
funding_text 1={This work has been partially funded by the European Regional Development Fund (ERDF).},
references={Agostinelli, F., McAleer, S., Shmakov, A., Baldi, P., Solving the Rubik's cube with deep reinforcement learning and search (2019) Nature Machine Intelligence, 1 (8), pp. 356-363; Bengio, Y., Louradour, J., Collobert, R., Weston, J., Curriculum learning (2009) Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, pp. 41-48. , New York, NY, USA: Association for Computing Machinery; Busoniu, L., Babuska, R., De Schutter, B., A comprehensive survey of multiagent reinforcement learning (2008) IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38 (2), pp. 156-172; Chen, W., Xu, Y., Wu, X., (2017) Deep Reinforcement Learning for Multi-resource Multi-machine Job Scheduling, , arXiv preprint arXiv:1711.07440. Accessed 15th September 2020; Finnsson, H., Björnsson, Y., Simulation-based approach to general game playing (2008) Proceedings of the National Conference on Artificial Intelligence, 1, pp. 259-264; Genesereth, M., Thielscher, M., General game playing (2014) Synthesis Lectures on Artificial Intelligence and Machine Learning, 8 (2), pp. 1-229; Gros, T.P., Groß, J., Wolf, V., (2020) Real-time Decision Making for A Car Manufacturing Process Using Deep Reinforcmenet Learning-Technical Report, , https://mgit.cs.uni-saarland.de/timopgros/carmanufacturing, Accessed 7th July 2020; Gupta, J.K., Egorov, M., Kochenderfer, M., Cooperative multi-agent control using deep reinforcement learning (2017) Autonomous Agents and Multiagent Systems, pp. 66-83. , edited by G. Sukthankar and J. A. Rodriguez-Aguilar Cham: Springer International Publishing; Kocsis, L., Szepesvári, C., Bandit based monte-carlo planning (2006) Machine Learning: ECML 2006, pp. 282-293. , edited by J. fürnkranz, T. Scheffer, and M. Spiliopoulou Berlin, Heidelberg: Springer Berlin Heidelberg; Kool, W., Van Hoof, H., Welling, M., (2018) Attention, Learn to Solve Routing Problems!, , Accessed 15th September 2020 arXiv preprint arXiv:1803.08475; Mao, H., Alizadeh, M., Menache, I., Kandula, S., Resource management with deep reinforcement learning (2016) Proceedings of the 15th ACM Workshop on Hot Topics in Networks, HotNets '16, pp. 50-56. , New York, NY, USA: Association for Computing Machinery; Mazyavkina, N., Sviridov, S., Ivanov, S., Burnaev, E., (2020) Reinforcement Learning for Combinatorial Optimization: A Survey, , arXiv preprint arXiv:2003.03600. Accessed 15th September 2020; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., (2013) Playing Atari with Deep Reinforcement Learning, , arXiv preprint arXiv:1312.5602. Accessed 15th September 2020; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Hassabis, D., Human-level control through deep reinforcement learning (2015) Nature, 518 (7540), pp. 529-533; Nazari, M., Oroojlooy, A., Snyder, L., Takac, M., Reinforcement learning for solving the vehicle routing problem (2018) Advances in Neural Information Processing Systems, 31, pp. 9839-9849. , edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett Curran Associates, Inc; Schadd, M.P.D., Winands, M.H.M., Van Den Herik, H.J., Chaslot, G.M.J.B., Uiterwijk, J.W.H.M., Single-player monte-carlo tree search (2008) Computers and Games, pp. 1-12. , edited by H. J. van den Herik, X. Xu, Z. Ma, and M. H. M. Winands Berlin, Heidelberg: Springer Berlin Heidelberg; Schwartz, H.M., (2014) Multi-agent Machine Learning: A Reinforcement Approach, , Hoboken, New Jersey, USA: John Wiley & Sons; Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Driessche Den, G.Van, Schrittwieser, J., Hassabis, D., Mastering the game of Go with deep neural networks and tree search (2016) Nature, 529 (7587), pp. 484-489; Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Graepel, T., (2017) Mastering Chess and Shogi by Self-play with A General Reinforcement Learning Algorithm, , arXiv preprint arXiv:1712.01815. Accessed 15th September 2020; Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Hassabis, D., A general reinforcement learning algorithm that masters chess, shogi, and go through self-play (2018) Science, 362 (6419), pp. 1140-1144; Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Hassabis, D., Mastering the game of Go without human knowledge (2017) Nature, 550 (7676), pp. 354-359; Sutton, R.S., Barto, A.G., (2018) Reinforcement Learning: An Introduction, , Second ed; Waschneck, B., Reichstaller, A., Belzner, L., Altenmüller, T., Bauernhansl, T., Knapp, A., Kyek, A., Optimization of global production scheduling with deep reinforcement learning (2018) Procedia CIRP, 72 (1), pp. 1264-1269},
editor={Bae K.-H., Feng B., Kim S., Lazarova-Molnar S., Zheng Z., Roeder T., Thiesing R.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={08917736},
isbn={9781728194998},
coden={WSCPD},
language={English},
abbrev_source_title={Proc. Winter Simul. Conf.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2020685,
author={Wang, L. and Ai, W. and Deng, T. and Shen, Z.-J.M. and Hong, C.},
title={Optimal production ramp-up in the smartphone manufacturing industry},
journal={Naval Research Logistics},
year={2020},
volume={67},
number={8},
pages={685-704},
doi={10.1002/nav.21886},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077910895&doi=10.1002%2fnav.21886&partnerID=40&md5=a9cd39d2b646884292599ee73181927a},
affiliation={Department of Industrial Engineering, Tsinghua University, Beijing, China; Department of Industrial Engineering and Operations Research, University of California Berkeley, Berkeley, CA, United States; Department of Civil and Environmental Engineering, University of California Berkeley, Berkeley, CA, United States; Lenovo Group Ltd., Beijing, China},
abstract={Motivated by challenges in the smartphone manufacturing industry, we develop a dynamic production ramp-up model that can be applied to economically satisfy nonstationary demand for short-life-cycle products by high-tech companies. Due to shorter life cycles and more rapid evolution of smartphones, production ramp-up has been increasingly critical to the success of a new smartphone. In the production ramp-up, the key challenge is to match the increasing capacity to nonstationary demand. The high-tech smartphone manufacturers are urged to jointly consider the effect of increasing capacity and decreasing demand. We study the production planning problem using a high-dimensional Markov decision process (MDP) model to characterize the production ramp-up. To address the curse of dimensionality, we refine Monte Carlo tree search (MCTS) algorithm and theoretically analyze its convergence and computational complexity. In a real case study, we find that the MDP model achieves revenue improvement by stopping producing the existing product earlier than the benchmark policy. In synthetic instances, we validate that the proposed MCTS algorithm saves computation time without loss of solution quality compared with traditional value iteration algorithm. As part of the Lenovo production solution, our MDP model enables high-tech smartphone manufacturers to better plan the production ramp-up. © 2020 Wiley Periodicals, Inc.},
author_keywords={Markov decision process;  Monte Carlo tree search;  production ramp-up;  reinforcement learning;  smartphone manufacturing industry},
keywords={Iterative methods;  Manufacture;  Markov processes;  Monte Carlo methods;  Production control;  Reinforcement learning;  Smartphones;  Trees (mathematics), Curse of dimensionality;  Increasing capacities;  Manufacturing industries;  Markov Decision Processes;  Monte Carlo tree search (MCTS);  Monte-Carlo tree searches;  Production ramp-up;  Short life cycle products, Life cycle},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 71822105, 71991462, 91746210},
funding_text 1={The authors thank the editor Ming Hu, associate editor and two anonymous referees for their constructive suggestions that have significantly improved this paper. Tianhu Deng acknowledges the support from the National Natural Science Foundation of China (Grant 71822105 and 71991462). Zuo‐Jun Max Shen acknowledges the support from the National Natural Science Foundation of China (Grant 91746210 and 71991462).},
references={Almgren, H., Pilot production and manufacturing start-up: The case of Volvo S80 (2000) International Journal of Production Research, 38, pp. 4577-4588; Aydin, M.E., Öztemel, E., Dynamic job-shop scheduling using reinforcement learning agents (2000) Robotics and Autonomous Systems., 33, pp. 169-178; Aytac, B., Wu, S.D., Characterization of demand for short life-cycle technology products (2013) Annals of Operations Research., 203, pp. 255-277; Ball, P., Roberts, S., Natalicchio, A., Scorzafave, C., Modelling production ramp-up of engineering products (2011) Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture, 225, pp. 959-971; Baud-Lavigne, B., Bassetto, S., Penz, B., A broader view of the economic design of the X-bar chart in the semiconductor industry (2010) International Journal of Production Research, 48, pp. 5843-5857; Bertsimas, D., Griffith, J.D., Gupta, V., Kochenderfer, M.J., Mišić, V.V., A comparison of Monte Carlo tree search and rolling horizon optimization for large-scale dynamic resource allocation problems (2017) European Journal of Operational Research., 263, pp. 664-678; Bhatnagar, R., Saddikutti, V., Rajgopalan, A., Contingent manpower planning in a high clock speed industry (2007) International Journal of Production Research, 45, pp. 2051-2072; Breiman, L., Probability (1992) Mathematical Gazette, 77, p. 128; Browne, C., Powley, E., Whitehouse, D., Lucas, S., Cowling, P., Rohlfshagen, P., Colton, S., A survey of Monte Carlo tree search methods (2012) IEEE Transactions on Computational Intelligence and AI in Games, 4, pp. 1-43; Chaslot, G.M.J.B., De Jong, S., Saito, J.-T., Uiterwijk, J.W.H.M., Monte-Carlo tree search in production management problems (2006) Proceedings of the 18th BeNeLux Conference on Artificial Intelligence, pp. 91-98. , Namur, Belgium Springer; (2018), https://www.dpreview.com/news/3840736084/fujifilm-plans-to-increase-interchangeable-lens-production-capacity-as-demand-grows, Fujifilm plans to increase interchangeable lens production capacity as demand grows, Retrieved from; Doltsinis, S., Ferreira, P., Lohse, N., Reinforcement learning for production ramp-up: A q-batch learning approach (2013) International Conference on Machine Learning and Applications, pp. 610-615. , Boca Raton, FL IEEE; Doltsinis, S., Ferreira, P., Lohse, N., An MDP model-based reinforcement learning approach for production station ramp-up optimization: Q-learning analysis (2014) IEEE Transactions on Systems Man & Cybernetics Systems., 44, pp. 1125-1138; Doltsinis, S.C., Lohse, N., A model-free reinforcement learning approach using Monte Carlo method for production ramp-up policy improvement—A copy exactly test case (2012) IFAC Proceedings Volumes, 45, pp. 1628-1634; Fjällström, S., Säfsten, K., Harlin, U., Stahre, J., Information enabling production ramp-up (2009) Journal of Manufacturing Technology Management, 20, pp. 178-196; (2015), http://fortune.com/2015/01/23/chart-demand-for-apples-iphone-66-stays-strong/, Chart Demand for Apple, s iPhone 6/6+ stays, Author. Retrieved from; Glock, C.H., Jaber, M.Y., Zolfaghari, S., Production planning for a ramp-up process with learning in production and growth in demand (2012) International Journal of Production Research, 50, pp. 5707-5718; Hu, K., Acimovic, J., Erize, F., Thomas, D., Van Mieghem, J.A., Forecasting product life cycle curves: Practical approach and empirical analysis (2019) Manufacturing & Service Operations Management, 21, pp. 66-85; Jiang Daniel, R., Alkanj, L., Powell, W.B., (2017), . Monte Carlo tree search with sampled information relaxation dual bounds. Technical Report, University of Pittsburgh, Pittsburgh, PA; Kartal, B., Koenig, J., Guy, S.J., Generating believable stories in large domains (2013) Ninth Artificial Intelligence and Interactive Digital Entertainment Conference, , Boston, Massachusetts; Kontio, J., Haapasalo, H., A project model in managing production ramp-up—A case study in wire harness industry (2005) International Journal of Innovation & Technology Management, 2, pp. 101-117; Kushner, H.J., Yin, G.G., (2003) Stochastic approximation and recursive algorithms and applications, , Berlin, Springer; Lovejoy, B., (2017), https://9to5mac.com/2017/05/30/nintendo-switch-components-apple/, Apple's appetite for components reportedly limiting Nintendo's production capacity for the switch, 9to5Mac. Retrieved from; Mao, C., Shen, Z., A reinforcement learning framework for the adaptive routing problem in stochastic time-dependent network (2018) Transportation Research Part C: Emerging Technologies, 93, pp. 179-197; Matta, A., Tomasella, M., Valente, A., Impact of ramp-up on the optimal capacity-related reconfiguration policy (2007) International Journal of Flexible Manufacturing Systems, 19, pp. 173-194; McKiernan, D., (2015), https://www.eflexsystems.com/lean-manufacturing-blog/scenarios-of-common-assembly-line-issues, 9 scenarios of common assembly line issues, eFlex Systems. Retrieved from; Meier, H., Homuth, M., Holistic ramp-up management in SME-networks (2006) Proceedings of the 16th CIRP International Design Seminar, , Kananaskis, Canada; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., (2013), . Playing Atari with deep reinforcement learning. Technical report. Deepmind Technologies, [cs.LG]; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Hassabis, D., Human-level control through deep reinforcement learning (2015) Nature, 518, p. 529; Nair, S., (2018), https://www.thestar.com.my/tech/tech-news/2018/02/26/1point5-billion-smartphones-were-sold-in-2017/, Gartner 1.5 billion smartphones were sold in 2017, The Star Online. Retrieved from; Peter, D., Steve, U., Move ordering vs heavy playouts: Where should heuristics be applied in Monte Carlo Go (2007) Proceedings of the 3rd North American Game-on Conference, pp. 273-280. , . Gainesville, Florida; Rasmussen, P., (2018), https://news.ewmfg.com/blog/electronics-components-shortage-what-you-need-to-know, Electronics components shortage What you need to know east west manufacturing, Retrieved from; Rdutnik, M., (2017), https://www.androidauthority.com/major-smartphone-releases-2017-2-746078/, 2017 release calendar All the major smartphones we're expecting this year, Android Authority. Retrieved from; Shah, N., (2018), https://www.counterpointresearch.com/apples-revenue-super-cycle/, Apple's revenue “super cycle” counterpoint, Retrieved from; Shahrabi, J., Adibi Mohammad, A., Mahootchi, M., A reinforcement learning approach to parameter estimation in dynamic job shop scheduling (2017) Computers & Industrial Engineering, 110, pp. 75-82; Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driessche, G., Hassabis, D., Mastering the game of Go with deep neural networks and tree search (2016) Nature, 529, p. 484; Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Hassabis, D., A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play (2018) Science, 362, pp. 1140-1144; Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hassabis, D., Mastering the game of Go without human knowledge (2017) Nature, 550, p. 354; (2015), https://www.smart.uio.no/research/life-cycle-of-mobile-phones/, Life cycle of mobile phones, Author. Retrieved from; (2018), https://www.statista.com/statistics/271491/worldwide-shipments-of-smartphones-since-2009/, Global smartphone shipments from 2009 to 2018 (in million units), Author. Retrieved from; Surbier, L., Alpan, G., Blanco, E., A comparative study on production ramp-up: State-of-the-art and new challenges (2014) Production Planning & Control., 25, pp. 1264-1286; Sutton Richard, S., Barto, A.G., (2018) Reinforcement learning: An introduction, , Cambridge, MA, MIT Press; Terwiesch, C., Bohn, R., Chea, K., International product transfer and production ramp-up: A case study from the data storage industry (2001) R&D Management, 31, pp. 435-451; Terwiesch, C., Bohn, R.E., Learning and process improvement during production ramp-up (1998) International Journal of Production Economics, 70, pp. 1-19; Terwiesch, C., Xu, Y., The copy-exactly ramp-up strategy: Trading-off learning with process change (2004) IEEE Transactions on Engineering Management., 51, pp. 70-84; Wang, Y.-C., Usher, J.M., Application of reinforcement learning for agent-based production scheduling (2005) Engineering Applications of Artificial Intelligence, 18, pp. 73-82; Wei, Z., Dietterich, T.G., A reinforcement learning approach to job-shop scheduling (1995) International Joint Conference on Artificial Intelligence, pp. 1114-1120. , . Montreal, Quebec Morgan Kaufmann; Winkler, H., Heins, M., Nyhuis, P., A controlling system based on cause-effect relationships for the ramp-up of production systems (2007) Production Engineering, 1, pp. 103-111},
correspondence_address1={Deng, T.; Department of Industrial Engineering, China; email: deng13@mail.tsinghua.edu.cn},
publisher={John Wiley and Sons Inc},
issn={0894069X},
coden={NRLOE},
language={English},
abbrev_source_title={Nav Res Logist},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2020,
author={Zhang, N. and Si, W.},
title={Deep reinforcement learning for condition-based maintenance planning of multi-component systems under dependent competing risks},
journal={Reliability Engineering and System Safety},
year={2020},
volume={203},
doi={10.1016/j.ress.2020.107094},
art_number={107094},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086987070&doi=10.1016%2fj.ress.2020.107094&partnerID=40&md5=047932a6cd75ef89e56e6c5eec063ea3},
affiliation={Massachusetts Mutual Life Insurance Company, New York City, NY  10010, United States; Department of Industrial, Systems and Manufacturing Engineering, Wichita State University, Wichita, KS  67260, United States},
abstract={Condition-Based Maintenance (CBM) planning for multi-component systems has been receiving increasing attention in recent years. Most existing research on CBM assumes that preventive maintenances should be conducted when the degradations of system components reach specific threshold levels upon inspection. However, the search of optimal maintenance threshold levels is often efficient for low-dimensional CBM but becomes challenging if the number of components gets large, especially when those components are subject to complex dependencies. To overcome the challenge, in this paper we propose a novel and flexible CBM model based on a customized deep reinforcement learning for multi-component systems with dependent competing risks. Both stochastic and economic dependencies among the components are considered. Specifically, different from the threshold-based decision making paradigm used in traditional CBM, the proposed model directly maps the multi-component degradation measurements at each inspection epoch to the maintenance decision space with a cost minimization objective, and the leverage of deep reinforcement learning enables high computational efficiencies and thus makes the proposed model suitable for both low and high dimensional CBM. Various numerical studies are conducted for model validations. © 2020 Elsevier Ltd},
author_keywords={Cost minimization;  Deep Q network;  Failure dependency;  Maintenance;  Markov decision process},
keywords={Decision making;  Learning systems;  Maintenance;  Reinforcement learning;  Stochastic systems, Condition-based maintenance;  Cost minimization;  Dependent competing risks;  Maintenance decisions;  Multi-component systems;  Number of components;  Optimal maintenance;  System components, Deep learning},
references={Dhillon, B.S., Engineering maintenance: a modern approach (2002), CRC Press; Zhang, N., Yang, Q., Optimal maintenance planning for repairable multi-component systems subject to dependent competing risks (2015) IIE Trans, 47, pp. 521-532; Ding, F., Tian, Z., Opportunistic maintenance optimization for wind turbine systems considering imperfect maintenance actions (2011) Int J Reliab Qual Saf Eng, 18, pp. 463-481; Wang, X., Zhou, H., Parlikad, A.K., Xie, M., Imperfect preventive maintenance policies with unpunctual execution (2020) IEEE Trans Reliab, pp. 1-13; Chen, N., Ye, Z.-S., Xiang, Y., Zhang, L., Condition-based maintenance using the inverse Gaussian degradation model (2015) Eur J Oper Res, 243, pp. 190-199; Tian, Z., Liao, H., Condition based maintenance optimization for multi-component systems using proportional hazards model (2011) Reliab Eng Syst Saf, 96, pp. 581-589; Chen, N., Chen, Y., Li, Z., Zhou, S., Sievenpiper, C., Optimal variability sensitive condition-based maintenance with a Cox PH model (2011) Int J Prod Res, 49, pp. 2083-2100; Dieulle, L., Bérenguer, C., Grall, A., Roussignol, M., Sequential condition-based maintenance scheduling for a deteriorating system (2003) Eur J Oper Res, 150, pp. 451-461; Grall, A., Bérenguer, C., Dieulle, L., A condition-based maintenance policy for stochastically deteriorating systems (2002) Reliab Eng Syst Saf, 76, pp. 167-180; Liao, H., Elsayed, E.A., Chan, L.-Y., Maintenance of continuously monitored degrading systems (2006) Eur J Oper Res, 175, pp. 821-835; Jonge, B., Teunter, R., Tinga, T., The influence of practical factors on the benefits of condition-based maintenance over time-based maintenance (2017) Reliab Eng System Saf, 158, pp. 21-30; Liu, B., Wu, S., Xie, M., Kuo, W., A condition-based maintenance policy for degrading systems with age-and state-dependent operating cost (2017) Eur J Oper Res, 263, pp. 879-887; Liu, B., Do, P., Iung, B., Xie, M., Stochastic filtering approach for condition-based maintenance considering sensor degradation (2019) IEEE Trans Autom Sci Eng, 17, pp. 177-190; Koochaki, J., Bokhorst, J.A., Wortmann, H., Klingenberg, W., Condition based maintenance in the context of opportunistic maintenance (2012) Int J Prod Res, 50, pp. 6918-6929; Shahraki, A.F., Yadav, O.P., Vogiatzis, C., Selective maintenance optimization for multi-state systems considering stochastically dependent components and stochastic imperfect maintenance actions (2020) Reliab Eng Syst Saf, 196; Van Horenbeek, A., Pintelon, L., A dynamic predictive maintenance policy for complex multi-component systems (2013) Reliab Eng Syst Saf, 120, pp. 39-50; Ding, F., Tian, Z., Opportunistic maintenance for wind farms considering multi-level imperfect maintenance thresholds (2012) Renew Energy, 45, pp. 175-182; Rasmekomen, N., Parlikad, A.K., Condition-based maintenance of multi-component systems with degradation state-rate interactions (2016) Reliab Eng Syst Saf, 148, pp. 1-10; Li, H., Deloux, E., Dieulle, L., A condition-based maintenance policy for multi-component systems with Lévy copulas dependence (2016) Reliaby Eng Syst Saf, 149, pp. 44-55; Do, P., Assaf, R., Scarf, P., Iung, B., Modelling and application of condition-based maintenance for a two-component system with stochastic and economic dependencies (2019) Reliab Eng Syst Saf, 182, pp. 86-97; Huynh, K.T., Barros, A., Bérenguer, C., Multi-level decision-making for the predictive maintenance of $ k $-out-of-$ n $: f deteriorating systems (2014) IEEE Trans Reliab, 64, pp. 94-117; Wei, S., Bao, Y., Li, H., Optimal policy for structure maintenance: a deep reinforcement learning framework (2020) Struct Saf, 83; Alaswad, S., Xiang, Y., A review on condition-based maintenance optimization models for stochastically deteriorating system (2017) Reliab Eng Syst Saf, 157, pp. 54-63; de Jonge, B., Scarf, P.A., A review on maintenance optimization (2019) Eur J Oper Res, 285, pp. 805-824; Keizer, M.C.O., Flapper, S.D.P., Teunter, R.H., Condition-based maintenance policies for systems with multiple dependent components: a review (2017) Eur J Oper Res, 261, pp. 405-420; Verbert, K., De Schutter, B., Babuška, R., Timely condition-based maintenance planning for multi-component systems (2017) Reliab Eng Syst Saf, 159, pp. 310-321; Liu, Q., Lv, W., Multi-component manufacturing system maintenance scheduling based on degradation information using genetic algorithm (2015) Ind Manag Data Syst, 115, pp. 1412-1434; Lin, Y.-H., Li, Y.-F., Zio, E., Fuzzy reliability assessment of systems with multiple-dependent competing degradation processes (2014) IEEE Trans Fuzzy Syst, 23, pp. 1428-1438; Andrzejczak, K., Stochastic modelling of the repairable system (2015) J KONBiN, 35, pp. 5-14; Hsieh, M.-H., Jeng, S.-L., Accelerated discrete degradation models for leakage current of ultra-thin gate oxides (2007) IEEE Trans Reliab, 56, pp. 369-380; Lawless, J., Crowder, M., Covariates and random effects in a gamma process model with application to degradation and failure (2004) Lifetime Data Anal, 10, pp. 213-227; Cheng, T., Pandey, M.D., van der Weide, J.A., The probability distribution of maintenance cost of a system affected by the gamma process of degradation: finite time solution (2012) Reliab Eng Syst Saf, 108, pp. 65-76; Li, W., Pham, H., An inspection-maintenance model for systems with multiple competing processes (2005) IEEE Trans Reliab, 54, pp. 318-327; Puterman, M.L., Markov decision processes.: discrete stochastic dynamic programming (2014), John Wiley & Sons; Sutton, R.S., Barto, A.G., Reinforcement learning: an introduction (2018), MIT Press; Bhagat, S., Banerjee, H., Ho Tse, Z.T., Ren, H., Deep reinforcement learning for soft, flexible robots: brief review with impending challenges (2019) Robotics, 8, p. 4; Tseng, H.H., Luo, Y., Cui, S., Chien, J.T., Ten Haken, R.K., El Naqa, I., Deep reinforcement learning for automated radiation adaptation in lung cancer (2017) Med Phys, 44, pp. 6690-6705; Atallah, R.F., Assi, C.M., Khabbaz, M.J., Scheduling the operation of a connected vehicular network using deep reinforcement learning (2018) IEEE Trans Intell Transp Syst, 20, pp. 1669-1682; Cuayáhuitl, H., Simpleds: a simple deep reinforcement learning dialogue system (2017) Dialogues with social robots, pp. 109-118. , Springer; Wu, J., He, H., Peng, J., Li, Y., Li, Z., Continuous reinforcement learning of energy management with deep Q network for a power split hybrid electric bus (2018) Appl Energy, 222, pp. 799-811; Shakeel, P.M., Baskar, S., Dhulipala, V.S., Mishra, S., Jaber, M.M., Maintaining security and privacy in health care system using learning based deep-Q-networks (2018) J Med Syst, 42, p. 186; Watkins, C.J., Dayan, P., Q-learning (1992) Mach Learn, 8, pp. 279-292},
correspondence_address1={Zhang, N.; Massachusetts Mutual Life Insurance CompanyUnited States; email: eo9364@wayne.edu},
publisher={Elsevier Ltd},
issn={09518320},
coden={RESSE},
language={English},
abbrev_source_title={Reliab Eng Syst Saf},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yao20201230,
author={Yao, L. and Dong, Q. and Jiang, J. and Ni, F.},
title={Deep reinforcement learning for long-term pavement maintenance planning},
journal={Computer-Aided Civil and Infrastructure Engineering},
year={2020},
volume={35},
number={11},
pages={1230-1245},
doi={10.1111/mice.12558},
note={cited By 32},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084995854&doi=10.1111%2fmice.12558&partnerID=40&md5=1ce36615b5d8e2647eec45d042e899d8},
affiliation={Department of Highway and Railway Engineering, College of Transportation, Southeast University, Nanjing, Jiangsu, China},
abstract={Inappropriate maintenance and rehabilitation strategies cause many problems such as maintenance budget waste, ineffective pavement distress treatments, and so forth. A method based on a machine learning algorithm called deep reinforcement learning (DRL) was developed in this presented research in order to learn better maintenance strategies that maximize the long-term cost-effectiveness in maintenance decision-making through trial and error. In this method, each single-lane pavement segment can have different treatments, and the long-term maintenance cost-effectiveness of the entire section is treated as the optimization goal. In the DRL algorithm, states are embodied by 42 parameters involving the pavement structures and materials, traffic loads, maintenance records, pavement conditions, and so forth. Specific treatments as well as do-nothing are the actions. The reward is defined as the increased or decreased cost-effectiveness after taking corresponding actions. Two expressways, the Ningchang and Zhenli expressways, were selected for a case study. The results show that the DRL model is capable of learning a better strategy to improve the long-term maintenance cost-effectiveness. By implementing the optimized maintenance strategies produced by the developed model, the pavement conditions can be controlled in an acceptable range. © 2020 Computer-Aided Civil and Infrastructure Engineering},
keywords={Budget control;  Cost effectiveness;  Costs;  Decision making;  Learning algorithms;  Learning systems;  Maintenance;  Pavements;  Reinforcement learning, Different treatments;  Long-term maintenances;  Maintenance and rehabilitation strategies;  Maintenance budgets;  Maintenance decision making;  Maintenance strategies;  Pavement maintenance;  Pavement structures, Deep learning, algorithm;  machine learning;  maintenance;  motorway;  pavement;  reinforcement},
references={Adeli, H., Neural networks in civil engineering: 1989–2000 (2001) Computer-Aided Civil and Infrastructure Engineering, 16 (2), pp. 126-142; Adeli, H., Hung, S.L., (1994) Machine learning: Neural networks, genetic algorithms, and fuzzy systems, , Hoboken, NJ, John Wiley & Sons; Aslani, M., Seipel, S., Wiering, M., Continuous residual reinforcement learning for traffic signal control optimization (2018) Canadian Journal of Civil Engineering, 45 (8), pp. 690-702; Augeri, M.G., Greco, S., Nicolosi, V., Planning urban pavement maintenance by a new interactive multiobjective optimization approach (2019) European Transport Research Review, 11 (1), p. 17; Azevedo, C.R., Von Zuben, F.J., Learning to anticipate flexible choices in multiple criteria decision-making under uncertainty (2015) IEEE Transactions on Cybernetics, 46 (3), pp. 778-791; Bellman, R., Dynamic programming (1966) Science, 153 (3731), pp. 34-37; Boyles, S.D., Zhang, Z., Waller, S.T., Optimal maintenance and repair policies under nonlinear preferences (2010) Journal of Infrastructure Systems, 16 (1), pp. 11-20; Cha, Y.J., Choi, W., Büyüköztürk, O., Deep learning-based crack damage detection using convolutional neural networks (2017) Computer-Aided Civil and Infrastructure Engineering, 32 (5), pp. 361-378; Chen, K., (2015) Deep Reinforcement Learning for Flappy Bird, , https://www.cs229.stanford.edu/proj2015/362_report.pdf, Retrieved from; Choi, J.H., Strategy for reducing carbon dioxide emissions from maintenance and rehabilitation of highway pavement (2019) Journal of Cleaner Production, 209, pp. 88-100; Dong, Q., Huang, B., Evaluation of effectiveness and cost-effectiveness of asphalt pavement rehabilitations utilizing LTPP data (2011) Journal of Transportation Engineering, 138 (6), pp. 681-689; Dong, Q., Huang, B., Richards, S.H., Yan, X., Cost-effectiveness analyses of maintenance treatments for low-and moderate-traffic asphalt pavements in Tennessee (2013) Journal of Transportation Engineering, 139 (8), pp. 797-803; Ernst, D., Geurts, P., Wehenkel, L., Tree-based batch mode reinforcement learning (2005) Journal of Machine Learning Research, 6, pp. 503-556; France-Mensah, J., O'Brien, W.J., Budget allocation models for pavement maintenance and rehabilitation: Comparative case study (2018) Journal of Management in Engineering, 34 (2); Friesz, T.L., Fernandez, J.E., A model of optimal transport maintenance with demand responsiveness (1979) Transportation Research Part B: Methodological, 13 (4), pp. 317-339; Fwa, T.F., Chan, W.T., Hoque, K.Z., Multiobjective optimization for pavement maintenance programming (2000) Journal of Transportation Engineering, 126 (5), pp. 367-374; Galehouse, L., Moulthrop, J.S., Hicks, R.G., (2003) Principles of pavement preservation: Definitions, benefits, issues, and barriers, , Washington, DC, Transportation Research Board; Gao, L., Aguiar-Moya, J., Zhang, Z., Performance modeling of infrastructure condition data with maintenance intervention (2011) Transportation Research Record, 1 (2225), pp. 109-116; Gao, L., Zhang, Z., Management of pavement maintenance, rehabilitation, and reconstruction through network partition (2013) Transportation Research Record, 2366 (1), pp. 59-63; Gao, Y., Mosalam, K.M., Deep transfer learning for image-based structural damage recognition (2018) Computer-Aided Civil and Infrastructure Engineering, 33 (9), pp. 748-768; Irfan, M., Khurshid, M.B., Bai, Q., Labi, S., Morin, T.L., Establishing optimal project-level strategies for pavement maintenance and rehabilitation—A framework and case study (2012) Engineering Optimization, 44 (5), pp. 565-589; Jiang, Z., Gu, J., Fan, W., Liu, W., Zhu, B., Q-learning approach to coordinated optimization of passenger inflow control with train skip-stopping on a urban rail transit line (2019) Computers & Industrial Engineering, 127, pp. 1131-1142; Kaelbling, L.P., Littman, M.L., Moore, A.W., Reinforcement learning: A survey (1996) Journal of Artificial Intelligence Research, 4, pp. 237-285; Labi, S., Lamptey, G., Kong, S.H., Effectiveness of microsurfacing treatments (2007) Journal of Transportation Engineering, 133 (5), pp. 298-307; Lample, G., Chaplot, D.S., (2017) Playing FPS games with deep reinforcement learning, , Proceeding of the thirty-first AAAI conference on artificial intelligence, San Francisco, CL; Lamptey, G., Labi, S., Li, Z., Decision support for optimal scheduling of highway pavement preventive maintenance within resurfacing cycle (2008) Decision Support Systems, 46 (1), pp. 376-387; Li, F., Chen, Y., Wang, J., Zhou, X., Tang, B., A reinforcement learning unit matching recurrent neural network for the state trend prediction of rolling bearings (2019) Measurement, 145, pp. 191-203; Li, H., (2015) The implementation of reinforcement learning algorithms on the elevator control system, pp. 1-4. , September)., 2015 IEEE 20th conference on emerging technologies & factory automation (). Luxembourg IEEE; Li, Y., Madanat, S., A steady-state solution for the optimal pavement resurfacing problem (2002) Transportation Research Part A: Policy and Practice, 36 (6), pp. 525-535; Li, Z., Liu, P., Xu, C., Duan, H., Wang, W., Reinforcement learning-based variable speed limit control strategy to reduce traffic congestion at freeway recurrent bottlenecks (2017) IEEE Transactions on Intelligent Transportation Systems, 18 (11), pp. 3204-3217; Liang, X., Du, X., Wang, G., Han, Z., A deep reinforcement learning network for traffic light cycle control (2019) IEEE Transactions on Vehicular Technology, 68 (2), pp. 1243-1253; Lin, K., Zhao, R., Xu, Z., Zhou, J., (2018) Efficient large-scale fleet management via multi-agent deep reinforcement learning, pp. 1774-1783. , Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, London (). Association for Computing Machinery; Liu, Z., Yao, C., Yu, H., Wu, T., Deep reinforcement learning with its application for lung cancer detection in medical Internet of Things (2019) Future Generation Computer Systems, 97, pp. 1-9; Madanat, S., Ben-Akiva, M., Optimal inspection and repair policies for infrastructure facilities (1994) Transportation Science, 28 (1), pp. 55-62; Mannion, P., Duggan, J., Howley, E., An experimental review of reinforcement learning algorithms for adaptive traffic signal control (2016) Autonomic road transport support systems, pp. 47-66. , T. L. McCluskey, A. Kotsialos, J. P. Müller, F. Klügl, O. Rana, R. Schumann, (Eds.),, Cham, Birkhäuser; Medury, A., Madanat, S., Simultaneous network optimization approach for pavement management systems (2013) Journal of Infrastructure Systems, 20 (3); Memarzadeh, M., Pozzi, M., Integrated inspection scheduling and maintenance planning for infrastructure systems (2016) Computer-Aided Civil and Infrastructure Engineering, 31 (6), pp. 403-415; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., (2013) Playing Atari with deep reinforcement learning, , https://arXiv.org/abs/1312.5602, Retrieved from; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Petersen, S., Human-level control through deep reinforcement learning (2015) Nature, 518 (7540), p. 529; Mocanu, E., Mocanu, D.C., Nguyen, P.H., Liotta, A., Webber, M.E., Gibescu, M., Slootweg, J.G., On-line building energy optimization using deep reinforcement learning (2018) IEEE Transactions on Smart Grid, 10 (4), pp. 3698-3708; Ng, M., Lin, D.Y., Waller, S.T., Optimal long-term infrastructure maintenance planning accounting for traffic dynamics (2009) Computer-Aided Civil and Infrastructure Engineering, 24 (7), pp. 459-469; Nguyen, T., Kashani, A., Ngo, T., Bordas, S., Deep neural network with high-order neuron for the prediction of foamed concrete strength (2019) Computer-Aided Civil and Infrastructure Engineering, 34 (4), pp. 316-332; Ouyang, Y., Madanat, S., Optimal scheduling of rehabilitation activities for multiple pavement facilities: Exact and approximate solutions (2004) Transportation Research Part A: Policy and Practice, 38 (5), pp. 347-365; Rafiei, M., Adeli, H., A novel machine learning model for estimation of sale prices of real estate units (2016) Journal of Construction Engineering and Management, 142 (2). , https://doi.org/10.1061/(ASCE)CO.1943-7862.0001047; Rafiei, M., Adeli, H., A new neural dynamic classification algorithm (2017) IEEE Transactions on Neural Networks and Learning Systems, 28 (12), pp. 3074-3083; Rafiei, M.H., Adeli, H., A novel machine learning-based algorithm to detect damage in high-rise building structures (2017) The Structural Design of Tall and Special Buildings, 26 (18); Rafiei, M., Adeli, H., NEEWS: A novel earthquake early warning model using neural dynamic classification and neural dynamic optimization (2017) Soil Dynamics and Earthquake Engineering, 100, pp. 417-427; Rafiei, M.H., Adeli, H., Novel machine-learning model for estimating construction costs considering economic variables and indexes (2018) Journal of Construction Engineering and Management, 144 (12); Rafiei, M., Khushefati, W., Demirboga, R., Adeli, H., Supervised deep restricted Boltzmann machine for estimation of concrete (2017) ACI Materials Journal, 114 (2), pp. 237-244; Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Dieleman, S., Mastering the game of go with deep neural networks and tree search (2016) Nature, 529 (7587), p. 484; Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Chen, Y., Mastering the game of go without human knowledge (2017) Nature, 550 (7676), p. 354; Su, P.H., Vandyke, D., Gasic, M., Kim, D., Mrksic, N., Wen, T.H., Young, S., (2015) Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems, , http://dblp.uni-trier.de/db/conf/interspeech/interspeech2015.html#SuVGKMWY15, Retrieved from; Sun, Y., Tan, W., A trust-aware task allocation method using deep q-learning for uncertain mobile crowdsourcing (2019) Human-Centric Computing and Information Sciences, 9 (1), p. 25; Sutton, R.S., Barto, A.G., (1998) Reinforcement learning: An introduction, , 1st ed., Cambridge, MA, MIT Press; Taggart, A., Tachtsi, L., Lugg, M., Davies, H., UKRLG framework for highway infrastructure asset management (2014) Infrastructure Asset Management, 1 (1), pp. 10-19; Tsunokawa, K., Schofer, J.L., Trend curve optimal control model for highway pavement maintenance: Case study and evaluation (1994) Transportation Research Part A: Policy and Practice, 28 (2), pp. 151-166; Walls, J., (1998) Life-cycle cost analysis in pavement design: In search of better investment decisions, , Washington, DC, US Department of Transportation, Federal Highway Administration; Walraven, E., Spaan, M.T., Bakker, B., Traffic flow optimization: A reinforcement learning approach (2016) Engineering Applications of Artificial Intelligence, 52, pp. 203-212; Wang, G., Morian, D., Frith, D., Cost-benefit analysis of thin surface treatments in pavement treatment strategies and cycle maintenance (2012) Journal of Materials in Civil Engineering, 25 (8), pp. 1050-1058; Wang, I., Tsai, Y., Li, F., A network flow model for clustering segments and minimizing total maintenance and rehabilitation cost (2011) Computers & Industrial Engineering, 60 (4), pp. 593-601; Wang, J., Uchibe, E., Doya, K., EM-based policy hyper parameter exploration: Application to standing and balancing of a two-wheeled smartphone robot (2016) Artificial Life and Robotics, 21 (1), pp. 125-131; Yang, Y., Huang, L., Wang, J., Xia, Y., Research on reference indicators for sustainable pavement maintenance cost control through data mining (2019) Sustainability, 11 (3), p. 877; Yao, L., Dong, Q., Jiang, J., Ni, F., Establishment of prediction models of asphalt pavement performance based on a novel data calibration method and neural network (2019) Transportation Research Record, 2673 (1), pp. 66-82; Yeo, H., Yoon, Y., Madanat, S., Maintenance optimization for heterogeneous infrastructure systems: Evolutionary algorithms for bottom-up methods (2010) Sustainable and resilient critical infrastructure systems, pp. 185-199. , K. Gopalakrishnan, (Ed.),, Berlin, Springer; Zhang, A., Wang, K.C., Fei, Y., Liu, Y., Chen, C., Yang, G., Qiu, S., Automated pixel-level pavement crack detection on 3D asphalt surfaces with a recurrent neural network (2019) Computer-Aided Civil and Infrastructure Engineering, 34 (3), pp. 213-229; Zhang, B., Mao, Z., Liu, W., Liu, J., Geometric reinforcement learning for path planning of UAVs (2015) Journal of Intelligent & Robotic Systems, 77 (2), pp. 391-409; Zhang, H., Keoleian, G.A., Lepech, M.D., Kendall, A., Life-cycle optimization of pavement overlay systems (2010) Journal of Infrastructure Systems, 16 (4), pp. 310-322; Zhang, Z., Reinforcement learning in clinical medicine: A method to optimize dynamic treatment regime over time (2019) Annals of Translational Medicine, 7 (14), p. 345; Zhou, L., Ni, F.J., Leng, Z., Development of an asphalt pavement distress evaluation method for freeways in China (2014) International Journal of Pavement Research and Technology, 7 (2), pp. 159-167; Zolfpour-Arokhlo, M., Selamat, A., Hashim, S.Z.M., Afkhami, H., Modeling of route planning system based on Q value-based dynamic programming with multi-agent reinforcement learning algorithms (2014) Engineering Applications of Artificial Intelligence, 29, pp. 163-177},
correspondence_address1={Dong, Q.; Department of Highway and Railway Engineering, China; email: qiaodong@seu.edu.cn},
publisher={Blackwell Publishing Inc.},
issn={10939687},
coden={CCIEF},
language={English},
abbrev_source_title={Comput.-Aided Civ. Infrastruct. Eng.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gannouni20202640,
author={Gannouni, A. and Samsonov, V. and Behery, M. and Meisen, T. and Lakemeyer, G.},
title={Neural Combinatorial Optimization for Production Scheduling with Sequence-Dependent Setup Waste},
journal={Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
year={2020},
volume={2020-October},
pages={2640-2647},
doi={10.1109/SMC42975.2020.9282869},
art_number={9282869},
note={cited By 2; Conference of 2020 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2020 ; Conference Date: 11 October 2020 Through 14 October 2020;  Conference Code:165855},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098867987&doi=10.1109%2fSMC42975.2020.9282869&partnerID=40&md5=f6b68c2ab240f893196f023bb1e92d8c},
affiliation={Rwth Aachen University, Institute of Information Management in Mechanical Engineering, Aachen, Germany; Rwth Aachen University, Knowledge-Based Systems Group, Aachen, Germany; University of Wuppertal, Chair of Technologies and Management of Digital Transformation, Wuppertal, Germany},
abstract={One of the main objectives of production planning is to minimize the usage of resources and manufacturing-related costs while meeting the customer's requirements, such as delivery dates and quality. Production planners deal with various scheduling problems that are often NP-hard and can not be optimally solved by humans. Solving such problems often relies on methods from the Operations Research (OR) field. Recently, Neural Combinatorial Optimization (NCO) has emerged as a promising field of research that aims at tackling different optimization tasks using the latest advancements in machine learning, including deep reinforcement learning. These methods can be successfully used for short-term production planning because of their flexibility and speed. In this paper, we examine the applicability and scalability of neural combinatorial optimization methods in the context of production planning. We define an evaluation metric to investigate the stability and quality of the solutions. Furthermore, we develop an experimental setup allowing to compare various approaches for production scheduling with sequence-dependent setup costs under real-world production conditions. Although an optimality gap is observed when compared to established OR methods, our experiments demonstrate the superiority of NCO in terms of scheduling time. © 2020 IEEE.},
author_keywords={Deep Reinforcement Learning;  Neural Combinatorial Optimization;  Production Scheduling;  Supervised Machine Learning},
keywords={Combinatorial optimization;  Deep learning;  Manufacture;  NP-hard;  Operations research;  Planning;  Production control;  Reinforcement learning;  Scheduling, Customer's requirements;  Evaluation metrics;  Optimization task;  Production Planning;  Production planning IS;  Production Scheduling;  Scheduling problem;  Sequence dependent setups, Optimization},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG, 390621612},
funding_text 1={The authors would like to thank the German Research Foundation (DFG) for the kind support within the Cluster of Excellence – EXC-2023 Internet of Production – 390621612.},
references={Bello, I., Pham, H., Le, Q.V., Norouzi, M., Bengio, S., (2016) Neural Combinatorial Optimization with Reinforcement Learning, , arXiv preprint arXiv:1611.09940; Lawler, E.L., Lenstra, J.K., Rinnooy Kan, A.H., Shmoys, D.B., Chapter 9 sequencing and scheduling: Algorithms and complexity (1993) Logistics of Production and Inventory, Ser. Handbooks in Operations Research and Management Science., 4, pp. 445-522. , Elsevier; Graves, S.C., A review of production scheduling (1981) Operations Research, 29 (4), pp. 646-675; Allahverdi, A., Soroush, H.M., The significance of reducing setup times/setup costs (2008) European Journal of Operational Research, 187 (3), pp. 978-984; Allahverdi, A., Gupta, J.N., Aldowaisan, T., A review of scheduling research involving setup considerations (1999) Omega, 27 (2), pp. 219-239; Chen, X., Wan, W., Xu, X., Modeling rolling batch planning as vehicle routing problem with time windows (1998) Computers & Operations Research, 25 (12), pp. 1127-1136; Papadimitriou, C.H., The euclidean travelling salesman problem is npcomplete (1977) Theoretical Computer Science, 4 (3), pp. 237-244; Baker, K.R., Trietsch, D., (2009) Principles of Sequencing and Scheduling., , Hoboken, NJ, USA: John Wiley & Sons, Inc; Bektas, T., The multiple traveling salesman problem: An overview of formulations and solution procedures (2006) Omega, 34 (3), pp. 209-219; François-Lavet, V., Henderson, P., Islam, R., Bellemare, M.G., Pineau, J., An introduction to deep reinforcement learning (2018) Foundations and Trendsr in Machine Learning, 11 (3-4), pp. 219-354; Williams, R.J., Simple statistical gradient-following algorithms for connectionist reinforcement learning (1992) Machine Learning, 8 (3-4), pp. 229-256; Tang, L., Liu, J., Rong, A., Yang, Z., A multiple traveling salesman problem model for hot rolling scheduling in Shanghai baoshan iron & steel complex (2000) European Journal of Operational Research, 124 (2), pp. 267-282; Bengio, Y., Lodi, A., Prouvost, A., (2018) Machine Learning for Combinatorial Optimization: A Methodological Tour d'Horizon, , arXiv preprint arXiv:1811.06128; Smith, K.A., Neural networks for combinatorial optimization: A review of more than a decade of research (1999) Informs Journal on Computing, 11 (1), pp. 15-34; Vinyals, O., Fortunato, M., Jaitly, N., Pointer networks (2015) Advances in Neural Information Processing Systems, pp. 2692-2700; Kaempfer, Y., Wolf, L., (2018) Learning the Multiple Traveling Salesmen Problem with Permutation Invariant Pooling Networks; Kool, W., Van Hoof, H., Welling, M., (2018) Attention, Learn to Solve Routing Problems!; Nazari, M., Oroojlooy, A., Snyder, L.V., Takáč, M., Reinforcement learning for solving the vehicle routing problem (2018) Advances in Neural Information Processing Systems, pp. 9839-9849; Chen, H.-K., Hsueh, C.-F., Chang, M.-S., Production scheduling and vehicle routing with time windows for perishable food products (2009) Computers & Operations Research, 36 (7), pp. 2311-2319; Fu, L.-L., Aloulou, M.A., Triki, C., Integrated production scheduling and vehicle routing problem with job splitting and delivery time windows (2017) International Journal of Production Research, 55 (20), pp. 5942-5957; Samsonov, V., Lipp, J., Noodt, P., Solvay, A.F., Meisen, T., More machine learning for less: Comparing data generation strategies in mechanical engineering and manufacturing (2019) 2019 Ieee Symposium Series on Computational Intelligence (SSCI)., pp. 799-807. , IEEE, 12/6/2019 -12/9; Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., Liu, T.-Y., Lightgbm: A highly efficient gradient boosting decision tree (2017) Advances in Neural Information Processing Systems, pp. 3146-3154; Ribeiro, M.T., Singh, S., Guestrin, C., Why should i trust you?": Explaining the predictions of any classifier (2016) Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining, pp. 1135-1144; Lundberg, S.M., Erion, G., Chen, H., DeGrave, A., Prutkin, J.M., Nair, B., Katz, R., Lee, S.-I., From local explanations to global understanding with explainable ai for trees (2020) Nature Machine Intelligence, 2 (1), pp. 56-67; Lundberg, S., Lee, S.-I., (2017) A Unified Approach to Interpreting Model Predictions; Shapley, L.S., A value for n -person games (1988) The Shapley Value, pp. 31-40. , A. E. Roth and L. S. Shapley, Eds. Cambridge University Press; (2020) Gurobi Optimizer Reference Manual, , Gurobi Optimization L.L.C; Perron, L., Furnon, V., Or-tools, , https://developers.google.com/optimization/, Google. [Online]},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1062922X},
isbn={9781728185262},
coden={PICYE},
language={English},
abbrev_source_title={Conf. Proc. IEEE Int. Conf. Syst. Man Cybern.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kumar20201795,
author={Kumar, A. and Dimitrakopoulos, R. and Maulen, M.},
title={Adaptive self-learning mechanisms for updating short-term production decisions in an industrial mining complex},
journal={Journal of Intelligent Manufacturing},
year={2020},
volume={31},
number={7},
pages={1795-1811},
doi={10.1007/s10845-020-01562-5},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084195321&doi=10.1007%2fs10845-020-01562-5&partnerID=40&md5=79baa64640feed56e24dc3a83a9bb174},
affiliation={COSMO – Stochastic Mine Planning Laboratory, Department of Mining and Materials Engineering, McGill University, FDA Building, 3450 University Street, Montreal, QC  H3A 0E8, Canada; Practice Lead Mining Technical, BHP, Santiago, Chile},
abstract={A mining complex is an integrated value chain where the materials extracted from a group of mineral deposits are sent to different processing streams to produce sellable products. A major short-term decision in a mining complex is to determine the flow of materials that first includes deciding which handling facilities to send the extracted materials and then determining how to utilize the processing facilities. The flow of materials through the mining complex is significantly dependent on the performance of and interaction between its different components. New digital technologies, including the development of advanced sensors and monitoring devices, have enabled a mining complex to acquire new information about the performance of its different components. This paper proposes a new continuous updating framework that combines policy gradient reinforcement learning and an extended ensemble Kalman filter to adapt the short-term flow of materials in a mining complex with incoming information. The framework first uses a new extended ensemble Kalman filter to update the uncertainty models of the different components of a mining complex with new incoming information. Then, the updated uncertainty models are fed to a neural network trained using a policy gradient reinforcement learning algorithm to adapt the short-term flow of materials in a mining complex. The proposed framework is applied to a copper mining complex and shows its ability to efficiently adapt the short-term flow of materials in an operational mining environment with new incoming information. The framework better meets the different production targets while improving the cumulative cash flow compared to industry standard approaches. © 2020, The Author(s).},
author_keywords={Artificial intelligence;  Deep learning;  Destination policies;  Ensemble Kalman filter;  Mining complex;  Production planning;  Real-time;  Reinforcement learning;  Sensor information},
keywords={Complex networks;  Copper compounds;  Digital devices;  Extraction;  Filtration;  Kalman filters;  Learning algorithms;  Materials handling;  Mineral resources;  Reinforcement learning, Cumulative cash flow;  Digital technologies;  Ensemble Kalman Filter;  Policy gradient reinforcement learning;  Policy-gradient reinforcement learning algorithm;  Processing facilities;  Production decisions;  Self-learning mechanism, Data mining},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC, 239019, 500414-16},
funding_details={Canada Research ChairsCanada Research Chairs},
funding_details={BHP BillitonBHP Billiton},
funding_details={BHPBHP},
funding_details={IAMGOLDIAMGOLD},
funding_text 1={The work in this paper was funded by the National Sciences and Engineering Research Council (NSERC) of Canada CRD Grant 500414-16 and NSERC Discovery Grant 239019, the industry consortium members of McGill University’s COSMO Stochastic Mine Planning Laboratory (AngloGold Ashanti, Barrick Gold, BHP, De Beers, IAMGOLD, Kinross Gold, Newmont Corporation, and Vale); and the Canada Research Chairs Program.},
funding_text 2={The work in this paper was funded by the National Sciences and Engineering Research Council (NSERC) of Canada CRD Grant 500414-16 and NSERC Discovery Grant 239019, the industry consortium members of McGill University?s COSMO Stochastic Mine Planning Laboratory (AngloGold Ashanti, Barrick Gold, BHP, De Beers, IAMGOLD, Kinross Gold, Newmont Corporation, and Vale); and the Canada Research Chairs Program.},
references={Aissani, N., Bekrar, A., Trentesaux, D., Beldjilali, B., Dynamic scheduling for multi-site companies: A decisional approach based on reinforcement multi-agent learning (2012) Journal of Intelligent Manufacturing, 23 (6), pp. 2513-2529; Asad, M.W.A., Qureshi, M.A., Jang, H., A review of cut-off grade policy models for open pit mining operations (2016) Resources Policy, 49, pp. 142-152; Barde, S.R.A., Yacout, S., Shin, H., Optimal preventive maintenance policy based on reinforcement learning of a fleet of military trucks (2019) Journal of Intelligent Manufacturing, 30 (1), pp. 147-161; Benndorf, J., Making use of online production data: Sequential updating of mineral resource models (2015) Mathematical Geosciences, 47 (5), pp. 547-563; Benndorf, J., Buxton, M.W.N., Sensor-based real-time resource model reconciliation for improved mine production control-a conceptual framework (2016) Mining Technology, 125 (1), pp. 54-64; Blom, M., Pearce, A.R., Stuckey, P.J., Short-term planning for open pit mines: A review (2019) International Journal of Mining, Reclamation and Environment, 33 (5), pp. 318-339; Bottou, L., Large-scale machine learning with stochastic gradient descent (2010) Proceedings of the COMPSTAT’2010, pp. 177-186. , https://doi.org/10.1007/978-3-7908-2604-3_16; Brewer, A., Nancy, S., Thomas, L., Intelligent tracking in manufacturing (1999) Journal of Intelligent Manufacturing, 10 (3), pp. 245-250; Chen, Y., Oliver, D.S., Ensemble randomized maximum likelihood method as an iterative ensemble smoother (2012) Mathematical Geosciences, 44 (1), pp. 1-26; Dalm, M., Buxton, M.W.N., van Ruitenbeek, F.J.A., Ore–waste discrimination in epithermal deposits using near-infrared to short-wavelength infrared (NIR-SWIR) hyperspectral imagery (2018) Mathematical Geosciences, 51 (7), pp. 1-27; Dalm, M., Buxton, M.W.N., Van Ruitenbeek, F.J.A., Voncken, J.H.L., Application of near-infrared spectroscopy to sensor based sorting of a porphyry copper ore (2014) Minerals Engineering, 58, pp. 7-16; Desbarats, A.J., Dimitrakopoulos, R., Geostatistical simulation of regionalized pore-size distributions using min/max autocorrelation factors (2000) Mathematical Geology, 32 (8), pp. 919-942; Dimitrakopoulos, R., Farrelly, C.T., Godoy, M., Moving forward from traditional optimisation: Grade uncertainty and risk effects in open pit design (2002) Mining Technology, 111 (1), pp. 82-88; Dimitrakopoulos, R., Godoy, M., Grade control based on economic ore/waste classification functions and stochastic simulations: Examples, comparisons and applications (2014) Mining Technology, 123 (2), pp. 90-106; Dimitrakopoulos, R., Luo, X., Generalized sequential Gaussian simulation on group size v and screen-effect approximations for large field simulations (2004) Mathematical Geology, 36 (5), pp. 567-590; Dovera, L., Della Rossa, E., Multimodal ensemble Kalman filtering using Gaussian mixture models (2011) Computational Geosciences, 15 (2), pp. 307-323; Evensen, G., Carlo, M., Carlo, M., Sequential data assimilation with a nonlinear quasi-geostrophic model using Monte Carlo methods to forecast error statistics (1994) Journal of Geophysical Research: Oceans, 99 (C5), pp. 10143-10162; Glorot, X., Bengio, Y., (2010) Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the 13th international conference on artificial intelligence and statistics (pp. 249–256), , http://proceedings.mlr.press/v9/glorot10a.html; Goetz, A.F.H., Curtiss, B., Shiley, D.A., Rapid gangue mineral concentration measurement over conveyors by NIR reflectance spectroscopy (2009) Minerals Engineering, 22 (5), pp. 490-499; Goodfellow, R., Dimitrakopoulos, R., Global optimization of open pit mining complexes with uncertainty (2016) Applied Soft Computing, 40, pp. 292-304; Goodfellow, R., Dimitrakopoulos, R., Simultaneous stochastic optimization of mining complexes and mineral value chains (2017) Mathematical Geosciences, 49 (3), pp. 341-360; Hinton, G.E., Srivastava, N., Swersky, K., (2012) Neural Netwrok for Machine Learning-Lecture 6A: Overview of Mini-Batch Gradient Descent, , http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf, Retrieved January 1, 2; Hou, J., Zhou, K., Zhang, X.S., Kang, X.D., Xie, H., A review of closed-loop reservoir management (2015) Petroleum Science, 12 (1), pp. 114-128; Iyakwari, S., Glass, H.J., Rollinson, G.K., Kowalczuk, P.B., Application of near infrared sensors to preconcentration of hydrothermally-formed copper ore (2016) Minerals Engineering, 85, pp. 148-167; Jewbali, A., Dimitrakopoulos, R., Implementation of conditional simulation by successive residuals (2011) Computers & Geosciences, 37 (2), pp. 129-142; Kargupta, H., Srakar, K., Gilligan, M., MineFleet® : An overview of a widely adopted distributed vehicle performance data mining system (2010) In Proceedings of the 16Th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 37-46. , https://doi.org/10.1145/1835804.1835812; Koellner, W.G., Brown, G.M., Rodríguez, J., Pontt, J., Cortés, P., Miranda, H., Recent advances in mining haul trucks (2004) IEEE Transactions on Industrial Electronics, 51 (2), pp. 321-329; Kumar, D., Srinivasan, S., Ensemble-based assimilation of nonlinearly related dynamic data in reservoir models exhibiting non-Gaussian characteristics (2019) Mathematical Geosciences, 51 (1), pp. 75-107; Lamghari, A., Mine planning and oil field development: A survey and research potentials (2017) Mathematical Geosciences, 49 (3), pp. 395-437; Lane, K.F., Cutoff grades for two minerals (1984) Proceedings of the 18Th International Symposium on Application of Computers and Operations Research in Mineral the Industries, pp. 485-492. , In, pp; Lane, K.F., (1988) The economic definition of ore: Cut-off grades in theory and practice, , Mining Journal Books Limited, London; Mai, N.L., Topal, E., Erten, O., Sommerville, B., A new risk-based optimisation method for the iron ore production scheduling using stochastic integer programming (2019) Resources Policy, 62, pp. 571-579; Matamoros, M.E.V., Dimitrakopoulos, R., Stochastic short-term mine production schedule accounting for fleet allocation, operational considerations and blending restrictions (2016) European Journal of Operational Research, 255 (3), pp. 911-921; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., (2013) Playing Atari with Deep Reinforcement Learning, , http://arxiv.org/abs/1312.5602, arXiv preprint, 1312.5602; Montiel, L., Dimitrakopoulos, R., Optimizing mining complexes with multiple processing and transportation alternatives: An uncertainty-based approach (2015) European Journal of Operational Research, 247 (1), pp. 166-178; Montiel, L., Dimitrakopoulos, R., A heuristic approach for the stochastic optimization of mine production schedules (2017) Journal of Heuristics, 23 (5), pp. 397-415; Montiel, L., Dimitrakopoulos, R., Simultaneous stochastic: Optimization of production scheduling at Twin Creeks mining complex, Nevada (2018) Mining Enginnering, 70 (12), pp. 48-56; Nair, V., Hinton, G.E., Rectified linear units improve restricted boltzmann machines (2010) Proceedings of the 27Th International Conference on Machine Learning, pp. 807-814. , In; Nguyen, D., Bui, X., A real-time regulation model in multi-agent decision support system for open pit mining (2015) In Proceedings of the 12Th International Symposium Continuous Surface Mining-Aachen, pp. 255-262. , https://doi.org/10.1007/978-3-319-12301-1; Paduraru, C., Dimitrakopoulos, R., Adaptive policies for short-term material flow optimization in a mining complex (2018) Mining Technology, 127 (1), pp. 56-63; Paduraru, C., Dimitrakopoulos, R., Responding to new information in a mining complex: Fast mechanisms using machine learning (2019) Mining Technology, 128 (3), pp. 129-142; Panzeri, M., Della Rossa, E.L., Dovera, L., Riva, M., Guadagnini, A., Integration of Markov mesh models and data assimilation techniques in complex reservoirs (2016) Computational Geosciences, 20 (3), pp. 637-653; Quigley, M., Dimitrakopoulos, R., Incorporating geological and equipment performance uncertainty while optimizing short-term mine production schedules (2019) International Journal of Mining, Reclamation and Environment; Rendu, J.-M., (2014) An introduction to cut-off grade estimation, , Society for Mining, Metallurgy & Exploration, Englewood, CO; Rosa, L., Davidvalery, W., Wortley, M., Ozkocak, T., Pike, M., The use of radio frequency ID tags to track ore in mining operations (2007) Proceedings of the 33Rd Application of Computers and Operations Research in the Mineral Industries, pp. 601-606; Rossi, M.E., Deutsch, C.V., (2013) Mineral resource estimation, , Springer, New York; Ruder, S., (2016) An Overview of Gradient Descent Optimization Algorithms. Arxiv Preprint Arxiv, 1609, p. 04747. , https://doi.org/10.1111/j.0006-341X.1999.00591.x; Sarma, P., Durlofsky, L.J., Aziz, K., Chen, W.H., Efficient real-time reservoir management using adjoint-based optimal control and model updating (2006) Computational Geosciences, 10 (1), pp. 3-36; Shirangi, M.G., (2017) Advanced Techniques for Closed-Loop Reservoir Optimization under Uncertainty (Doctoral Dissertation), , Stanford, Stanford University; Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Mastering the game of Go with deep neural networks and tree search (2016) Nature, 529 (7587), pp. 484-489; Sutton, R.S., McAllester, D., Singh, S., Mansour, Y., Policy gradient methods for reinforcement learning with function approximation (2000) Proceedings of the Advances in Neural Information Processing Systems, pp. 1057-1063. , http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf, . In, (pp,); Vargas-Guzmán, J.A., Dimitrakopoulos, R., Conditional simulation of random fields by successive residuals (2002) Mathematical Geology, 34 (5), pp. 597-611; Verly, G., Grade control classification of ore and waste: A critical review of estimation and simulation based procedures (2005) Mathematical Geology, 37 (5), pp. 451-475; Vo, H.X., Durlofsky, L.J., A new differentiable parameterization based on principal component analysis for the low-dimensional representation of complex geological models (2014) Mathematical Geosciences, 46 (7), pp. 775-813; Wambeke, T., Benndorf, J., A study of the influence of measurement volume, blending ratios and sensor precision on real-time reconciliation of grade control models (2018) Mathematical Geosciences, 50 (7), pp. 801-826; Xu, T., Hernández, J.G., Simultaneous identification of a contaminant source and hydraulic conductivity via the restart normal-score ensemble Kalman filter (2019) Advances in Water Resources, 112, pp. 106-123; Xue, L., Zhang, D., A multimodel data assimilation framework via the ensemble Kalman filter (2014) Water Resources Research, 50 (5), pp. 4197-4219; Yüksel, C., Minnecker, C., Shishvan, M.S., Benndorf, J., Buxton, M., Value of information introduced by a resource model updating framework (2018) Mathematical Geosciences, 51 (7), pp. 1-19},
correspondence_address1={Kumar, A.; COSMO – Stochastic Mine Planning Laboratory, 3450 University Street, Canada; email: ashish.kumar@mail.mcgill.ca},
publisher={Springer},
issn={09565515},
coden={JIMNE},
language={English},
abbrev_source_title={J Intell Manuf},
document_type={Article},
source={Scopus},
}

@ARTICLE{Guo20204071,
author={Guo, W. and Gu, X.},
title={Joint decision-making of production and maintenance in mixed model assembly systems with delayed differentiation configurations},
journal={International Journal of Production Research},
year={2020},
volume={58},
number={13},
pages={4071-4085},
doi={10.1080/00207543.2019.1641641},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081653794&doi=10.1080%2f00207543.2019.1641641&partnerID=40&md5=6277ca713569c96b3305ae295e16de93},
affiliation={Department of Industrial and Systems Engineering, Rutgers University, Piscataway, NJ, United States; Department of Mechanical and Aerospace Engineering, Rutgers University, Piscataway, NJ, United States},
abstract={Mixed model assembly systems (MMASs) can simultaneously manufacture multiple product variants and are developed to satisfy customers’ increasing desire for products with a high variety. This paper investigates the joint decision-making of production and maintenance policies in MMASs with delayed differentiation configurations, where common operations are performed before differentiated processes. The problem is formulated as a Markov Decision Process (MDP) problem that minimises the average cost per unit time. Monte Carlo simulation is used to evaluate the system performance measures (e.g. volume mix ratio, product quality) under the optimal policy. Numerical examples are presented to illustrate the structure of the optimal policy and the impact of different factors on the system performance in an MMAS that produces two types of product variants. Techniques that can potentially solve the problem in large-sized MMASs are also discussed. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={maintenance;  manufacturing systems;  Markov decision process;  mixed model assembly},
keywords={Behavioral research;  Customer satisfaction;  Decision making;  Intelligent systems;  Maintenance;  Manufacture;  Markov processes;  Monte Carlo methods;  Reinforcement learning;  Riveting;  Structural optimization, Average cost per unit time;  Delayed differentiations;  Joint decision making;  Maintenance policy;  Markov Decision Processes;  Mixed modeling;  Mixed-model assembly systems;  System performance measures, Assembly},
references={Abad, A., Guo, W., Jin, J., Algebraic Expression of System Configurations and Performance Metrics for Mixed Model Assembly Systems (2014) IIE Transactions, 46 (3), pp. 230-248; Ahmad, R., Kamaruddin, S., An Overview of Time-Based and Condition-Based Maintenance in Industrial Application (2012) Computers & Industrial Engineering, 63 (1), pp. 135-149; Askin, R.G., Zhou, M., A Parallel Station Heuristic for the Mixed-Model Production Line Balancing Problem (1997) International Journal of Production Research, 35 (11), pp. 3095-3106; Bertsekas, D.P., (2005) Dynamic Programming and Optimal Control, , Belmont, MA: Athena Scientific; Boysen, N., Fliedner, M., Scholl, A., Sequencing Mixed-Model Assembly Lines: Survey, Classification and Model Critique (2009) European Journal of Operational Research, 192, pp. 349-373; Fisher, M.L., Ittner, C.D., The Impact of Product Variety on Automobile Assembly Operations: Empirical Evidence and Simulation (1999) Management Science, 45 (6), pp. 771-786; Gamerman, D., Lopes, H.F., (2006) Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference, , Boca Raton, FL: Chapman and Hall/CRC; Gu, X., The Impact of Maintainability on the Manufacturing System Architecture (2017) International Journal of Production Research, 55 (15), pp. 4392-4410; Gu, X., Guo, W., (2017), Joint Production and Maintenance Decision-Making Mixed-Model Assembly Systems. ASME International Manufacturing Science and Engineering Conference, volume 3: Manufacturing Equipment and Systems, Los Angeles, CA, V003T04A013, MSEC2017-2940; Gu, X., Jin, X., Guo, W., Ni, J., Estimation of Active Maintenance Opportunity Windows in Bernoulli Production Lines (2017) Journal of Manufacturing Systems, 45, pp. 109-120; Gu, X., Jin, X., Ni, J., Prediction of Passive Maintenance Opportunity Windows on Bottleneck Machines in Complex Manfuacturing Systems (2015) ASME Journal of Manufacturing Science and Engineering, 137 (3), p. 031017; Guo, W., Jin, J., Hu, S.J., Allocation of Maintenance Resources in Mixed Model Assembly Systems (2013) Journal of Manufacturing Systems, 32 (3), pp. 473-479; Guo, W., Tian, Q., Jiang, Z., Wang, H., A Graph-Based Cost Model for Supply Chain Reconfiguration (2018) Journal of Manufacturing Systems, 48, pp. 55-63; Hu, S.J., Ko, J., Weyand, L., ElMaraghy, H.A., Lien, T.K., Koren, Y., Bley, H., Shpitalni, M., Assembly System Design and Operations for Product Variety (2011) CIRP Annals, 60 (2), pp. 715-733; Hwang, R., Katayama, H., Integrated Procedure of Balancing and Sequencing for Mixed-Model Assembly Lines: A Multi-Objective Evolutionary Approach (2010) International Journal of Production Research, 48 (21), pp. 6417-6441; Kazaz, B., Sloan, T.W., Production Policies Under Deteriorating Process Conditions (2008) IIE Transactions, 40 (3), pp. 187-205; Kazaz, B., Sloan, T.W., The Impact of Process Deterioration on Production and Maintenance Policies (2013) European Journal of Operational Research, 227, pp. 88-100; Ko, J., Hu, S.J., Balancing of Manufacturing Systems with Complex Configurations for Delayed Product Differentiation (2008) International Journal of Production Research, 46 (15), pp. 4285-4308; Koren, Y., (2010) The Global Manufacturing Revolution: Product-Process-Business Integration and Reconfigurable Systems, , Hoboken, NJ: Wiley; Koren, Y., Hu, S.J., Gu, P., Shpitalni, M., Open-Architecture Products (2013) CIRP Annals, 62 (2), pp. 719-729; Ni, J., Gu, X., Jin, X., Maintenance Opportunities in Large Production Systems (2015) CIRP Annals, 64 (1), pp. 447-450; Nourelfath, M., Fitouhi, M., Machani, M., An Integrated Model for Production and Preventive Maintenance Planning in Multi-State Systems (2010) IEEE Transactions on Reliability, 59 (3), pp. 496-506; Powell, W.B., (2011) Approximate Dynamic Programming: Solving the Curses of Dimensionality, , Hoboken, NJ: Wiley; Ross, S.M., (1996) Stochastic Processes, , New York, NY: Wiley; Sloan, T.W., Shanthikumar, J.G., Combined Production and Maintenance Scheduling for a Multiple-Product, Single-Machine Production System (2000) Production and Operations Management, 9, pp. 379-399; Thomopoulos, N.T., Line Balancing-Sequencing for Mixed-Model Assembly (1967) Management Science, 14 (2), pp. B-59-B-75; Wang, H., A Survey of Maintenance Policies of Deteriorating Systems (2002) European Journal of Operational Research, 139 (3), pp. 469-489; Xia, T., Xi, L., Du, S., Xiao, L., Pan, E., Energy-Oriented Maintenance Decision-Making for Sustainable Manufacturing Based on Energy Saving Window (2018) ASME Journal of Manufacturing Science and Engineering, 140 (5), p. 051001; Xiang, Y., Cassady, C.R., Jin, T., Zhang, C.W., Joint Production and Maintenance Planning with Machine Deterioration and Random Yield (2014) International Journal of Production Research, 52 (6), pp. 1644-1657; Yao, X., Fernandez-Gaucherand, E., Fu, M.C., Marcus, S.I., Optimal Preventive Maintenance Scheduling in Semiconductor Manufacturing (2005) IEEE Transactions on Semiconductor Manufacturing, 17 (3), pp. 345-356; Yao, X., Xie, X., Fu, M.C., Marcus, S.I., Optimal Joint Preventive Maintenance and Production Policies (2005) Naval Research Logistics, 52 (7), pp. 668-681; Zhu, X., Hu, S.J., Koren, Y., Marin, S.P., Modeling of Manufacturing Complexity in Mixed-Model Assembly Lines (2008) ASME Journal of Manufacturing Science and Engineering, 130, p. 051013},
correspondence_address1={Guo, W.; Department of Industrial and Systems Engineering, United States; email: wg152@rutgers.edu},
publisher={Taylor and Francis Ltd.},
issn={00207543},
coden={IJPRB},
language={English},
abbrev_source_title={Int J Prod Res},
document_type={Article},
source={Scopus},
}

@ARTICLE{Altenmüller2020319,
author={Altenmüller, T. and Stüker, T. and Waschneck, B. and Kuhnle, A. and Lanza, G.},
title={Reinforcement learning for an intelligent and autonomous production control of complex job-shops under time constraints},
journal={Production Engineering},
year={2020},
volume={14},
number={3},
pages={319-328},
doi={10.1007/s11740-020-00967-8},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086023968&doi=10.1007%2fs11740-020-00967-8&partnerID=40&md5=611af8d374e35a0df5369d15945c86e9},
affiliation={Infineon Technologies AG, Am Campeon 1-12, Neubiberg, 85579, Germany; wbk Institute of Production Science, KIT, Kaiserstr. 12, Karlsruhe, 76131, Germany},
abstract={Reinforcement learning (RL) offers promising opportunities to handle the ever-increasing complexity in managing modern production systems. We apply a Q-learning algorithm in combination with a process-based discrete-event simulation in order to train a self-learning, intelligent, and autonomous agent for the decision problem of order dispatching in a complex job shop with strict time constraints. For the first time, we combine RL in production control with strict time constraints. The simulation represents the characteristics of complex job shops typically found in semiconductor manufacturing. A real-world use case from a wafer fab is addressed with a developed and implemented framework. The performance of an RL approach and benchmark heuristics are compared. It is shown that RL can be successfully applied to manage order dispatching in a complex environment including time constraints. An RL-agent with a gain function rewarding the selection of the least critical order with respect to time-constraints beats heuristic rules strictly by picking the most critical lot first. Hence, this work demonstrates that a self-learning agent can successfully manage time constraints with the agent performing better than the traditional benchmark, a time-constraint heuristic combining due date deviations and a classical first-in-first-out approach. © 2020, The Author(s).},
author_keywords={Complex job shop;  Production planning and control;  Reinforcement learning;  Time constraints},
keywords={Autonomous agents;  Benchmarking;  Discrete event simulation;  Learning algorithms;  Production control;  Reinforcement learning;  Semiconductor device manufacture, Complex environments;  Complex job shop;  Decision problems;  First in first outs;  Modern production systems;  Q-learning algorithms;  Semiconductor manufacturing;  Time constraints, Job shop scheduling},
references={Bauernhansl, T., ten Hompel, M., Vogel-Heuser, B., (2014) Industrie 4.0 in produktion, automatisierung und logistik, , Springer Fachmedien Wiesbaden, Wiesbaden; Sun, D.-S., Choung, Y.-I., Lee, Y.-J., Jang, Y.-C., Scheduling and control for time-constrained processes in semiconductor manufacturing (2005) ISSM 2005, IEEE International Symposium on Semiconductor Manufacturing, pp. 295-298; Eversheim, W., Wiendahl, H.P., (2000) Wörterbuch der PPS—Dictionary of PPC: Deutsch–Englisch/Englisch–Deutsch|German–English/English–German, , (eds), Springer, Berlin; Greschke, P., Schönemann, M., Thiede, S., Herrmann, C., Matrix structures for high volumes and flexibility in production systems (2014) Proced CIRP, 17, pp. 160-165; Kiener, S., Maier-Scheubeck, N., Obermaier, R., Weiß, M., (2017) Produktionsmanagement Grundlagen der Produktionsplanung und -steuerung, , 11, De Gruyter, Berlin; Klemmt, A., Mönch, L., Scheduling jobs with time constraints between consecutive process steps in semiconductor manufacturing (2012) Proceedings of the 2012 Winter Simulation Conference (WSC), pp. 1-10; Knopp, S., (2016) Complex Job-Shop Scheduling with Batching in Semiconductor Manufacturing, , Ph.D. thesis, L’Université de Lyon; Kuhnle, A., Jakubik, J., Lanza, G., Reinforcement learning for opportunistic maintenance optimization (2019) Prod Eng Res Devel, 13, pp. 33-41; Kuhnle, A., Röhrig, N., Lanza, G., Autonomous order dispatching in the semiconductor industry using reinforcement learning (2019) 12Th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 79, pp. 391-396. , Procedia CIRP; Kuhnle, A., Schäfer, L., Stricker, N., Lanza, G., Design, implementation and evaluation of reinforcement learning for an adaptive order dispatching in job shop manufacturing systems (2019) 52Nd CIRP Conference on Manufacturing Systems, Procedia CIRP, 81, pp. 234-239; Lanza, G., Ferdows, K., Kara, S., Mourtzis, D., Schuh, G., Váncza, J., Wang, L., Wiendahl, H.P., Global production networks: design and operation (2019) CIRP Ann, 68 (2), pp. 823-841; Lödding, H., (2016) Verfahren der Fertigungssteuerung Grundlagen, Beschreibung, Konfiguration, , 3, Springer, Berlin; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Ostrovski, G., Human-level control through deep reinforcement learning (2015) Nature, 518 (7540), pp. 529-533; Mönch, L., Fowler, J., Mason, S., (2013) Production planning and control for semiconductor wafer fabrication facilities: modeling, analysis, and systems, , Springer, New York; Plappert, M., (2016) Keras-Rl, , https://github.com/keras-rl/keras-rl, Accessed 17 Oct 2019; Russell, S.J., Norvig, P., (2009) Artificial intelligence: a modern approach, , 3, Pearson Education, Upper Saddle River; Schuh, G., Stich, V., (2012) Produktionsplanung und-steuerung 1: Grundlagen der PPS, , 4, Springer, Berlin; Schuh, G., Stich, V., (2012) Produktionsplanung und-steuerung 2: evolution der PPS, , 4, Springer, Berlin; Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Hassabis, D., Mastering the game of Go without human knowledge (2017) Nature, 550 (7676), pp. 354-359; Stricker, N., Kuhnle, A., Sturm, R., Friess, S., Reinforcement learning for adaptive order dispatching in the semiconductor industry (2018) CIRP Ann, 67 (1), pp. 511-514; Sutton, R.S., Barto, A.G., (2018) Reinforcement learning: an introduction, , 2, MIT Press, Cambridge; Waschneck, B., Altenmüller, T., Bauernhansl, T., Knapp, A., Kyek, A., Reichstaller, A., Belzner, L., Deep reinforcement learning for semiconductor production scheduling (2018) 2018 29Th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC), pp. 301-306; Waschneck, B., Altenmüller, T., Bauernhansl, T., Kyek, A., Production scheduling in complex job shops from an Industrie 4. 0 perspective: A review and challenges in the semiconductor industry (2016) Proceedings of Of Sami40 Workshop at I-Know ’16; Wiendahl, H.P., ElMaraghy, H., Nyhuis, P., Zäh, M., Wiendahl, H.H., Duffie, N., Brieke, M., Changeable manufacturing—classification, design and operation (2007) CIRP Ann, 56 (2), pp. 783-809},
correspondence_address1={Altenmüller, T.; Infineon Technologies AG, Am Campeon 1-12, Germany; email: thomas.altenmueller@infineon.com},
publisher={Springer},
issn={09446524},
language={English},
abbrev_source_title={Prod. Eng.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Martins202010810,
author={Martins, M.S.E. and Viegas, J.L. and Coito, T. and Firme, B.M. and Sousa, J.M.C. and Figueredo, J. and Vieira, S.M.},
title={Reinforcement learning for dual-resource constrained scheduling},
journal={IFAC-PapersOnLine},
year={2020},
volume={53},
pages={10810-10815},
doi={10.1016/j.ifacol.2020.12.2866},
note={cited By 3; Conference of 21st IFAC World Congress 2020 ; Conference Date: 12 July 2020 Through 17 July 2020;  Conference Code:145388},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107805245&doi=10.1016%2fj.ifacol.2020.12.2866&partnerID=40&md5=9b84172cd1a1167568aea9a925a5fef7},
affiliation={IDMEC, Instituto Superior Técnico, Universidade de Lisboa, Portugal; Department of Physics, Universidade de Évora, Évora, Portugal},
abstract={This paper proposes using reinforcement learning to solve scheduling problems where two types of resources of limited availability must be allocated. The goal is to minimize the makespan of a dual-resource constrained flexible job shop scheduling problem. Efficient practical implementation is very valuable to industry, yet it is often only solved combining heuristics and expert knowledge. A framework for training a reinforcement learning agent to schedule diverse dual-resource constrained job shops is presented. Comparison with other state-of-theart approaches is done on both simpler and more complex instances that the ones used for training. Results show the agent produces competitive solutions for small instances that can outperform the implemented heuristic if given enough time. Other extensions are needed before real-world deployment, such as deadlines and constraining resources to work shifts. Copyright © 2020 The Authors.},
author_keywords={Intelligent manufacturing systems;  Job and activity scheduling;  Production planning and control},
keywords={Optimization;  Reinforcement learning;  Scheduling, Dual resource constrained;  Expert knowledge;  Flexible job-shop scheduling problem;  Makespan;  Real world deployment;  Reinforcement learning agent;  Scheduling problem;  Work shifts, Job shop scheduling},
funding_details={Fundação para a Ciência e a TecnologiaFundação para a Ciência e a Tecnologia, FCT, UIDP/50022/2020},
funding_text 1={This work was supported by FCT, through IDMEC, under LAETA, project UIDP/50022/2020},
funding_text 2={a Tsehcisonwdormk awtearsiasﬄuprpeosrotuedrcebymFigChTt, atﬄhsrooubgeh nIDeeMdEedC,siumndueﬄ-r etesairﬄ.ed(2ﬄ0e1v8e)ﬄ;oCf ufrnehedaoemt aoﬄf. m(2u0ﬄ1ti9p)ﬄ.eOvnarythinegseinmteorsvternetcioennts ★This work was supported by FCT, through IDMEC, under pubﬄications, fforker intervention is modeﬄﬄed ffith the LAThisEhTisA,wporrokjecwtaUsIDsuppuPp/p5orted0r0t2e2d/2b0y20FCT, through IDMEC, under depesrireopderaﬄevtioeﬄn.ofTfrheeedofomrmoufﬄamtiouﬄntipcﬄoensvidearyingred foinrtertheventffioonsrk LAThisETA,wproorkjectwaUsIDPsupp/50022/202orted by0FCT, through IDMEC, under per operation. The formuﬄation considered for the ffork LAETA, project UIDP/50022/2020 per operation. The formuﬄation considered for the ffork L2405-8963 Copyright AETA, project UIDP©/50022/2022020 The Authors. This is an open access article under the CC BY-NC-ND license0 per operation. The formu.ﬄation considered for the ffork},
references={Araz, O.U., A simulation based multi-criteria scheduling approach of dual-resource constrained manufacturing systems with neural networks (2005) AI 2005: Advances in Artificial Intelligence, pp. 1047-1053. , Springer; Cunha, M., Viegas, J.L., Sousa, M.C., Vieira, S.M., Dual-resource constrained scheduling for quality control laboratories (2019) IFAC-PapersOnLine, p. 6; Dhiflaoui, M., Nouri, H.E., Driss, O.B., Dualresource constraints in classical and flexible job shop problems: A state-of-the-art review (2018) Procedia Computer Science, 126, pp. 1507-1515; Ham, A., Scheduling of dual resource constrained lithography production: Using (2018) IEEE Transactions on Semiconductor Manufacturing, 31 (1), pp. 52-61; Huiyuan, R., Lili, J., Xiaoying, X., Muzhi, L., Heuristic optimization for dual-resource constrained job shop scheduling (2009) 2009 International Asia Conference on Informatics in Control, Automation and Robotics, pp. 485-488. , IEEE; Morshed, M.S., Jain, S.A., Meeran, S., (2017) A Stateof-the-art Review of Job-Shop Scheduling Techniques.; Nouri, H.E., Belkahla, O., Khaled, D., (2016) Hybrid Metaheuristics for Scheduling of Machines and Transport Robots in Job Shop Environment., pp. 808-828; Pinedo, M.L., (2009) Planning and Scheduling in Manufacturing and Services., , Springer, 2 edition; Shen, L., Dauzère-Pérès, S., Neufeld, J.S., Solving the flexible job shop scheduling problem with sequence-dependent setup times (2018) European Journal of Operational Research, 265 (1), pp. 503-516; Sutton, R.S., Barto, A.G., (2018) Reinforcement Learning: An Introduction, , The MIT Press, 2nd edition; Viegas, J.L., Cunha, M., Martins, M., Coito, T., Figueiredo, J., Sousa, J.M., Vieira, S.M., A flexible heuristic for the dual-resource constrained scheduling problem in quality control laboratories [abstract] (2019) 30th European Conference on Operational Research.; Zhang, W., Dietterich, T.G., Solving combinatorial optimization tasks by reinforcement learning (2000) Journal of Artificial Intelligence Research, 1},
correspondence_address1={Martins, M.S.E.; IDMEC, Portugal; email: miguelsemartins@tecnico.ulisboa.pt; Viegas, J.L.; IDMEC, Portugal; email: joaquim.viegas@tecnico.ulisboa.pt; Coito, T.; IDMEC, Portugal; email: tiagoascoito@tecnico.ulisboa.pt; Firme, B.M.; IDMEC, Portugal; email: bernardo.firme@tecnico.ulisboa.pt; Sousa, J.M.C.; IDMEC, Portugal; email: jmsousa@tecnico.ulisboa.pt; Vieira, S.M.; IDMEC, Portugal; email: susana.vieira@tecnico.ulisboa.pt},
publisher={Elsevier B.V.},
issn={24058963},
language={English},
abbrev_source_title={IFAC-PapersOnLine},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tang20209309,
author={Tang, Y. and Agrawal, S. and Faenza, Y.},
title={Reinforcement learning for integer programming: Learning to cut},
journal={37th International Conference on Machine Learning, ICML 2020},
year={2020},
volume={PartF168147-13},
pages={9309-9318},
note={cited By 16; Conference of 37th International Conference on Machine Learning, ICML 2020 ; Conference Date: 13 July 2020 Through 18 July 2020;  Conference Code:168147},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105254794&partnerID=40&md5=9b4e053bbf80eb5aaf482c0722d2e5fd},
affiliation={Columbia University, New York, United States},
abstract={Integer programming is a general optimization framework with a wide variety of applications, e.g., in scheduling, production planning, and graph optimization. As Integer Programs (IPs) model many provably hard to solve problems, modern IP solvers rely on heuristics. These heuristics are often human-designed, and tuned over time using experience and data. The goal of this work is to show that the performance of those heuristics can be greatly enhanced using reinforcement learning (RL). In particular, we investigate a specific methodology for solving IPs, known as the cutting plane method. This method is employed as a subroutine by all modern IP solvers. We present a deep RL formulation, network architecture, and algorithms for intelligent adaptive selection of cutting planes (aka cuts). Across a wide range of IP tasks, we show that our trained RL agent significantly outperforms human-designed heuristics. Further, our experiments show that the RL agent adds meaningful cuts (e.g. resembling cover inequalities when applied to the knapsack problem), and has generalization properties across instance sizes and problem classes. The trained agent is also demonstrated to benefit the popular downstream application of cutting plane methods in Branch-And-Cut algorithm, which is the backbone of state-of-The-Art commercial IP solvers. © 2020 by the Authors.},
keywords={Combinatorial optimization;  Internet protocols;  Learning systems;  Network architecture;  Production control;  Reinforcement learning;  User experience, Adaptive selection;  Branch-and-cut algorithms;  Cutting plane methods;  Downstream applications;  General optimizations;  Generalization properties;  Graph optimization;  Production Planning, Integer programming},
references={Balas, E., Ceria, S., Cornuéjols, G., A lift-Andproject cutting plane algorithm for mixed 0?1 programs (1993) Mathematical programming, 58 (1-3), pp. 295-324; Balcan, M.-F., Dick, T., Sandholm, T., Vitercik, E., (2018) Learning to branch, , arXiv preprint arXiv:1803.10150; Bello, I., Pham, H., Le, Q. V., Norouzi, M., Bengio, S., (2017) Neural combinatorial optimization; Bengio, Y., Lodi, A., Prouvost, A., (2018) Machine learning for combinatorial optimization: A methodological tour d?horizon, , arXiv preprint arXiv:1811.06128; Bixby, B., Optimization: past, present, future (2017) Plenary talk at INFORMS Annual Meeting; Conforti, M., Cornuéjols, G., Zambelli, G., (2014) Integer programming, 271. , Springer; Cornuéjols, G., Margot, F., Nannicini, G., On the safety of gomory cut generators (2013) Math. Program. Comput, 5 (4), pp. 345-395; Crowder, H., Johnson, E. L., Padberg, M., Solving large-scale zero-one linear programming problems (1983) Operations Research, 31 (5), pp. 803-834; Dai, H., Khalil, E. B., Zhang, Y., Dilkina, B., Song, L., (2017) Learning combinatorial optimization algorithms over graphs, , arXiv preprint arXiv:1704.01665; Dantzig, G., Fulkerson, R., Johnson, S., Solution of a large-scale traveling-salesman problem (1954) Journal of the operations research society of America, 2 (4), pp. 393-410; Dey, S. S., Molinaro, M., Theoretical challenges towards cutting-plane selection (2018) Mathematical Programming, 170 (1), pp. 237-266; Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Dunning, I., (2018) Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures, , arXiv preprint arXiv:1802.01561; Gomory, R., (1960) An algorithm for the mixed integer problem, , Technical report, RAND CORP SANTA MONICA CA; Gurobi Optimization, I., (2015) Gurobi optimizer reference manual, , http://www.gurobi.com; Hochreiter, S., Schmidhuber, J., Long short-Term memory (1997) Neural computation, 9 (8), pp. 1735-1780; Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., Dabney, W., (2018) Recurrent experience replay in distributed reinforcement learning; Kellerer, H., Pferschy, U., Pisinger, D., (2003) Knapsack problems, , 2004; Khalil, E., Dai, H., Zhang, Y., Dilkina, B., Song, L., Learning combinatorial optimization algorithms over graphs (2017) Advances in Neural Information Processing Systems, pp. 6348-6358; Khalil, E. B., Le Bodic, P., Song, L., Nemhauser, G. L., Dilkina, B. N., Learning to branch in mixed integer programming (2016) AAAI, pp. 724-731; Kingma, D. P., Ba, J., (2014) Adam: A method for stochastic optimization, , arXiv preprint arXiv:1412.6980; Kool, W., Welling, M., (2018) Attention solves your tsp, , arXiv preprint arXiv:1803.08475; Li, Z., Chen, Q., Koltun, V., Combinatorial optimization with graph convolutional networks and guided tree search (2018) Advances in Neural Information Processing Systems, pp. 539-548; Martello, S., Toth, P., (1990) Knapsack problems: Algorithms and computer implementations, , Wiley-Interscience series in discrete mathematics and optimiza tion; Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., Kavukcuoglu, K., Asynchronous methods for deep reinforcement learning (2016) International conference on machine learning, pp. 1928-1937; Nowak, A., Villar, S., Bandeira, A. S., Bruna, J., (2017) A note on learning algorithms for quadratic assignment with graph neural networks, , arXiv preprint arXiv:1706.07450; Pochet, Y., andWolsey, L. A., (2006) Production planning by mixed integer programming, , Springer Science & Business Media; Rothvoß, T., Sanità, L., 0/1 polytopes with quadratic chvátal rank (2017) Operations Research, 65 (1), pp. 212-220; Salimans, T., Ho, J., Chen, X., Sidor, S., Sutskever, I., (2017) Evolution strategies as a scalable alternative to reinforcement learning, , arXiv preprint arXiv:1703.03864; Sutskever, I., Vinyals, O., Le, Q. V., Sequence to sequence learning with neural networks (2014) Advances in neural information processing systems, pp. 3104-3112; Tokui, S., Oono, K., Hido, S., Clayton, J., Chainer: A next-generation open source framework for deep learning (2015) Proceedings of workshop on machine learning systems (LearningSys) in the twenty-ninth annual conference on neural information processing systems (NIPS), 5, pp. 1-6; Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., Polosukhin, I., Attention is all you need (2017) Advances in neural information processing systems, pp. 5998-6008; Vinyals, O., Fortunato, M., Jaitly, N., Pointer networks (2015) Advances in Neural Information Processing Systems, pp. 2692-2700; Wesselmann, F., Stuhl, U., (2012) Implementing cutting plane management and selection techniques, , Technical report, Tech. rep., University of Paderborn},
correspondence_address1={Tang, Y.; Columbia UniversityUnited States; email: yt2541@coluymbia.edu},
editor={Daume H., Singh A.},
publisher={International Machine Learning Society (IMLS)},
isbn={9781713821120},
language={English},
abbrev_source_title={Int. Conf. Machin. Learn., ICML},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{May20203,
author={May, M.C. and Kiefer, L. and Kuhnle, A. and Stricker, N. and Lanza, G.},
title={Decentralized Multi-Agent Production Control through Economic Model Bidding for Matrix Production Systems},
journal={Procedia CIRP},
year={2020},
volume={96},
pages={3-8},
doi={10.1016/j.procir.2021.01.043},
note={cited By 6; Conference of 8th CIRP Global Web Conference on Flexible Mass Customisation, CIRPe 2020 ; Conference Date: 14 October 2020 Through 16 October 2020;  Conference Code:167016},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101102144&doi=10.1016%2fj.procir.2021.01.043&partnerID=40&md5=b71a1e99ef7f6e07c238cea29bbc2c6b},
affiliation={Wbk Institute of Production Science, Karlsruhe Institute of Technology (KIT), Kaiserstr. 12, Karlsruhe, 76131, Germany},
abstract={Due to increasing demand for unique products, large variety in product portfolios and the associated rise in individualization, the efficient use of resources in traditional line production dwindles. One answer to these new challenges is the application of matrix-shaped layouts with multiple production cells, called Matrix Production Systems. The cycle time independence and redundancy of production cell capabilities within a Matrix Production System enable individual production paths per job for Flexible Mass Customisation. However, the increased degrees of freedom strengthen the need for reliable production control systems compared to traditional production systems such as line production. Beyond reliability a need for intelligent production within a smart factory in order to ensure goal-oriented production control under ever-changing manufacturing conditions can be ascertained. Learning-based methods can leverage condition-based reactions for goal-oriented production control. While centralized control performs well in single-objective situations, it is hard to achieve contradictory targets for individual products or resources. Hence, in order to master these challenges, a production control concept based on a decentralized multi-agent bidding system is presented. In this price-based model, individual production agents - jobs, production cells and transport system - interact based on an economic model and attempt to maximize monetary revenues. Evaluating the application of learning and priority-based control policies shows that decentralized multi-agent production control can outperform traditional approaches for certain control objectives. The introduction of decentralized multi-agent reinforcement learning systems is a starting point for further research in this area of intelligent production control within smart manufacturing. © 2020 Elsevier B.V.. All rights reserved.},
author_keywords={Control;  Industry 4.0;  Mass Customisation;  Matrix Production;  Production Planning;  Smart Factory},
keywords={Degrees of freedom (mechanics);  Economics;  Industrial research;  Learning systems;  Manufacture;  Production control;  Reinforcement learning, Centralized control;  Intelligent production control;  Learning-based methods;  Manufacturing conditions;  Mass customisation;  Multi-agent reinforcement learning;  Smart manufacturing;  Traditional approaches, Multi agent systems},
funding_details={Horizon 2020Horizon 2020, 814225},
funding_text 1={This research work was undertaken in the context of DIGI-MAN4.0 project (“DIGItal MANufacturing Technologies for Zero-defect Industry 4.0 Production”, http://www.digiman4-0.mek.dtu.dk/). DIGIMAN4.0 is a European Training Network supported by Horizon 2020, the EU Framework Programme for Research and Innovation (Project ID: 814225).},
references={Balaji, P., Srinivasan, D., (2010) An Introduction to Multi-agent Systems, In: Innovations in Multi-agent Systems and applications-1, pp. 1-27. , Springer; Bochmann, L.S., (2018) Entwicklung und Bewertung Eines Flexiblen und Dezentral Gesteuerten Fertigungssystems für Variantenreiche Produkte (Development and Evaluation of A Flexible and Decentrally Controlled Manufacturing System for Multi-variant Products), , Ph.D. thesis. ETH Zurich; Davidsson, P., Wernstedt, F., A multi-agent system architecture for coordination of just-in-time production and distribution (2002) The Knowledge Engineering Review, 17, pp. 317-329; Dittrich, M.A., Fohlmeister, S., Cooperative multi-agent system for production control using reinforcement learning (2020) CIRP Annals; Duray, R., Ward, P.T., Milligan, G.W., Berry, W.L., Approaches to mass customization: Configurations and empirical validation (2000) Journal of Operations Management, 18, pp. 605-625; Echsler Minguillon, F., Lanza, G., Maschinelles lernen in der pps (Machine Learning in PPC) (2017) Wt Werkstatttechnik Online, 107, pp. 630-634; Elmaraghy, H., Schuh, G., Elmaraghy, W., Piller, F., Schönsleben, P., Tseng, M., Bernard, A., Product variety management (2013) Cirp Annals, 62, pp. 629-652; Filz, M.A., Gerberding, J., Herrmann, C., Thiede, S., Analyzing different material supply strategies in matrix-structured manufacturing systems (2019) Procedia CIRP, 81, pp. 1004-1009; Giordani, S., Lujak, M., Martinelli, F., A distributed multi-agent production planning and scheduling framework for mobile robots (2013) Computers & Industrial Engineering, 64, pp. 19-30; Greschke, P., Herrmann, C., The human potential of a clock-independent assembly (2014) ZWF Zeitschrift für Wirtschaftlichen Fabrikbetrieb, 109, pp. 687-690; Greschke, P., Schönemann, M., Thiede, S., Herrmann, C., Matrix structures for high volumes and flexibility in production systems (2014) Procedia CIRP, 17, pp. 160-165; Greschke, P.I., (2016) Matrix-Produktion Als Konzept Einer Taktun-abhängigen Fließfertigung (Matrix Production As A Concept of Takt-free Flowline Manufacturing), , Vulkan Verlag; Gu, P., Balasubramanian, S., Norrie, D., Bidding-based process planning and scheduling in a multi-agent system (1997) Computers & Industrial Engineering, 32, pp. 477-496; Heger, J., Branke, J., Hildebrandt, T., Scholz-Reiter, B., Dynamic adjustment of dispatching rule parameters in flow shops with sequence-dependent set-up times (2016) International Journal of Production Research, 54, pp. 6812-6824; Kaihara, T., Multi-agent based supply chain modelling with dynamic environment (2003) International Journal of Production Economics, 85, pp. 263-269; Kang, N., Zhao, C., Li, J., Horst, J.A., A hierarchical structure of key performance indicators for operation management and continuous improvement in production systems (2016) International Journal of Production Research, 54, pp. 6333-6350; Kern, W., Rusitschka, F., Kopytynski, W., Keckl, S., Bauernhansl, T., Alternatives to assembly line production in the automotive industry (2015) Proceedings of the 23rd International Conference on Production Research (IFPR); Malus, A., Kozjek, D., (2020) Real-time Order Dispatching for A Fleet of Autonomous Mobile Robots Using Multi-agent Reinforcement Learning, , CIRP Annals; May, M.C., Kuhnle, A., Lanza, G., Digitale produktion und intel-ligente steuerung (Digital Production and intelligent control) (2020) Wt Werk-statttechnik Online, 110, pp. 655-660. , https://doi.org/10.5445/IR/1000119555; May, M.C., Overbeck, L., Wurster, M., Kuhnle, A., Lanza, G., Fore-sighted digital twin for situational agent selection in production control in press (2020) Procedia CIRP; Monostori, L., Váncza, J., Kumara, S.R., Agent-based systems for manufacturing (2006) CIRP Annals, 55, pp. 697-720; Naso, D., Turchiano, B., Multicriteria meta-heuristics for agv dispatching control based on computational intelligence (2005) IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 35, pp. 208-226; Schaarschmidt, M., Kuhnle, A., Fricke, K., (2017) Tensorforce: A Tensor-flow Library for Applied Reinforcement Learning, , Web page; Scholz-Reiter, B., Görges, M., Philipp, T., Autonomously controlled production systems - Influence of autonomous control level on logistic performance (2009) CIRP Annals, 58, pp. 395-398; Scholz-Reiter, B., Hamann, T., The behaviour of learning production control (2008) CIRP Annals, 57, pp. 459-462; Schönemann, M., Herrmann, C., Greschke, P., Thiede, S., Simulation of matrix-structured manufacturing systems (2015) Journal of Manufacturing Systems, 37, pp. 104-112; Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O., (2017) Proximal Policy Optimization Algorithms, , preprint arXiv:1707.06347; Stricker, N., Kuhnle, A., Sturm, R., Friess, S., Reinforcement learning for adaptive order dispatching in the semiconductor industry (2018) CIRP Annals, 67, pp. 511-514. , doi:10.1016/j.cirp.2018.04.041; Sutton, R.S., Barto, A.G., (2018) Reinforcement Learning: An Introduction, , MIT press; Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru, J., Vicente, R., Multiagent cooperation and competition with deep reinforcement learning (2017) PloS One, p. 12; Tuyls, K., Nowe, A., Guessoum, Z., Kudenko, D., Adaptive Agents and Multi-Agent Systems III. Adaptation and Multi-Agent Learning: 5th, 6th, and 7th European Symposium (2008) ALAMAS 2005-2007 on Adaptive and Learning Agents and Multi-Agent Systems, , Springer; Wang, S., Wan, J., Zhang, D., Li, D., Zhang, C., Towards smart factory for industry 4.0: A self-organized multi-agent system with big data based feedback and coordination (2016) Computer Networks, 101, pp. 158-168},
correspondence_address1={May, M.C.; Wbk Institute of Production Science, Kaiserstr. 12, Germany; email: marvin.may@kit.edu},
editor={Kellens K., Ferraris E., Demeester E.},
publisher={Elsevier B.V.},
issn={22128271},
language={English},
abbrev_source_title={Procedia CIRP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dittrich2020389,
author={Dittrich, M.-A. and Fohlmeister, S.},
title={Cooperative multi-agent system for production control using reinforcement learning},
journal={CIRP Annals},
year={2020},
volume={69},
number={1},
pages={389-392},
doi={10.1016/j.cirp.2020.04.005},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084728013&doi=10.1016%2fj.cirp.2020.04.005&partnerID=40&md5=046d76b9e6fff85a74be67286f8c14cf},
affiliation={Institute of Production Engineering and Machine Tools (IFW), Leibniz Universität Hannover, Germany},
abstract={Multi-agent systems can limit the control problem in complex production systems and solve them more efficiently. However, they often show local optimization tendencies. This paper presents a novel approach for a cooperative multi-agent system, which uses reinforcement learning and considers global key performance indicators. For this purpose, a central deep q-learning module transfers its knowledge to the cooperative order agents. The order agent's experience is stored in a replay memory for subsequent reinforcement learning. Interdependencies between the characteristics of nonlinear production systems and learning parameters are investigated and the performance is evaluated in comparison to conventional methods of production control. © 2020 CIRP},
author_keywords={Machine learning;  Multi-agent system;  Production planning},
keywords={Benchmarking;  Deep learning;  Learning systems;  Production control;  Reinforcement learning;  Transfer learning, Complex production systems;  Control problems;  Conventional methods;  Key performance indicators;  Learning parameters;  Local optimizations;  Production system;  Q-learning, Multi agent systems},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG},
funding_text 1={The presented investigations were conducted within the research project DE 447/181-1. We would like to thank the German Research Foundation (DFG) for the support of this project. In addition, we would like to thank Berend Denkena for his valuable comments and his support.},
references={ElMaraghy, H., Schuh, G., ElMaraghy, W., Piller, F., Schönsleben, P., Product variety management (2013) CIRP Annals-Manufacturing Technology, 62 (2), pp. 629-652; Lödding, H., Handbook of Manufacturing Control – Fundamentals, Description, Configuration (2013), Springer Heidelberg; Williamson, D.P., Hall, L.A., Hoogeveen, J.A., Hurkens, C.A.J., Lenstra, J.K., Short Shop Schedules (1997) Operation Research, 45 (2), pp. 288-294; Freitag, M., Hildebrandt, T., Automatic design of scheduling rules for complex manufacturing systems by multi-objective simulation-based optimization (2016) CIRP Annals-Manufacturing Technology, 65 (1), pp. 433-436; Vrabic, R., Kozjek, D., Malus, A., Zaletelj, V., Butala, P., Distributed control with rationally bounded agents in cyber-physical production systems (2018) CIRP Annals-Manufacturing Technology, 67 (1), pp. 507-510; Schuh, G., Reuter, C., Prote, J.P., Brambring, F., Ays, J., Increasing data integrity for improving decision making in production planning and control (2017) CIRP Annals-Manufacturing Technology, 66 (1), pp. 425-428; Wooldridge, M.J., An introduction to multiagent systems (2002), Wiley Chichester; Leitão, P., Agent-based distributed manufacturing control: A state-of-the-art survey (2009) Engineering Applications of Artifical Intelligence, 22 (7), pp. 979-991; Monostori, L., Váncza, J., Kumara, S.R.T., Agent-Based Systems for Manufacturing (2006) CIRP Annals-Manufacturing Technology, 55 (2), pp. 697-720; Monostori, L., Kádár, B., Bauernhansl, T., Kondoh, S., Kumara, S., Cyber-physical systems in manufacturing (2016) CIRP Annals-Manufacturing Technology, 65 (2), pp. 621-641; Waschneck, B., Reichstaller, A., Belzner, L., Altenmüller, T., Bauernhansl, T., Optimization of global production scheduling with deep reinforcement learning (2018) Procedia CIRP, 72, pp. 1264-1269; Sutton, R.S., Barto, A.G., Reinforcement Learning: An Introduction (1998), The MIT Press Cambridge, Massachusetts, London; Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Human-level control through deep reinforcement learning (2015) Nature, 518, pp. 529-533; Stricker, N., Kuhnle, A., Sturm, R., Friess, S., Reinforcement learning for adaptive order dispatching in the semiconductor industry (2018) CIRP Annals-Manufacturing Technology, 67 (1), pp. 511-514; Hornik, K., Approximation capabilities of multilayer feedforward networks (1991) Neural Networks, 4 (2), pp. 251-257; Glorot, X., Bordes, A., Bengio, Y., Deep Sparse Rectifier Neural Networks (2011) Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 315-323},
correspondence_address1={Dittrich, M.-A.; Institute of Production Engineering and Machine Tools (IFW), Germany; email: dittrich@ifw.uni-hannover.de},
publisher={Elsevier USA},
issn={00078506},
coden={CIRAA},
language={English},
abbrev_source_title={CIRP Ann},
document_type={Article},
source={Scopus},
}

@ARTICLE{Seyr2019,
author={Seyr, H. and Muskulus, M.},
title={Use of Markov decision processes in the evaluation of corrective maintenance scheduling policies for offshore wind farms},
journal={Energies},
year={2019},
volume={14},
number={15},
doi={10.3390/en12152993},
art_number={2993},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070329651&doi=10.3390%2fen12152993&partnerID=40&md5=9c3f0a9f0649fd2c62a9327c4babd1b9},
affiliation={Department of Civil and Environmental Engineering, Norwegian University of Science and Technology, NTNU, Trondheim, NO-7491, Norway},
abstract={Optimization of the maintenance policies for offshore wind parks is an important step in lowering the costs of energy production from wind. The yield from wind energy production is expected to fall, which will increase the need to be cost efficient. In this article, the Markov decision process is presented and how it can be applied to evaluate different policies for corrective maintenance planning. In the case study, we show an alternative to the current state-of-the-art policy for corrective maintenance that will achieve a cost-reduction when energy production prices drop below the current levels. The presented method can be extended and applied to evaluate additional policies, with some examples provided. © 2019 by the authors.},
author_keywords={Corrective maintenance;  Maintenance;  Maintenance planning;  Maintenance scheduling;  Maintenance strategy;  Modeling;  Offshore wind energy;  Optimization;  Repair},
keywords={Behavioral research;  Cost reduction;  Learning algorithms;  Maintenance;  Markov processes;  Models;  Offshore oil well production;  Optimization;  Planning;  Reinforcement learning;  Repair;  Scheduling, Corrective maintenance;  Maintenance planning;  Maintenance scheduling;  Maintenance strategies;  Off-shore wind energy, Offshore wind farms},
funding_details={Awesome Foundation for the Arts and SciencesAwesome Foundation for the Arts and Sciences},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020},
funding_details={H2020 Marie Skłodowska-Curie ActionsH2020 Marie Skłodowska-Curie Actions, MSCA, 642108},
funding_details={Bundesministerium für Wirtschaft und EnergieBundesministerium für Wirtschaft und Energie, BMWi},
funding_text 1={Funding: Part of the work leading to this publication was financed by the AWESOME project (awesome-h2020.eu), which has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie Grant No. 642108. Most of the work of the first author was completed without funding after the funding period was over.},
funding_text 2={Part of the work leading to this publication was financed by the AWESOME project (awesome-h2020.eu), which has received funding from the European Union?s Horizon 2020 research and innovation programme under the Marie Sk?odowska-Curie Grant No. 642108. Most of the work of the first author was completed without funding after the funding period was over.},
funding_text 3={Wind and wave data from the FINO 1 project are provided by the Bundesministerium für Wirtschaft und Energie (BMWi), Federal Ministry for Economic Affairs and Energy and the Projektträger Jülich, project executing organization (PTJ). They can be downloaded from http://fino.bsh.de/ by users from Europe, for research purposes.},
references={Feng, Y., Tavner, P., Long, H., Early experience with UK round 1 offshore wind farms (2010) Energy, 163, pp. 167-181; Seyr, H., Muskulus, M., Decision support models for operations and maintenance for offshore wind farms: A review (2019) Appl. Sci., 9, p. 278; Gintautas, T., Sørensen, J.D., Improved methodology of weather window prediction for offshore operations based on probabilities of operation failure (2017) J. Mar. Sci. Eng., 5, p. 20; Stock-Williams, C., Swamy, S.K., Automated daily maintenance planning for offshore wind farms (2018) Renew. Energy; Seyr, H., Muskulus, M., Value of information of repair times for offshore wind farm maintenance planning (2016) J. Physics: Conf. Ser., 753, p. 092009; Seyr, H., Muskulus, M., How does accuracy of weather forecasts influence the maintenance cost in offshore wind farms? (2017) Proceedings of the 27th International Ocean and Polar Engineering Conference, International Society of Offshore and Polar Engineers, , San Francisco, CA, USA, 25–30 June; Gonzalez, E., Nanos, E.M., Seyr, H., Valldecabres, L., Yürüşen, N.Y., Smolka, U., Muskulus, M., Melero, J.J., Key performance indicators for wind farm operation and maintenance (2017) Energy Procedia, 137, pp. 559-570; Puterman, M.L., (2014) Markov Decision Processes: Discrete Stochastic Dynamic Programming, , John Wiley & Sons: Hoboken, NJ, USA; Jonkman, J., Butterfield, S., Musial, W., Scott, G., (2009) Definition of A 5-MW Reference Wind Turbine for Offshore System Development, , National Renewable Energy Laboratory: Golden, CO, USA, CrossRef; Bak, C., Zahle, F., Bitsche, R., Kim, T., Yde, A., Henriksen, L.C., Hansen, M.H., Natarajan, A., The DTU 10-MW reference wind turbine (2013) Danish Wind Power Research; Sound/Visual Production, , Trinity: Fredericia, Denmark; Carroll, J., McDonald, A., McMillan, D., Failure rate, repair time and unscheduled O&M cost analysis of offshore wind turbines (2015) Wind. Energy, 19, pp. 1107-1119; FINO Datenbank, , http://fino.bsh.de, Seeschifffahrt und Hydrographie BSH. Available online: accessed on 23 January 2018; Dinwoodie, I., Endrerud, O.E.V., Hofmann, M., Martin, R., Sperstad, I.B., Reference cases for verification of operation and maintenance simulation models for offshore wind farms (2015) Wind. Eng.; Dornhelm, E., Seyr, H., Muskulus, M., Vindby—A serious offshore wind farm design game (2019) Energies, 12, p. 1499; van Bussel, G., Bierbooms, W., The DOWEC offshore reference windfarm: Analysis of transportation for operation and maintenance (2003) Wind. Eng., 27, pp. 381-391; Energy Statistics-Electricity Prices for Domestic and Industrial Consumers, Price Components, , https://ec.europa.eu/eurostat/web/energy/data/database, Eurostat, the Statistical Office of the European Union. Available online: accessed on 4 June 2019; Kost, C., Shammugam, S., Jülch, V., Nguyen, H.T., Schlegl, T., (2014) Stromgestehungskosten Erneuerbare Energien, , Fraunhofer-Institut für Solare Energiesysteme ISE: Freiburg, Germany, German; Martin, R., Lazakis, I., Barbouchi, S., Johanning, L., Sensitivity analysis of offshore wind farm operation and maintenance cost and availability (2016) Renew. Energy, 85, pp. 1226-1236},
correspondence_address1={Seyr, H.; Department of Civil and Environmental Engineering, Norway; email: helene.seyr@gmail.com},
publisher={MDPI AG},
issn={19961073},
language={English},
abbrev_source_title={Energies},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ender201951,
author={Ender, J. and Wagner, J.C. and Kunert, G. and Larek, R. and Pawletta, T. and Guo, F.B.},
title={Design of an Assisting Workplace Cell for Human-Robot Collaboration},
journal={2019 International Interdisciplinary PhD Workshop, IIPhDW 2019},
year={2019},
pages={51-56},
doi={10.1109/IIPHDW.2019.8755412},
art_number={8755412},
note={cited By 6; Conference of 2019 International Interdisciplinary PhD Workshop, IIPhDW 2019 ; Conference Date: 15 May 2019 Through 17 May 2019;  Conference Code:149334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069539237&doi=10.1109%2fIIPHDW.2019.8755412&partnerID=40&md5=65e949220f584383c9ec3a32148c04ed},
affiliation={Faculty of Engineering, Hochschule Wismar University of Applied Sciences: Technology, Business and Design, Wismar, Germany; Faculty of Engineering and Technology, Liverpool John Moores University, Liverpool, United Kingdom},
abstract={Currently, greater attention is paid to the nature of work and workplaces within the digitized industry due to the increasing complexity of work tasks. The operators are under additional stress due to the range of high dynamic processes and due to the integration of robots and autonomous operating machines. There have been few studies on how Human Factors influence the design of workplaces for Human-Robot Collaboration (HRC). Furthermore, a comprehensive, systematic and human-centered design solution for industrial workplaces particularly considering Human Factor needs within HRC is widely uncertain and a specific application with reference to production workplaces is missing.The research findings demonstrated in this paper aim to fill in the gap of designing industrial workplaces for assisting human-robot teams through a human-centered design approach in order to reduce workers' strain during their interactions with robots and/or machines. Furthermore, the concept of an Assisting-Industrial-Workplace-System (AIWS) has been developed as a flexible hybrid cell for HRC to be integrated into a Self-Adapting-Production-Planning-System (SAPPS), which aims to enable industrial production processes to be adaptable to the constantly changing requirements of volatile markets. © 2019 IEEE.},
author_keywords={Assisting workplace design;  extended network plan;  Human Factors Design;  Human-Robot Collaboration;  human-robot teams;  machine learning;  post-optimizing reinforcement learning},
keywords={Human engineering;  Industrial research;  Learning systems;  Machine design;  Machine learning;  Production control;  Reinforcement learning, Additional stress;  Extended networks;  Human-centered designs;  Human-robot collaboration;  Human-robot-team;  Industrial production;  Production planning systems;  Workplace design, Human robot interaction},
references={Spath, D., (2013) Produktionsarbeit der Zukunft-Industrie 4.0: Fraunhofer Verlag; (2013) Deutschlands Zukunft Als Produktionsstandort Sichern, , acatech: Umsetzungsempfehlungen für das Zukunftsprojekt Industrie 4.0: Abschlussbericht des Arbeitskreises Industrie 4.0, Apr; Freitag, M., Molzow-Voit, F., Quandt, M., Spöttl, G., Aktuelle entwicklung der robotik und ihre implikationen für den menschen (2016) Robotik in der Logistik: Qualifizierung für Fachkräfte und Entscheider, pp. 9-20. , F. Molzow-Voit, M. Quandt, M. Freitag, and G. Spöttl, Eds., 1st ed., Wiesbaden: Springer Gabler; Lorenz, M., Rüßmann, M., Strack, R., Lueth, K.L., Bolle, M., Man and machine in industry 4.0: How will technology transform the industrial workforce through 2025 (2018) BCC the Boston Consulting Group, , http://englishbulletin.adapt.it/wpcontent/uploads/2015/10/BCG-Man-and-Machine-in-Industry-4-0-Sep-2015-tcm80-197250.pdf, Accessed on: Oct. 15 2018; Onnasch, L., Maier, X., Jürgensohn, T., (2016) Mensch-Roboter-Interaktion-Eine Taxonomie für Alle Anwendungsfälle; (2017) Arbeitswelten der Zukunft: Jahresbericht, , Fraunhofer IAO; Stark, J., Mota, R.R.C., Sharlin, E., Personal space intrusion in human-robot collaboration (2018) Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction-HRI '18, pp. 245-246. , Chicago, IL, USA; International Ergonomics Association IEA, Definition and Domains of Ergonomics, , https://www.iea.cc/, Accessed on: Feb. 25 2019; Bullinger, H.-J., (1994) Ergonomie: Produkt-und Arbeitsplatzgestaltung. Wiesbaden, S.l.: Vieweg+Teubner Verlag; Bullinger-Hoffmann, A.C., Mühlstedt, J., (2016) Homo Sapiens Digitalis-Virtuelle Ergonomie und Digitale Menschmodelle, , Wiesbaden: Springer Vieweg; Goschke, T., Aktivationstheoretische Ansätze: Motivation, Emotion, Volition (2013) Tu Dresden; Yerkes, R.M., Dodson, J.D., The relation of strength of stimulus to rapidity of habit-formation (1908) J. Comp. Neurol. Psychol, 18 (5), pp. 459-482; Braseth, A.O., (2015) Information-Rich Design: A Concept for Large-Screen Display Graphics: Design Principles and Graphic Elements for Real-World Complex Processes, , Norwegian University of Science and Technology; Endsley, M.R., Jones, D.G., (2011) Designing for Situation Awareness: An Approach to User-centered Design, , 2nd ed. Boca Raton, Fla.: CRC Press; Bauernhansl, T., Ten Hompel, M., Vogel-Heuser, B., (2014) Industrie 4.0 in Produktion, Automatisierung und Logistik: Anwendung, Technologien, Migration, , Wiesbaden: Springer Vieweg; (2016) Zukunft der Arbeit: Innovationen für Die Arbeit von Morgen, , Bundesministerium für Bildung und Forschung, Ed Jan; Sanders, E.B.-N., Stappers, P.J., Co-creation and the new landscapes of design (2008) CoDesign, 4 (1), pp. 5-18; (2018) Without Design, Industry 4.0 Will Fail: Six Challenges Where Design Accelerates Successful Digital Transformation in Manufacturing, , https://www.ixds.com/without-design-industry-40-will-fail, IXDS Human Industries Venture Accessed on: Jun. 27 2018; Norman, D.A., (2013) The Design of Everyday Things, , New York, NY: Basic Books; (2017) 360Focus-Creativity: Creativity, Work and the Physical Environment 17-0005439, , t Steelcase Inc; Lee, J.D., (2017) An Introduction to Human Factors Engineering: A Beta Version. [S.l.]: Createspace; Westkämper, E., Spath, D., Constantinescu, C., Lentes, J., (2013) Digitale Produktion. Berlin, Heidelberg, S.l, , Springer Berlin Heidelberg; Bannat, A., (2014) Ein Assistenzsystem Zur Digitalen Werker-Unterstützung in der Industriellen Produktion: TU München, Lehrstuhl für Mensch-Maschine-Kommunikation; (2018) ZIM-Erfolgsbeispiel: Exakt Montiert-sicher Verpackt-zufriedene Kunden, , AiF Projekt GmbH Jan; Grendel, H., Larek, R., Riedel, F., Wagner, J.C., Enabling manual assembly and integration of aerospace structures for Industry 4.0-methods (2017) New Production Technologies in Aerospace Industry: MIC Proceedings 2017, Hannover; Ziegler, J., (2015) Wearables im Industriellen Einsatz: Befähigung zu Mobiler IT-gestützter Arbeit Durch Verteilte Tragbare Benutzungsschnittstellen; Molzow-Voit, F., Quandt, M., Freitag, M., Spöttl, G., (2016) Robotik in der Logistik: Qualifizierung für Fachkräfte und Entscheider, , 1st ed. Wiesbaden: Springer Gabler; Michalos, G., ROBO-PARTNER: Seamless human-robot cooperation for intelligent, flexible and safe operations in the assembly factories of the future (2014) ScienceDirect, Procedia CIRP, (23), pp. 71-76; (2016), acatech, Ed., Innovationspotenziale der Mensch-Maschine-Interaktion. Munich: Herbert Utz Verlag GmbH; Bundesministerium für Arbeit und Soziales Abteilung Grundsatzfragen des Sozialstaats, der Arbeitswelt und der sozialen Marktwirtschaft (2017) WEISS BUCH Arbeiten 4.0: Arbeit Weiter Denken; Kunert, G., Pawletta, T., Generierung von Steuerungen für Gelenkarmroboter mit simulationsbasiertem Reinforcement-Learning (2018) 24. Symposium Simulationstechnik ASIM, 2018 (56); Vogel-Heuser, B., Bauernhansl, T., Hompelten, M., (2017) Handbuch Industrie 4.0: Produktion, , 2nd ed. Berlin: Springer-Verlag GmbH Deutschland; Ten Hompel, M., Henke, M., (2014) Logistik 4.0 in SpringerLink, Industrie 4.0 in Produktion, Automatisierung und Logistik: Anwendung, Technologien, Migration, pp. 615-624. , T. Bauernhansl, M. ten Hompel, and B. Vogel-Heuser, Eds., Wiesbaden: Springer Vieweg; Hackl, B., Wagner, M., Attmer, L., Baumann, D., (2017) New Work: Auf Dem Weg Zur Neuen Arbeitswelt: Management-Impulse, Praxisbeispiele, Studien, , Wiesbaden: Springer Gabler; Kuka, A.G., Hello Industrie 4.0-we Go Digital, , https://www.nebbiolo.tech/wp-content/uploads/KUKAIndustrie-4.0.pdf, weiskind.com I40/EN/01/0416, Apr. 2016 Accessed on: Jul. 19 2018; Dostal, W., Kamp, A.-W., Lahner, M., Seessle, W.P., (1982) Flexible Fertigungssysteme und Arbeitsplatzstrukturen, , Stuttgart: W. Kohlhammer GmbH; Larek, R., Grendel, H., Wagner, J.C., Riedel, F., Industry 4.0 in manual assembly processes- A concept for real time production steering and decision making ScienceDirect, Procedia CIRP 12th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 2018. Unpublished; Wagner, J.C., Larek, R., Nüchter, A., Der maximalnetzplan als neuinterpretation der netzplantechnik Proceedings 11. Wismarer Wirtschaftsinformatiktage., Wismar, pp. 123-135; Kunert, G., Pawletta, T., Generating of task-based controls for joint-arm robots with simulation-based reinforcement learning (2018) SNE, 28 (4), pp. 149-156},
sponsors={Hochschule Wismar, Faculty of Engineering},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728104232},
language={English},
abbrev_source_title={Int. Interdiscip. PhD Workshop, IIPhDW},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kuhnle201933,
author={Kuhnle, A. and Jakubik, J. and Lanza, G.},
title={Reinforcement learning for opportunistic maintenance optimization},
journal={Production Engineering},
year={2019},
volume={13},
number={1},
pages={33-41},
doi={10.1007/s11740-018-0855-7},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055991618&doi=10.1007%2fs11740-018-0855-7&partnerID=40&md5=c710a9a712200b8a697ba8f739a0533d},
affiliation={wbk, Institute of Production Science Karlsruhe Institute of Technology (KIT), Kaiserstrasse 12, Karlsruhe, 76131, Germany},
abstract={Intelligent systems, that support the maintenance of production resources, offer real-time data-based approaches to optimize the maintenance effort and to reduce the usage of resources within production systems. However, unused potentials remain regarding maintenance schedules with minimal opportunity costs of the measures taken. This work provides a novel, machine-learning-based approach for the exploitation of these remaining optimization opportunities as an exemplary extension of the current state of the art. The determination of an optimal maintenance schedule for parallel working machines, is based on the data of a production system. The main result of this work is the performance of the implemented reinforcement learning algorithms, both in terms of downtime reduction, which increases the production output, and in terms of reducing maintenance costs compared to existing maintenance strategies. Hence, this work provides a holistic approach to the optimization of maintenance strategies and gives further evidence of a meaningful applicability of reinforcement learning algorithms in manufacturing processes. © 2018, German Academic Society for Production Engineering (WGP).},
author_keywords={Multi-agent-systems;  Opportunistic maintenance;  Opportunity cost reduction;  Production planning and control;  Proximal policy optimization;  Reinforcement learning},
keywords={Cost reduction;  Intelligent agents;  Intelligent systems;  Maintenance;  Multi agent systems;  Production control;  Real time systems;  Reinforcement learning, Maintenance schedules;  Maintenance strategies;  Manufacturing process;  Opportunistic maintenance;  Opportunity costs;  Policy optimization;  Production planning and control;  Production resources, Learning algorithms},
funding_details={Bundesministerium für Bildung und ForschungBundesministerium für Bildung und Forschung, BMBF, 02K16C082},
funding_text 1={aTotal sum of costs of maintenance and repairs in units of cost of a repair. The cost of a maintenance measure is assumed to be 30% of the cost of a repair, as a maintenance execution linearly takes 30% the time of a repair Acknowledgements We extend our sincere thanks to the German Federal Ministry of Education and Research (BMBF) for supporting this research project 02K16C082 Produktionsbezogene Dienstleistungssys-teme auf Basis von Big-Data-Analysen (ProData).},
references={Wuest, T., Machine learning in manufacturing: advantages, challenges, and applications (2016) Prod Manuf Res, 4, pp. 23-45; Colledani, M., Magnanini, M.C., Tolio, T., Impact of opportunistic maintenance on manufacturing system performance (2018) CIRP Ann, 67 (1), pp. 499-502; Hashemian, H.M., Bean Wendell, C., State-of-the-art predictive maintenance techniques (2011) IEEE Trans Instrum Meas, 60 (10), pp. 3480-3492; Lindström, J., Larsson, H., Jonsson, M., Leyon, E., Towards intelligent and sustainable production: combining and integratingonline predictive maintenance and continuous quality control (2017) Procedia CIRP, 63, pp. 443-448; Yang, L., Opportunistic maintenance of production systems subject to random wait time and multiple control limits (2018) J Manuf Syst, 47, pp. 12-34; Stricker, N., Reinforcement learning for adaptive order dispatching in the semiconductor industry (2018) CIRP Ann, 67 (1), pp. 511-514; Wang, X., Reinforcement learning based predictive maintenance for a machine with multiple deteriorating yield levels (2014) J Comput Inf Syst, 10 (1), pp. 9-19; Wang, J., Multi-agent reinforcement learning based maintenance policy for a resource constrained flow line system (2016) J Intell Manuf, 27 (2), pp. 325-333; Sutton, R.S., Barto, A.G., (2017) Reinforcement learning: an introduction, , MIT Press, Cambridge; Crites, R.H., Barto, A.G., Improving elevator performance using reinforcement-learning (1995) Adv Neural Inf Process Syst, 8, pp. 1017-1023; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., Playing atari with deep reinforcement learning (2013) Corr; Williams, R.J., Simple statistic gradient-following algorithms for connectionist reinforcement learning (1992) Mach Learn, 8, pp. 229-256; Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P., Trust region policy optimization (2015) Proceedings of the 31St International Conference on Machine Learning, 37, pp. 1889-1897; Schulman, J., Proximal policy optimization algorithms (2017) Adv Neural Inf Process Syst, 8, pp. 1017-1023; Schijve, J., (2009) Fatigue of structures and materials, pp. 15-21. , Springer, Amsterdam; Xie, M., Lai, C.D., Reliability analysis using an additive Weibull model with bathtub-shaped failure rate function (1996) Reliab Eng Syst Saf, 52 (1), pp. 87-93},
correspondence_address1={Kuhnle, A.; wbk, Kaiserstrasse 12, Germany; email: andreas.kuhnle@kit.edu},
publisher={Springer Verlag},
issn={09446524},
language={English},
abbrev_source_title={Prod. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shin2019556,
author={Shin, J. and Lee, J.H.},
title={Multi-timescale, multi-period decision-making model development by combining reinforcement learning and mathematical programming},
journal={Computers and Chemical Engineering},
year={2019},
volume={121},
pages={556-573},
doi={10.1016/j.compchemeng.2018.11.020},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057534192&doi=10.1016%2fj.compchemeng.2018.11.020&partnerID=40&md5=ca9739f9937c5aa51d9aa0743f50cff5},
affiliation={Chemical and Biomolecular Engineering Department, Korea Advaced Institute of Science and Technology, Daejeon, South Korea},
abstract={This study focuses on the linkage between decision layers that have different time scales. The resulting expansion of the boundary of decision-making process can provide more robust and flexible management and operation strategies by resolving inconsistencies between different levels. For this, we develop a multi-timescale decision-making model that combines Markov decision process (MDP) and mathematical programming (MP) in a complementary way and introduce a computationally tractable solution algorithm based on reinforcement learning (RL) to solve the MP-embedded MDP problem. To support the integration of the decision hierarchy, a data-driven uncertainty prediction model is suggested which is valid across all time scales considered. A practical example of refinery procurement and production planning is presented to illustrate the proposed method, along with numerical results of a benchmark case study. © 2018 Elsevier Ltd},
author_keywords={Decision under uncertainty;  Markov decision process;  Mathematical programming;  Multi-timescale decision making;  Reinforcement learning},
keywords={Behavioral research;  Markov processes;  Mathematical programming;  Numerical methods;  Production control;  Reinforcement learning, Benchmark case studies;  Decision making models;  Decision making process;  Decision under uncertainty;  Different time scale;  Markov Decision Processes;  Multi-timescale;  Production planning IS, Decision making},
references={Amaro, A., Barbosa-Póvoa, A.P.F., Planning and scheduling of industrial supply chains with reverse flows: a real pharmaceutical case study (2008) Comput. Chem. Eng., 32 (11), pp. 2606-2625; Barro, D., Canestrelli, E., Combining stochastic programming and optimal control to decompose multistage stochastic optimization problems (2016) OR Spectr., 38 (3), pp. 711-742; Bassett, M.H., Pekny, J.F., Reklaitis, G.V., Decomposition techniques for the solution of large‐scale scheduling problems (1996) AIChE J., 42 (12), pp. 3373-3387; Bengtsson, J., Nonås, S.-L., Refinery planning and scheduling: an overview (2010) Energy, Natural Resources and Environmental Economics, pp. 115-130. , Springer; Birge, J.R., Louveaux, F., Introduction to Stochastic Programming (2011), Springer Science & Business Media; Bose, S., Pekny, J., A model predictive framework for planning and scheduling problems: a case study of consumer goods supply chain (2000) Comput. Chem. Eng., 24 (2-7), pp. 329-335; Bradtke, S.J., Barto, A.G., Linear least-squares algorithms for temporal difference learning (1996) Mach. Learn., 22 (1-3), pp. 33-57; Braun, M.W., Rivera, D.E., Flores, M., Carlyle, W.M., Kempf, K.G., A model predictive control framework for robust management of multi-product, multi-echelon demand networks (2003) Annu. Rev. Control, 27 (2), pp. 229-245; Chen, Y.-H., Lu, S.-Y., Chang, Y.-R., Lee, T.-T., Hu, M.-C., Economic analysis and optimal energy management models for microgrid systems: a case study in Taiwan (2013) Appl. Energy, 103, pp. 145-154; Cheng, L., Subrahmanian, E., Westerberg, A., Design and planning under uncertainty: issues on problem formulation and solution (2003) Comput. Chem. Eng., 27 (6), pp. 781-801; Cheng, L., Subrahmanian, E., Westerberg, A.W., A comparison of optimal control and stochastic programming from a formulation and computation perspective (2004) Comput. Chem. Eng., 29 (1), pp. 149-164; Dann, C., Neumann, G., Peters, J., Policy evaluation with temporal differences: a survey and comparison (2014) J. Mach. Learn. Res., 15 (1), pp. 809-883; Dogan, M.E., Grossmann, I.E., A decomposition method for the simultaneous planning and scheduling of single-stage continuous multiproduct plants (2006) Ind. Eng. Chem. Res., 45 (1), pp. 299-315; Dunn, S., Holloway, J., The pricing of crude oil (2012) RBA Bull., pp. 65-74; Dupačová, J., Sladký, K., Comparison of multistage stochastic programs with recourse and stochastic dynamic programs with discrete time (2002) ZAMM J. Appl. Math. Mech./Zeitschrift für Angewandte Mathematik und Mechanik, 82 (11-12), pp. 753-765; Favennec, J., Petroleum refining V5. Refinery operation and management (2001) Technip; Fisher, M., Ramdas, K., Zheng, Y.-S., Ending inventory valuation in multiperiod production scheduling (2001) Manag. Sci., 47 (5), pp. 679-692; Grossmann, I.E., Enterprise‐wide optimization: a new frontier in process systems engineering (2005) AIChE J., 51 (7), pp. 1846-1857; Grossmann, I.E., Advances in mathematical programming models for enterprise-wide optimization (2012) Comput. Chem. Eng., 47, pp. 2-18; Grossmann, I.E., Guillén-Gosálbez, G., Scope for the application of mathematical programming techniques in the synthesis and planning of sustainable processes (2010) Comput. Chem. Eng., 34 (9), pp. 1365-1376; Grunow, M., Günther, H.-O., Lehmann, M., Campaign planning for multi-stage batch processes in the chemical industry (2002) OR Spectr., 24 (3), pp. 281-314; Hawkes, A., Leach, M., Modelling high level system design and unit commitment for a microgrid (2009) Appl. Energy, 86 (7), pp. 1253-1265; Honkomp, S., Mockus, L., Reklaitis, G., A framework for schedule evaluation with processing uncertainty (1999) Comput. Chem. Eng., 23 (4-5), pp. 595-609; Konicz, A.K., Pisinger, D., Rasmussen, K.M., Steffensen, M., A combined stochastic programming and optimal control approach to personal finance and pensions (2015) OR Spectr., 37 (3), pp. 583-616; Lee, J.H., Energy supply planning and supply chain optimization under uncertainty (2014) J. Process Control, 24 (2), pp. 323-331; Lin, X., Floudas, C.A., Modi, S., Juhasz, N.M., Continuous-time optimization approach for medium-range production scheduling of a multiproduct batch plant (2002) Ind. Eng. Chem. Res., 41 (16), pp. 3884-3906; Maravelias, C.T., Sung, C., Integration of production planning and scheduling: overview, challenges and opportunities (2009) Comput. Chem. Eng., 33 (12), pp. 1919-1930; McDonald, C., Synthesizing enterprise-wide optimization with global information technologies: harmony or discord (1998) Foundations of Computer Aided Process Operations, pp. 62-74. , J. Pekny G. Blau; McKAY, K.N., Safayeni, F.R., Buzacott, J.A., A review of hierarchical production planning and its applicability for modern manufacturing (1995) Prod. Plann. Control, 6 (5), pp. 384-394; Mestan, E., Türkay, M., Arkun, Y., Optimization of operations in supply chain systems using hybrid systems approach and model predictive control (2006) Ind. Eng. Chem. Res., 45 (19), pp. 6493-6503; Papageorgiou, L.G., Pantelides, C.C., Optimal campaign planning/scheduling of multipurpose batch/semicontinuous plants. 1. Mathematical formulation (1996) Ind. Eng. Chem. Res., 35 (2), pp. 488-509; Perea-Lopez, E., Ydstie, B.E., Grossmann, I.E., A model predictive control strategy for supply chain optimization (2003) Comput. Chem. Eng., 27 (8), pp. 1201-1218; Powell, W.B., (2007) Approximate Dynamic Programming: Solving the Curses of Dimensionality, 703. , John Wiley & Sons; Powell, W.B., AI, OR and Control Theory: A Rosetta Stone for Stochastic Optimization (2012), Princeton University; Powell, W.B., Clearing the jungle of stochastic optimization (2014) Bridging Data and Decisions, pp. 109-137. , Informs; Powell, W.B., Meisel, S., Tutorial on stochastic optimization in energy—Part I: modeling and policies (2016) IEEE Trans. Power Syst., 31 (2), pp. 1459-1467; Pratikakis, N.E., Multistage Decisions and Risk in Markov Decision Processes: Towards Effective Approximate Dynamic Programming Architectures (2009), Georgia Institute of Technology; Puterman, M.L., Markov Decision Processes: Discrete Stochastic Dynamic Programming (2014), John Wiley & Sons; Ren, H., Gao, W., A MILP model for integrated plan and evaluation of distributed energy systems (2010) Appl. Energy, 87 (3), pp. 1001-1014; Sahinidis, N.V., Optimization under uncertainty: state-of-the-art and opportunities (2004) Comput. Chem. Eng., 28 (6), pp. 971-983; Sand, G., Engell, S., Modeling and solving real-time scheduling problems by stochastic integer programming (2004) Comput. Chem. Eng., 28 (6-7), pp. 1087-1103; Shobrys, D.E., White, D.C., Planning, scheduling and control systems: why cannot they work together (2002) Comput. Chem. Eng., 26 (2), pp. 149-160; Stefansson, H., Shah, N., Jensson, P., Multiscale planning and scheduling in the secondary pharmaceutical industry (2006) AIChE J., 52 (12), pp. 4133-4149; Sutton, R.S., Barto, A.G., (1998) Reinforcement Learning: An Introduction, 1. , MIT press Cambridge; van den Heever, S.A., Grossmann, I.E., A strategy for the integration of production planning and reactive scheduling in the optimization of a hydrogen supply network (2003) Comput. Chem. Engineering, 27 (12), pp. 1813-1839; Wang, H., Recursive estimation and time-series analysis (1986) Acoust. Speech Signal Process. IEEE Trans., 34 (6). , 1678-1678; Wu, D., Ierapetritou, M., Hierarchical approach for production planning and scheduling under uncertainty (2007) Chem. Eng. Process., 46 (11), pp. 1129-1140; Yan, H.-S., Xia, Q.-F., Zhu, M.-R., Liu, X.-L., Guo, Z.-M., Integrated production planning and scheduling on automobile assembly lines (2003) IIE Trans., 35 (8), pp. 711-725},
correspondence_address1={Lee, J.H.; Chemical and Biomolecular Engineering Department, South Korea; email: jayhlee@kaist.ac.kr},
publisher={Elsevier Ltd},
issn={00981354},
coden={CCEND},
language={English},
abbrev_source_title={Comput. Chem. Eng.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Shi2019,
author={Shi, Y. and Xiang, Y. and Jin, T.},
title={Structured maintenance policies for deteriorating transportation infrastructures: Combination of maintenance types},
journal={Proceedings - Annual Reliability and Maintainability Symposium},
year={2019},
volume={2019-January},
doi={10.1109/RAMS.2019.8769227},
art_number={8769227},
note={cited By 2; Conference of 2019 Annual Reliability and Maintainability Symposium, RAMS 2019 ; Conference Date: 28 January 2019 Through 31 January 2019;  Conference Code:149921},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069959375&doi=10.1109%2fRAMS.2019.8769227&partnerID=40&md5=8c0a143d9893fa639070bee57e003b9a},
affiliation={Lamar University, 4400MLK Blvd, Beaumont, TX  77710, United States; Texas State University, 601 University Drive, San Marcos, TX  78666, United States},
abstract={Maintenance planning for deteriorating transportation infrastructures is quite challenging because multiple maintenance actions with complex effects are usually required. This paper proposes a novel analytical model for deteriorating infrastructures with consideration of multiple types of preventive maintenance actions which have complex effects. Upon each inspection, a decision maker needs to decide whether preventive maintenance is needed, and what type of preventive maintenance action is appropriate if it is desirable. We formulate the maintenance optimization problem as a finite-horizon Markov decision process and investigate the structural properties of the optimal maintenance policies by minimizing the expected cost-To-go function. Computational study is provided to demonstrate the monotonically non-decreasing property of the optimal policies in condition and age using real-world pavement deterioration data. The monotone structure of the optimal policies is appealing in practice, because it can save the computational efforts and facilitates implementation. © 2019 IEEE.},
author_keywords={Complex maintenance effects;  Deteriorating transportation infrastructures;  Markov decision process},
keywords={Decision making;  Deterioration;  Maintainability;  Markov processes;  Reinforcement learning;  Structural optimization, Computational studies;  Finite-horizon Markov decision process;  Maintenance effect;  Maintenance optimization;  Markov Decision Processes;  Optimal maintenance policies;  Pavement deterioration;  Transportation infrastructures, Preventive maintenance},
funding_details={1728257},
funding_text 1={ACKNOWLEDGEMENTS This study is partially supported by the U.S. National Science Foundation through Award 1728257.},
references={(2017) ASCE, , 2017 infrastructure report card; (2013) ASCE, , 2013 Report Card for American Infrastructure-Bridges; Ann Johnson, P., Best practices handbook on asphalt pavement maintenance (2000) Minnesota Technology Transfer Center, Minnesota; Liu, M., Frangopol, D.M., Multiobjective maintenance planning optimization for deteriorating bridges considering condition, safety, and life-cycle cost (2005) Journal of Structural Engineering, 131, pp. 833-842; Neves, L.C., Frangopol, D.M., Condition, safety and cost profiles for deteriorating structures with emphasis on bridges (2005) Reliability Engineering & System Safety, 89, pp. 185-198; Neves, L.A., Frangopol, D.M., Cruz, P.J., Probabilistic lifetime-oriented multiobjective optimization of bridge maintenance: Single maintenance type (2006) Journal of Structural Engineering, 132, pp. 991-1005; Neves, L.A., Frangopol, D.M., Petcherdchoo, A., Probabilistic lifetime-oriented multiobjective optimization of bridge maintenance: Combination of maintenance types (2006) Journal of Structural Engineering, 132, pp. 1821-1834; Ouyang, Y., Madanat, S., Optimal scheduling of rehabilitation activities for multiple pavement facilities: Exact and approximate solutions (2004) Transportation Research Part A: Policy and Practice, 38, pp. 347-365; Kijima, M., Some results for repairable systems with general repair (1989) Journal of Applied Probability, 26, pp. 89-102; Kurt, M., Kharoufeh, J.P., Monotone optimal replacement policies for a Markovian deteriorating system in a controllable environment (2010) Operations Research Letters, 38, pp. 273-279; Chen, N., Ye, Z.-S., Xiang, Y., Zhang, L., Conditionbased maintenance using the inverse Gaussian degradation model (2015) European Journal of Operational Research, 243, pp. 190-199; Office, F.D.O.T., (2015) 2015 Flexible Pavement Condition Survey Handbook, , ed; FHWA 2013 status of the Nations highways, bridges, and transit: Conditions & performance (2013) Rep.To Congress; Shaked, M., Shanthikumar, J.G., (2007) Stochastic Orders, , Springer Science & Business Media},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={0149144X},
isbn={9781538665541},
language={English},
abbrev_source_title={Proc. Annu. Reliab. Maintainability Symp.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kuhnle2019391,
author={Kuhnle, A. and Röhrig, N. and Lanza, G.},
title={Autonomous order dispatching in the semiconductor industry using reinforcement learning},
journal={Procedia CIRP},
year={2019},
volume={79},
pages={391-396},
doi={10.1016/j.procir.2019.02.101},
note={cited By 19; Conference of 12th CIRP Conference on Intelligent Computation in Manufacturing Engineering, CIRP ICME 2018 ; Conference Date: 18 July 2018 Through 20 July 2018;  Conference Code:147561},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065424368&doi=10.1016%2fj.procir.2019.02.101&partnerID=40&md5=bdb03f175ad9ad145b60b5428ab2eb11},
affiliation={Wbk Institute of Production Science, Karlsruhe Institute of Technology (KIT), Kaiserstr. 12, Karlsruhe, 76137, Germany},
abstract={Cyber Physical Production Systems (CPPS) provide a huge amount of data. Simultaneously, operational decisions are getting ever more complex due to smaller batch sizes, a larger product variety and complex processes in production systems. Production engineers struggle to utilize the recorded data to optimize production processes effectively because of a rising level of complexity. This paper shows the successful implementation of an autonomous order dispatching system that is based on a Reinforcement Learning (RL) algorithm. The real-world use case in the semiconductor industry is a highly suitable example of a cyber physical and digitized production system. © 2019 The Author(s).},
author_keywords={Production planning;  Reinforcement learning;  Semiconductor industry},
keywords={Cyber Physical System;  Machine learning;  Production control;  Semiconductor device manufacture, Complex Processes;  Dispatching systems;  Operational decisions;  Product variety;  Production Planning;  Production process;  Production system;  Semiconductor industry, Reinforcement learning},
funding_details={Bundesministerium für Bildung und ForschungBundesministerium für Bildung und Forschung, BMBF},
funding_text 1={We extend our sincere thanks to the German Federal Ministry of Education and Research (BMBF) for supporting},
references={Mönch, L., Fowler, J.W., Mason, S.J., (2013) Production Planning and Control for Semiconductor Wafer Fabrication Facilities, , 1st ed. Springer New York; Monostori, L., Csáji, B.C., Kádár, B., Adaptation and Learning in Distributed Production Control (2004) CIRP Annals, 53, pp. 349-352; Csáji, B.C., Monostori, L., Kádár, B., Reinforcement learning in a distributed market-based production control system (2006) Advanced Engineering Informatics, 20, pp. 279-288; Waschneck, B., Altenmüller, T., Bauernhansl, T., Kyek, A., Production Scheduling in Complex Job Shops from an Industry 4.0 Perspective (2016) CEUR Workshop Proceedings, 1793, pp. 12-24; Monostori, L., Váncza, J., Kumara, S.R.T., Agent-Based Systems for Manufacturing (2006) CIRP Annals, 55, pp. 697-720; Lawler, E.L., Lenstra, J.K., Kan, A.H.R., Shmoys, D.B., Sequencing and Scheduling: Algorithms and Complexity (1993) Handbooks in Operations Research and Management Science, 4, pp. 445-522; Wang, Y.C., Usher, J.M., Application of reinforcement learning for agent-based production scheduling (2005) Engineering Applications of Artificial Intelligence, 18, pp. 73-82; Luck, M., McBurney, P., (2005) Agent Technology Roadmap, , 1st ed. AgentLink Southampton; Russel, S., Norvig, P., (2016) Artificial Intelligence, , 3rd ed. Pearson Education Limited Malaysia; Sutton, R.S., Barto, A.G., (1998) Reinforcement Learning: An Introduction, , 1st ed. MIT press Cambridge; Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O., (2017) Proximal Policy Optimization Algorithms, , arXiv preprint:1707.06347; Schulman, J., Levine, S., Moritz, P., Jordan, M.I., Abbeel, P., Trust Region Policy Optimization (2015) International Conference on Machine Learning, pp. 1889-1897; Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., (2013) Playing Atari with Deep Reinforcement Learning, , arXiv preprint:1312.5602; Kormushev, P., Calinon, S., Caldwell, D.G., Robot motor skill coordination with EM-based Reinforcement Learning (2010) Intelligent Robots and Systems, 3232-3237; Philip, T., Michael, B., Antonie, V., Kathleen, J., Application of the Actor-Critic Architecture to Functional Electrical Stimulation Control of a Human Arm (2009) Proc Innov Appl Artif Intell Conf., 165-172; Günther, J., Pilarski, P.M., Helfrich, G., Shen, H., Diepold, K., Intelligent laser welding through representation (2016) Prediction, and Control Learning. Mechatronics, 34, pp. 1-11; Reinforcement, S.F., (2000) Learning Zur Dispositiven Auftragssteuerung in der Varianten-Reihenproduktion, , 1st ed. Herbert Utz Verlag},
correspondence_address1={Kuhnle, A.; Wbk Institute of Production Science, Kaiserstr. 12, Germany; email: andreas.kuhnle@kit.edu},
editor={Teti R.},
sponsors={Fraunhofer Joint Laboratory of Excellence on Advanced Production Technology (Fh-J_LEAPT Naples); International Academy for Production Engineering (CIRP)},
publisher={Elsevier B.V.},
issn={22128271},
language={English},
abbrev_source_title={Procedia CIRP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schneckenreither2019545,
author={Schneckenreither, M. and Haeussler, S.},
title={Reinforcement learning methods for operations research applications: The order release problem},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11331 LNCS},
pages={545-559},
doi={10.1007/978-3-030-13709-0_46},
note={cited By 10; Conference of 4th International Conference on Machine Learning, Optimization, and Data Science, LOD 2018 ; Conference Date: 13 September 2018 Through 16 September 2018;  Conference Code:223779},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063579604&doi=10.1007%2f978-3-030-13709-0_46&partnerID=40&md5=ea5ee95b3b1a2819dea2a23a0d0d8af8},
affiliation={Department of Information Systems, Production and Logistics Management, University of Innsbruck, Innsbruck, Austria},
abstract={An important goal in Manufacturing Planning and Control systems is to achieve short and predictable flow times, especially where high flexibility in meeting customer demand is required. Besides achieving short flow times, one should also maintain high output and due-date performance. One approach to address this problem is the use of an order release mechanism which collects all incoming orders in an order-pool and thereafter determines when to release the orders to the shop-floor. A major disadvantage of traditional order release mechanisms is their inability to consider the nonlinear relationship between resource utilization and flow times which is well known from practice and queuing theory. Therefore, we propose a novel adaptive order release mechanism which utilizes deep reinforcement learning to set release times of the orders and provide several techniques for challenging operations research problems with reinforcement learning. We use a simulation model of a two-stage flow-shop and show that our approach outperforms well-known order release mechanism. © Springer Nature Switzerland AG 2019.},
author_keywords={Machine learning;  Operations research;  Order release;  Production planning;  Reinforcement learning},
keywords={Deep learning;  Learning systems;  Machine learning;  Operations research;  Production control;  Queueing theory, Manufacturing planning and control;  Non-linear relationships;  Order release;  Order release mechanisms;  Production Planning;  Reinforcement learning method;  Research applications;  Resource utilizations, Reinforcement learning},
references={Ackerman, S., Even-flow a scheduling method for reducing lateness in job shops (1963) Manag. Technol., 3, pp. 20-32; Akyol, D.E., Bayhan, G.M., A review on evolution of production scheduling with neural networks (2007) Comput. Ind. Eng., 53 (1), pp. 95-122. , http://www.sciencedirect.com/science/article/pii/S0360835207000666; Aytug, H., Bhattacharyya, S., Koehler, G.J., Snowdon, J.L., A review of machine learning in scheduling (1994) IEEE Trans. Eng. Manag., 41, pp. 165-171; Baykasoglu, A., Gocken, M., A simulation based approach to analyse the effects of job release on the performance of a multi-stage job-shop with processing flexibility (2011) Int. J. Prod. Res., 49 (2), pp. 585-610; Bechte, W., Theory and practice of load-oriented manufacturing control (1988) Int. J. Prod. Res., 26 (3), pp. 375-395; Bechte, W., Load-oriented manufacturing control just-in-time production for job shops (1994) Prod. Plan. Control, 5 (3), pp. 292-307; Bertrand, J.W.M., Wortmann, J.C., (1981) Production Control and Information Systems for Component Manufacturing Shops, , Elsevier Science Inc., New York; Bertrand, J., Wortmann, J., Wijngaard, J., (1990) Production Control: A Structural and Design Oriented Approach, , Elsevier, Amsterdam; Conover, W., (1999) Practical Nonparametric Statistics. Wiley Series in Probability and Statistics, , 3rd edn. Wiley, New York; Enns, S., Suwanruji, P., Work load responsive adjustment of planned lead times (2004) J. Manuf. Technol. Manag., 15 (1), pp. 90-100; Gelders, L., van Wassenhove, L.N., Hierarchical integration in production planning: Theory and practice (1982) J. Oper. Manag., 3 (1), pp. 27-35; Hendry, L., Kingsman, B., Production planning systems and their applicability to make-to-order companies (1989) Eur. J. Oper. Res., 40 (1), pp. 1-15. , http://www.sciencedirect.com/science/article/pii/037722178990266X; Hendry, L., Kingsman, B., A decision support system for job release in make-to-order companies (1991) Int. J. Ope. Prod. Manag., 11 (6), pp. 6-16; Hoyt, J., Dynamic lead times that fit today’s dynamic planning (Quoat lead times) (1978) Prod. Inventory Manag., 19 (1), pp. 63-71; Hsu, S.Y., Sha, D.Y., Due date assignment using artificial neural networks under different shop floor control strategies (2004) Int. J. Prod. Res., 42 (9), pp. 1727-1745. , https://doi.org/10.1080/00207540310001624375; Karaoglan, A.D., Karademir, O., Flow time and product cost estimation by using an artificial neural network (ANN): A case study for transformer orders (2017) Eng. Econ., 62 (3), pp. 272-292. , https://doi.org/10.1080/0013791X.2016.1185808; Knollmann, M., Windt, K., Control-theoretic analysis of the lead time syndrome and its impact on the logistic target achievement (2013) Procedia CIRP, 7, pp. 97-102; Law, A.M., Kelton, W.D., (2000) Simulation Modeling & Analysis, 3Rd Edn, , McGraw-Hill Inc., New York; Lee, C.Y., Piramuthu, S., Tsai, Y.K., Job shop scheduling with a genetic algorithm and machine learning (1997) Int. J. Prod. Res., 35 (4), pp. 1171-1191. , https://doi.org/10.1080/002075497195605; Li, S., Li, Y., Liu, Y., Xu, Y., A GA-based NN approach for makespan estimation (2007) Appl. Math. Comput., 185 (2), pp. 1003-1014. , http://www.sciencedirect.com/science/article/pii/S0096300306008253, Special Issue on Intelligent Computing Theory and Methodology; Lillicrap, T.P., (2015) Continuous Control with Deep Reinforcement Learning; Lin, L.J., (1993) Reinforcement Learning for Robots Using Neural Networks. Technical Report, School of Computer Science, , Carnegie-Mellon University, Pittsburgh, PA; Mahadevan, S., Average reward reinforcement learning: Foundations, algorithms, and empirical results (1996) Mach. Learn., 22 (1), pp. 159-195. , https://doi.org/10.1007/BF00114727; Mather, H., Plossl, G.W., (1978) Priority Fixation versus Throughput Planning. Prod. Inventory Manag, 19, pp. 27-51; Melnyks, Order review release-research issues and perspectives (1989) Int. J. Prod. Res., 27 (7), pp. 1081-1096; Metan, G., Sabuncuoglu, I., Pierreval, H., Real time selection of scheduling rules and knowledge extraction via dynamically controlled data mining (2010) Int. J. Prod. Res., 48 (23), pp. 6909-6938. , https://doi.org/10.1080/00207540903307581; Mnih, V., Asynchronous methods for deep reinforcement learning (2016) International Conference on Machine Learning, pp. 1928-1937. , pp; Mnih, V., Human-level control through deep reinforcement learning (2015) Nature, 518 (7540), pp. 529-533; Molinder, A., Joint optimization of lot-sizes, safety stocks and safety lead times in a MRP system (1997) Int. J. Prod. Res., 35 (4), pp. 983-994; Pahl, J., Voß, S., Woodruff, D.L., Production planning with load dependent lead times: An update of research (2007) Ann. Oper. Res., 153 (1), pp. 297-345. , https://doi.org/10.1007/s10479-007-0173-5; Paternina-Arboleda, C.D., Das, T.K., Intelligent dynamic control policies for serial production lines (2001) IIE Trans, 33 (1), pp. 65-77. , https://doi.org/10.1080/07408170108936807; Patil, R., Using ensemble and metaheuristics learning principles with artificial neural networks to improve due date prediction performance (2008) Int. J. Prod. Res., 46 (21), pp. 6009-6027; Philipoom, P.R., Rees, L.P., Wiegmann, L., Using neural networks to determine internally-set due-date assignments for shop scheduling (1994) Decis. Sci., 25 (5-6), pp. 825-851. , http://dx.doi.org/10.1111/j.1540-5915.1994.tb01871.x; Raaymakers, W., Weijters, A., Makespan estimation in batch process industries: A comparison between regression analysis and neural networks (2003) Eur. J. Oper. Res., 145 (1), pp. 14-30. , http://www.sciencedirect.com/science/article/pii/S037722170200173X; Savell, D.V., Perez, R.A., Koh, S.W., Scheduling semiconductor wafer production: An expert system implementation (1989) IEEE Expert, 4 (3), pp. 9-15. , Fall; Schneeweiss, C., Distributed decision making–a unified approach (2003) Eur. J. Oper. Res., 150 (2), pp. 237-252; Selcuk, B., Fransoo, J.C., de Kok, A., The effect of updating lead times on the performance of hierarchical planning systems (2006) Int. J. Prod. Econ., 104 (2), pp. 427-440; Silver, D., (2017) Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm; Spearman, M.L., Woodruff, D.L., Hopp, W.J., CONWIP: A pull alternative to Kanban (1990) Int. J. Prod. Res., 28 (5), pp. 879-894. , http://www.tandfonline.com/doi/abs/10.1080/00207549008942761; Sutton, R.S., Barto, A.G., (1998) Reinforcement Learning: An Introduction, 1. , vol., MIT Press, Cambridge; Tatsiopoulos, I., Kingsman, B., Lead time management (1983) Eur. J. Oper. Res., 14 (4), pp. 351-358; Teo, C.C., Bhatnagar, R., Graves, S.C., An application of master schedule smoothing and planned lead time control (2012) Prod. Oper. Manag., 21 (2), pp. 211-223; Thuerer, M., Stevenson, M., Silva, C., Three decades of workload control research: A systematic review of the literature (2011) Int. J. Prod. Res., 49 (23), pp. 6905-6935; Thuerer, M., Stevenson, M., Silva, C., Land, M.J., Fredendall, L.D., Workload control and order release: A lean solution for make-to-order companies (2012) Prod. Oper. Manag., 21 (5), pp. 939-953; Tsitsiklis, J.N., van Roy, B., Analysis of temporal-difference learning with function approximation (1997) Advances in Neural Information Processing Systems, pp. 1075-1081. , pp; Watkins, C.J.C.H., Dayan, P., Q-learning (1992) Mach. Learn., 8 (3), pp. 279-292. , https://doi.org/10.1007/BF00992698; Wiendahl, H., (1995) Load-Oriented Manufacturing Control, 1St Edn, , https://doi.org/10.1007/978-3-642-57743-7.http://books.google.at/books-id=e66fmQEACAAJ, Springer, Berlin; Wuest, T., Weimer, D., Irgens, C., Thoben, K.D., Machine learning in manufacturing: Advantages, challenges, and applications (2016) Prod. Manuf. Res., 4 (1), pp. 23-45. , https://doi.org/10.1080/21693277.2016.1192517; Yano, C., Setting planning lead times in serial production systems with earliness costs (1987) Manag. Sci., 33 (1), pp. 95-106; Zhang, G.P., Avoiding pitfalls in neural network research (2007) IEEE Trans. Syst. Man Cybern. Part C (Appl. Rev.), 37 (1), pp. 3-16},
correspondence_address1={Schneckenreither, M.; Department of Information Systems, Austria; email: manuel.schneckenreither@uibk.ac.at},
editor={Nicosia G., Giuffrida G., Nicosia G., Pardalos P., Sciacca V., Umeton R.},
publisher={Springer Verlag},
issn={03029743},
isbn={9783030137083},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Stricker2018511,
author={Stricker, N. and Kuhnle, A. and Sturm, R. and Friess, S.},
title={Reinforcement learning for adaptive order dispatching in the semiconductor industry},
journal={CIRP Annals},
year={2018},
volume={67},
number={1},
pages={511-514},
doi={10.1016/j.cirp.2018.04.041},
note={cited By 61},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045954603&doi=10.1016%2fj.cirp.2018.04.041&partnerID=40&md5=d5f9375f302bdc283d7e370609b720f1},
affiliation={wbk Institute of Production Science, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Infineon Technologies AG, Regensburg, Germany},
abstract={The digitalization of production systems tends to provide a huge amount of data from heterogeneous sources. This is particularly true for the semiconductor industry wherein real time process monitoring is inherently required to achieve a high yield of good parts. An application of data-driven algorithms in production planning to enhance operational excellence for complex semiconductor production systems is currently missing. This paper shows the successful implementation of a reinforcement learning-based adaptive control system for order dispatching in the semiconductor industry. Furthermore, a performance comparison of the learning-based control system with the traditionally used rule-based system shows remarkable results. Since a strict rulebook does not bind the learning-based control system, a flexible adaption to changes in the environment can be achieved through a combination of online and offline learning. © 2018},
author_keywords={Artificial intelligence;  Production planning;  Semiconductor industry},
keywords={Artificial intelligence;  Planning;  Process monitoring;  Production control;  Reinforcement learning;  Semiconductor device manufacture, Data-driven algorithm;  Heterogeneous sources;  Operational excellence;  Performance comparison;  Production Planning;  Real-time process monitoring;  Semiconductor industry;  Semiconductor production, Adaptive control systems},
funding_details={Bundesministerium fÃ¼r Bildung und ForschungBundesministerium fÃ¼r Bildung und Forschung, BMBF, 02P14B161},
funding_text 1={We extend our sincere thanks to the German Federal Ministry of Education and Research (BMBF) for supporting this research project 02P14B161 “Empowerment and Implementation Strategies for Industry 4.0”.},
references={Oliff, H., Liu, Y., Towards Industry 4.0 Utilizing Data-Mining Techniques: A Case Study on Quality Improvement (2017) Procedia CIRP, 63, pp. 167-172; Henke, N., Bughin, J., Chui, M., Manyika, J., Saleh, T., Wiseman, B., Sethupathy, G., The Age of Analytics: Competing in a Data-Driven World (2016), McKinsey Global Institute; Waschneck, B., Altenmüller, T., Bauernhansl, T., Kyek, A., Production Scheduling in Complex Job Shops from an Industry 4.0 Perspective: A Review and Challenges in the Semiconductor Industry (2016) CEUR Workshop Proceedings, 1793, pp. 12-24; Moyne, J., Iskandar, J., Big Data Analytics for Smart Manufacturing: Case Studies in Semiconductor Manufacturing (2017) Processes, 5 (3), pp. 39-59; Schuh, G., Reuter, C., Prote, J.P., Brambring, F., Ays, J., Increasing Data Integrity for Improving Decision Making in Production Planning and Control (2017) CIRP Annals – Manufacturing Technology, 66 (1), pp. 425-428; Fordyce, K., Milne, R.J., Wang, C.-T., Zisgen, H., Modeling and Integration of Planning, Scheduling, and Equipment Configuration in Semiconductor Manufacturing Part I. Review of Successes and Opportunities (2015) International Journal of Industrial Engineering: Theory, Applications and Practice, 22 (5), pp. 575-600; Mönch, L., Fowler, J.W., Mason, S.J., Production Planning and Control for Semiconductor Wafer Fabrication Facilities: Modeling, Analysis, and System (2013), Springer; Freitag, M., Hildebrandt, T., Automatic Design of Scheduling Rules for Complex Manufacturing Systems by Multi-Objective Simulation-Based Optimization (2016) CIRP Annals – Manufacturing Technology, 65 (1), pp. 433-436; Uzsoy, R., Church, L.K., Ovacik, I.M., Hinchman, J., Performance Evaluation of Dispatching Rules for Semiconductor Testing Operations (1993) Journal of Electronics Manufacturing, 3 (2), pp. 95-105; Monostori, L., Váncza, J., Kumara, S., Agent-Based Systems for Manufacturing (2006) CIRP Annals – Manufacturing Technology, 55 (2), pp. 697-720; Monostori, L., Csáji, B.C., Kádár, B., Adaptation and Learning in Distributed Production Control (2004) CIRP Annals – Manufacturing Technology, 53 (1), pp. 349-352; Günther, J., Pilarski, P.M., Helfrich, G., Shen, H., Diepold, K., Intelligent Laser Welding Through Representation, Prediction, and Control Learning: An Architecture with Deep Neural Networks and Reinforcement Learning (2016) Mechatronics, 34, pp. 1-11; Wang, P., Gao, R.X., Yan, R., A Deep Learning-Based Approach to Material Removal Rate Prediction in Polishing (2017) CIRP Annals – Manufacturing Technology, 66 (1), pp. 429-432; Russell, S., Norvig, P., Artificial Intelligence: A Modern Approach (2009), Prentice Hall Press; Sutton, R.S., Barto, A.G., Reinforcement Learning: An Introduction (2012), MIT Press; Tsitsiklis, J.N., Asynchronous Stochastic Approximation and Q-Learning (1994) Machine Learning, 16 (3), pp. 185-202},
correspondence_address1={Stricker, N.; wbk Institute of Production Science, Germany; email: nicole.stricker@kit.edu},
publisher={Elsevier USA},
issn={00078506},
coden={CIRAA},
language={English},
abbrev_source_title={CIRP Ann},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xanthopoulos2017576,
author={Xanthopoulos, A.S. and Kiatipis, A. and Koulouriotis, D.E. and Stieger, S.},
title={Reinforcement Learning-Based and Parametric Production-Maintenance Control Policies for a Deteriorating Manufacturing System},
journal={IEEE Access},
year={2017},
volume={6},
pages={576-588},
doi={10.1109/ACCESS.2017.2771827},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035760391&doi=10.1109%2fACCESS.2017.2771827&partnerID=40&md5=b4fcc54b5e7e3e2eade0f7fcc980025c},
affiliation={Department of Production and Management Engineering, Democritus University of Thrace, Xanthi, 671 00, Greece; Fujitsu Technology Solutions GmbH, Munich, 80807, Germany},
abstract={The model of a stochastic production/inventory system that is subject to deterioration failures is developed and examined in this paper. Customer interarrival times are assumed to be random and backorders are allowed. The system experiences a number of deterioration stages before it ultimately fails and is rendered inoperable. Repair and maintenance activities restore the system to its initial and previous deterioration state, respectively. The duration of both repair and maintenance is assumed to be stochastic. We address the problem of minimizing the expected sum of two conflicting objective functions: The average inventory level and the average number of backorders. The solution to this problem consists of finding the optimal tradeoff between maintaining a high service level and carrying as low inventory as possible. The primary goal of this research is to obtain optimal or near-optimal joint production/maintenance control policies, by means of a novel reinforcement learning-based approach. Furthermore, we examine parametric production and maintenance policies that are often used in practical situations, namely, Kanban, (s, S), threshold-Type condition based maintenance and periodic maintenance. The proposed approach is compared with the parametric policies in an extensive series of simulation experiments and it is found to clearly outperform them in all cases. Based on the numerical results obtained by the experiments, the behavior of the parametric policies as well as the structure of the control policies derived by the Reinforcement Learning-based approach is investigated. © 2013 IEEE.},
author_keywords={intelligent manufacturing systems;  Inventory control;  preventive maintenance;  reinforcement learning},
keywords={Deterioration;  Economic and social effects;  Electronic mail;  Engineering education;  Inventory control;  Maintainability;  Maintenance;  Manufacture;  Optimization;  Random processes;  Reinforcement learning;  Repair;  Stochastic models;  Stochastic systems, Condition based maintenance;  Conflicting objectives;  Intelligent manufacturing system;  Learning (artificial intelligence);  Periodic maintenance;  Production facility;  Repair and maintenance;  Stochastic production, Preventive maintenance},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 642963},
funding_details={H2020 Marie Skłodowska-Curie ActionsH2020 Marie Skłodowska-Curie Actions, MSCA, H2020-MSCA-ITN-2014-642963},
funding_details={European CommissionEuropean Commission, EC},
funding_text 1={The work of A. Kiatipis and S. Stieger was supported by the BigStorage: Storage-Based Convergence Between HPC and Cloud to Handle Big Data project from the European Union through the Marie Skłodowska-Curie Actions framework under Grant H2020-MSCA-ITN-2014-642963.},
references={Eloy Ruiz-Castro, J.E., Preventive maintenance of a multi-state device subject to internal failure and damage due to external shocks (2014) IEEE Trans. Rel, 63 (2), pp. 646-660. , Jun; Cai, Y., Hasenbein, J.J., Kutanoglu, E., Liao, M., Single-machine multiple-recipe predictive maintenance (2013) Probab. Eng. Inf. Sci., 27 (2), pp. 209-235; Fallahnezhad, M.S., A finite horizon dynamic programming model for production and repair decisions (2014) Commun. Statist.-Theory Methods, 43 (15), pp. 3302-3313; Li, N., Chan, F.T.S., Chung, S.H., Tai, A.H., A stochastic productioninventory model in a two-state production system with inventory deterioration, rework process, and backordering (2017) IEEE Trans. Syst Cybern., 47 (6), pp. 916-926. , Jun; Kyriakidis, E.G., Equilibrium probabilities for a production-inventory system maintained by a control-limit policy (2016) Commun. Statist.-Theory Methods, 45 (1), pp. 194-200; Wolter, A., Helber, S., Simultaneous production and maintenance planning for a single capacitated resource facing both a dynamic demand and intensive wear and tear (2016) Central Eur. J. Oper. Res., 24 (3), pp. 489-513; Nahas, N., Buffer allocation and preventive maintenance optimization in unreliable production lines (2017) J. Intell. Manuf., 28 (1), pp. 85-93; Wang, X., Wang, H., Qi, C., Multi-Agent reinforcement learning based maintenance policy for a resource constrained flow line (2016) J. Intell. Manuf., 27 (3), pp. 325-333; Hajej, Z., Turki, S., Rezg, N., Modelling and analysis for sequentially optimising production and delivery activities taking into account product returns (2015) J. Prod. Res., 53 (15), pp. 4694-4719; Kader, B., Sofiene, D., Nidhal, R., Walid, E., Ecological and joint optimization of preventive maintenance and spare parts inventories for an optimal production plan (2015) IFAC-PapersOnLine, 48 (3), pp. 2139-2144; Liao, G.-L., Production and maintenance policies for an EPQ model with perfect repair, rework, free-repair warranty, and preventive maintenance (2016) IEEE Trans. Syst Cybern., 46 (8), pp. 1129-1139. , Aug; He, K., Maillart, L.M., Prokopyev, O.A., Scheduling preventive maintenance as a function of an imperfect inspection interval (2015) IEEE Trans. Rel, 64 (3), pp. 983-997. , Sep; Li, N., Chan, F.T.S., Chung, S.H., Tai, A.H., An EPQ model for deteriorating production system and items with rework (2015) Math. Problems Eng, 2015. , Art 957970; Jafary, B., Nagaraju, V., Fiondella, L., Impact of correlated component failure on preventive maintenance policies (2017) IEEE Trans. Rel, 66 (2), pp. 575-586. , Jun; Zhang, X., Zeng, J., Joint optimization of condition-based opportunistic maintenance and spare parts provisioning policy in multiunit systems (2017) Eur. J. Oper. Res., 262 (2), pp. 479-498; Zhao, S., Wang, L., Zheng, Y., Integrating production planning and maintenance: An iterative method (2014) Ind. Manage., 114 (2), pp. 162-182; Xanthopoulos, A.S., Koulouriotis, D.E., Botsaris, P.N., Single-stage Kanban system with deterioration failures and condition-based preventive maintenance (2015) Rel. Eng. Safety, 142, pp. 111-122. , Oct; Yao, X., Xie, X., Fu, M.C., Marcus, S.I., Optimal joint preventive maintenance and production policies (2005) Naval Res. Logistics, 52 (7), pp. 668-681; Chen, D., Trivedi, K.S., Closed-form analytical results for conditionbased maintenance (2002) Rel. Eng. Safety, 76 (1), pp. 43-51; Das, T.K., Sarkar, S., Optimal preventive maintenance in a production inventory system (1999) IIE Trans., 31 (1), pp. 537-551; Iravani, S.M.R., Duenyas, I., Integrated maintenance and production control of a deteriorating production system (2002) IIE Trans., 34 (5), pp. 423-435; Geraghty, J., Heavey, C., An investigation of the infiuence of coefficient of variation in the demand distribution on the performance of several lean production control strategies (2010) Int. J. Manuf. Technol. Manage., 20 (1-4), pp. 94-119; Axsäter, S., (2015) Inventory Control, , New York, NY, USA Springer; Xanthopoulos, A.S., Koulouriotis, D.E., Tourassis, V.D., Emiris, D.M., Intelligent controllers for bi-objective dynamic scheduling on a single machine with sequence-dependent setups (2013) Appl. Soft Comput., 13 (12), pp. 4704-4717; Sutton, R.S., Barto, A.G., (1998) Reinforcement Learning: An Introduction, 1. , Cambridge, MA, USA MIT Press; Xanthopoulos, A.S., Koulouriotis, D.E., Gasteratos, A., Ioannidis, S., Efficient priority rules for dynamic sequencing with sequence-dependent setups (2016) Int. J. Ind. Eng. Comput., 7 (3), pp. 367-384; Schwartz, A., A reinforcement learning method for maximizing undiscounted rewards (1993) Proc. 10th Int. Conf. Mach. Learn, pp. 298-305; Singh, S., Reinforcement learning algorithms for average-payoff Markovian decision processes (1994) Proc. 12th Nat. Conf. Artif. Intell, pp. 202-207; Gosavi, A., A reinforcement learning algorithm based on policy iteration for average reward: Empirical results with yield management and convergence analysis (2004) Mach. Learn., 55 (1), pp. 5-29},
correspondence_address1={Kiatipis, A.; Fujitsu Technology Solutions GmbHGermany; email: athanasios.kiatipis@ts.fujitsu.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Fonseca-Reyna2017281,
author={Fonseca-Reyna, Y.C. and Martínez-Jiménez, Y. and Nowé, A.},
title={Q-learning algorithm performance for m-machine, n-jobs flow shop scheduling problems to minimize makespan},
journal={Investigacion Operacional},
year={2017},
volume={38},
number={3},
pages={281-290},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019621318&partnerID=40&md5=3bdcfdcc0b059e0b389e25e087ddaed5},
affiliation={Universidad de Granma, Bayamo, Granma, Cuba; Universidad Central de Las Villas, Santa Clara, Villa Clara, Cuba; Vrije Universiteit Brussel, Brussel, Belgium},
abstract={Flow Shop Scheduling Problems circumscribes an important class of sequencing problems in the field of production planning. The problem considered here is to find a permutation of jobs to be sequentially processed on a number of machines under the restriction that the processing of each job has to be continuous with respect to the objective of minimizing the completion time of all jobs, known in literature as makespan or Cmax. This problem is as NP-hard, it is typical of combinatorial optimization and can be found in manufacturing environments, where there are conventional machines-tools and different types of pieces which share the same route. The following research presents a Reinforcement Learning algorithm known as Q-Learning to solve problems of the Flow Shop category. This algorithm is based on learning an action-value function that gives the expected utility of taking a given action in a given state where an agent is associated to each of the resources. To validate the quality of the solutions, test cases of the specialized literature are used and the results obtained are compared with the reported optimal results.},
author_keywords={Flow-shop;  Makespan;  Optimization;  Q-learning;  Scheduling},
references={Álvarez, M., Toro, E., Gallego, R., Simulated annealing heuristic for flow shop scheduling problems (2008) Scientia et Technica, 14, pp. 159-164; Ancâu, M., On solving flow shop scheduling problems (2012) Proceedings of the Romanian Academy, 13, pp. 71-79; Anurag, A., Selcuk, C., Eryarsoy, E., Improvement heuristic for the flow-shop scheduling problem: An adaptive-learning approach (2006) European Journal of Operational Research, 169, pp. 801-815; Beasley, J.E., (1990) OR-Library, , http://people.brunel.ac.uk/~mastjjb/jeb/info.html, Consulted January 14, 2014; Betul, Y., Mehmet Mutlu, Y., Ant colony optimization for multi-objective flow shop scheduling problem (2008) Computers & Industrial Engineering, 54, pp. 411-420; Betul, Y., Mehmet Mutlu, Y., A multi-objective ant colony system algorithm for flow shop scheduling problem (2010) Expert Systems with Applications, 37, pp. 1361-1368; Brucker, P., (2007) Scheduling Algorithms, , Springer-Verlag, Berlin; Cicková, Z., Števo, S., (2010) Flow Shop Scheduling Using Differential Evolution. Management Information Systems, (5), pp. 008-013; Chaudhry, I.A., Munem Khan, A., Minimizing makespan for a no-wait flowshop using genetic algorithm (2012) Sadhana, (36), pp. 695-707; Fonseca, Y., Martínez, Y., Figueredo, A.E., Pernía, L.A., Behavior of the main parameters of the genetic algorithm for flow shop scheduling problems (2014) Revista Cubana de Ciencias Informáticas, (8), pp. 99-111; Framinan, J.M., Leisten, R., Ruiz-Usano, R., Efficient heuristics for flowshop sequencing with objectives of makespan and flowtime minimization (2002) European Journal of Operational Research, 141, pp. 561-571; Gabel, T., Riedmiller, M., On a successful application of multi-agent reinforcement learning to operations research benchmarks (2007) IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning, , Honolulu, USA. I. Press; Garey, M.R., Johnson, D.S., Sethi, R., The complexity of flowshop and jobshop scheduling (1976) Mathematics of Operations Research, 1, pp. 117-129; Johnson, S.M., Optimal two and three stage production schedules with setup times included (1954) Naval Research Logistics Quarterly, 1, pp. 402-452; Kubiak, W., Blazewicz, J., Formanowicz, P., Schmidt, G., Two-machine flowshop with limited machine availability (2002) Eur. J. Oper. Res, 136, pp. 528-540; Li, X., Baki, M.F., Aneja, Y.P., Flow shop scheduling to minimize the total completion time with a permanently present operator: Models and ant colony optimization metaheuristic (2011) Computers & Operations Research, (38), pp. 152-164; Ling Wang, L., Zhang, L., Zheng, D.-Z., An effective hybrid genetic algorithm for flow shop scheduling with limited buffers (2006) Computers & Operations Research, 33, pp. 2960-2971; Martínez, Y., (2012) A Generic Multi-Agent Reinforcement Learning Approach for Scheduling Problems, 169p. , PhD Thesis, Vrije Universiteit Brussel; Mehmet, Y., Betul, Y., Multi-objective permutation flow shop scheduling problem: Literature review, classification and current trends (2014) Omega, (45), pp. 119-135; Moriarty, D., Schultz, A., Grefenstette, J., Evolutionary algorithms for reinforcement learning (1999) Journal of Artificial Intelligence Research, 11, pp. 241-276; Nagar, A., Heragu, S., Haddock, J., A branch and bound approach for two-machine flowshop scheduling problem (1995) Journal of the Operational Research Society, 46, pp. 721-734; Nawaz, M., Enscore, E., Ham, I., A heuristic algorithm for the m-machine, n-job flowshop sequencing problem (1983) OMEGA-The International Journal of Management Science, 11, pp. 91-95; Pinedo, M., (2008) Scheduling Theory, Algorithms, and Systems, , Prentice Hall Inc., New Jersey; Quan-Ke, P., Fatih, M.T., Yun-Chia, L., A discrete particle swarm optimization algorithm for the no-wait flowshop scheduling problem (2008) Computers and Operations Research, 35, pp. 2807-2839; Rahimi-Vahed, A., Sm, M., A multi-objective particle swarm for a flowshop scheduling problem (2007) Journal of Combinatorial Optimization, 13, pp. 79-102; Rajendran, C., Ziegler, H., Ant-colony algorithms for permutation flowshop scheduling to minimize makespan-total flowtime of jobs (2004) European Journal of Operation Research, 115, pp. 426-438; Ramezanian, R., Aryanezhad, M.B., Heydar, M., A mathematical programming model for flow shop scheduling problems for considering just in time production (2010) International Journal of Industrial Engineering & Production Research, 21, pp. 97-104; Reeves, C.R., A genetic algorithm for flowshop sequencing (1995) Computers & Operations Research, 22, pp. 5-13; Ríos-Mercado, Z., An enhanced TSP based heuristic for makespan minimization in a Flowshop with setup times (1999) Journal of Heuristics, 5, pp. 57-74; Ríos-Mercado, Z., Secuenciando óptimamente líneas de flujo en sistemas de manufactura (2001) Revista de Ingenierías, 4, pp. 48-67; Ruiz, R., Moroto, C., A comprehensive review and evaluation of permutation flowshop heuristics (2005) European Journal of Operation Research, 64, pp. 278-295; Sadegheih, A., Scheduling problem using genetic algorithm, simulated annealing and the effects of parameter values on GA performance (2006) Applied Mathematical Modelling, 30, pp. 147-154; Sayin, S., Karabati, S., A bicriteria approach to the two-machine flowshop scheduling problem (1999) European Journal of Operational Research, 112, pp. 435-449; Šeda, M., (2007) Mathematical Models of Flow Shop and Job Shop Scheduling Problems. World Academy of Science, Engineering and Technology, (1), pp. 122-127; Sutton, R., Barto, A., (1998) Reinforcement Learning (An Introduction), , The MIT Press, Cambridge, Massachusetts; Taillard, E., Benchmarks for basic scheduling problems (1993) European Journal of Operational Research, 64, pp. 278-285; Tasgetiren, M.F., Liang, Y.C., Sevkli, M., Gencyilmaz, G., A particle swarm optimization algorithm for makespan and total flowtime minimization in the permutation flowshop sequencing problem (2007) European Journal of Operational Research, 177, pp. 1930-1947; Tavares-Neto, R.F., Godinho-Filho, M., An ant colony optimization approach to a permutational flowshop scheduling problem with outsourcing allowed (2011) Computers & Operations Research, (38), pp. 1286-1293; Toro, M., Restrepo, G., Granada, M., Adaptación de la técnica de Particle Swarm al problema de secuenciación de tareas (2006) Scientia et Technica UTP, 12, pp. 307-313; Toro, M., Restrepo, G.Y., Granada, E.M., Algoritmo genético modificado aplicado al problema de secuenciamiento de tareas en sistemas de producción lineal-Flow Shop (2006) Scientia et Technica, 12, pp. 285-290; Tsitsiklis, J., Asynchronous stochastic approximation an Q-learning (1994) Machine Learning, 16, pp. 185-202; Varadharajan, T., Rajendran, C., A multi-objective simulated-annealing algorithm for scheduling in flowshops to minimize the makespan and total flowtime of jobs (2005) European Journal of Operational Research, 167, pp. 772-795; Watkins, C., (1989) Learning from Delayed Rewards, p. 152. , PhD Thesis, University of Cambridge; Watkins, C., Dayan, P., Technical note: Q-learning (1992) Machine Learning, 8, pp. 279-292; Wu, T., Ye, N., Zhang, T., Comparison of distributed methods for resource allocation (2005) International Journal of Production Research, 43, pp. 515-536; Yamada, T., (2003) Studies on Metaheuristics for Jobshop and Flowshop Scheduling Problems, p. 120. , Tesis Doctoral, Kyoto University; Zhang, Y., Li, X., Wang, Q., Hybrid genetic algorithm for permutation flowshop scheduling problems with total flowtime minimization (2009) European Journal of Operational Research, 196, pp. 869-876},
publisher={Universidad de La Habana},
issn={02574306},
language={English},
abbrev_source_title={Invest. Oper.},
document_type={Article},
source={Scopus},
}

@ARTICLE{FonsecaReyna2015225,
author={Fonseca Reyna, Y.C. and Martínez Jiménez, Y. and Bermúdez Cabrera, J.M. and Méndez Hernández, B.M.},
title={A reinforcement learning approach for scheduling problems},
journal={Investigacion Operacional},
year={2015},
volume={36},
number={3},
pages={225-231},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937803432&partnerID=40&md5=99dd6b22d7914c4230d7a1401f9ef6d9},
affiliation={Universidad de Granma, Bayamo, Granma, Cuba; Universidad Central de Las Villas, Santa Clara, Villa Clara, Cuba; Departamento de Informática, Universidad de Granma, Km 18 1/2 Carretera Manzanillo, Bayamo, Granma, Cuba},
abstract={Scheduling problems are an important class of sequencing problems that can be found in many real life situations, especially in the field of production planning. The problem considered in this work is to find a permutation of operations to be sequentially processed on a number of machines under the restriction that the processing of each job has to be continuous with respect to the objective of minimizing the completion time of all jobs, known in literature as makespan or Cmax. This problem is as NP-hard, it is typical of combinatorial optimization and can be found in manufacturing environments, where there are conventional machines-tools and different types of pieces which can, in some scenarios, share the same route or not. The following research presents a Reinforcement Learning algorithm known as Q-Learning to solve scheduling problems, specifically Job Shop and Flow Shop. This algorithm is based on learning an action-value function that gives the expected utility of taking a given action in a given state, where an agent is associated to each of the resources. To validate the quality of the solutions, test cases of the specialized literature are used and the results obtained were compared with the reported optimal results.},
author_keywords={Flow shop;  Job shop;  Multi-agent systems;  Reinforcement learning;  Scheduling},
references={Álvarez, M., Toro, E., Gallego, R., Simulated annealing heuristic for flow shop scheduling problems (2008) Scientia et Technica, 14, pp. 159-164; Ancâu, M., On solving flow shop scheduling problems (2012) Proceedings of the Romanian Academy, 13, pp. 71-79; Beasley, J.E., (1990) OR-Library, , http://people.brunel.ac.uk/-mastjjb/jeb/info.html, Disponible en, Consulted January 14, 2014; Blazewicz, J., Ecker, K., Pesch, E., Schmidt, G., Weglarz, J., (2007) Handbook on Scheduling from Theory to Applications, , Springer-Verlag, Berlin; Čičková, Z., Števo, S., Flow shop scheduling using differential evolution (2010) Management Information Systems, 5, pp. 008-013; Gabel, T., Riedmiller, M., On a successful application of multi-agent reinforcement learning to operations research benchmarks (2007) IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning, , Honolulu, USA. I. Press; González, M., (2011) Soluciones Metaheurísticas Al Job-Shop Scheduling Problem with Sequence-Dependent Setup Times, p. 281. , PhD. Thesis, Universidad de Oviedo; Kaelbling, L.P., Littman, M., Moore, A., Reinforcement Learning: A survey (1996) Journal of Artificial Intelligence Research, 4, pp. 237-285; Martínez, Y., (2012) A Generic Multi-Agent Reinforcement Learning Approach Scheduling Problems., p. 169. , PhD Thesis, Vrije Universiteit Brussel; Moriarty, D., Schultz, A., Grefenstette, J., Evolutionary algorithms for reinforcement learning (1999) Journal of Artificial Intelligence Research, 11, pp. 241-276; Pinedo, M., (2008) Scheduling Theory, Algorithms, and Systems, , Prentice Hall Inc., New Jersey; Reeves, C.R., A genetic algorithm for flowshop sequencing (1995) Computers & Operations Research., 22, pp. 5-13; Ríos-Mercado, Z., An enhanced TSPbased heuristic for makespan minimization in a Flowshop with setup times (1999) Journal of Heuristics, 5, pp. 57-74; Ríos-Mercado, Z., Secuenciando óptimamente líneas de flujo en sistemas de manufactura (2001) Revista de Ingenierías, 4, pp. 48-67; Sutton, R., Barto, A., (1998) Reinforcement Learning (An Introduction), , The MIT Press, Cambridge, Massachusetts; Toro, M., Restrepo, G., Granada, M., Adaptación de la técnica de Particle Swarm al problema de secuenciación de tareas (2006) Scientia et Technica UTP, 12, pp. 307-313; Toro, M., Restrepo, G.Y., Granada, E.M., Algoritmo genético modificado aplicado al problema de secuenciamiento de tareas en sistemas de producción lineal-Flow Shop (2006) Scientia et Technica, 12, pp. 285-290; Tsitsiklis, J., Asynchronous stochastic approximation an Q-learning (1994) Machine Learning, 16, pp. 185-202; Watkins, C., (1989) Learning from Delayed Rewards., p. 152. , PhD Thesis, University of Cambridge; Watkins, C., Dayan, P., Technical note: Q-learning (1992) Machine Learning, 8, pp. 279-292; Yamada, T., (2003) Studies on Metaheuristics for Jobshop and Flowshop Scheduling Problems., p. 120. , PhD Thesis, Kyoto University},
publisher={Universidad de La Habana},
issn={02574306},
language={English},
abbrev_source_title={Invest. Oper.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Xu2014351,
author={Xu, D. and Son, Y.-J.},
title={Production planning and control via service-oriented simulation integration platform},
journal={IIE Annual Conference and Expo 2014},
year={2014},
pages={351-360},
note={cited By 1; Conference of IIE Annual Conference and Expo 2014 ; Conference Date: 31 May 2014 Through 3 June 2014;  Conference Code:114710},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910050599&partnerID=40&md5=02e36974dc661ac2e4204405aae1f932},
affiliation={Systems and Industrial Engineering, University of Arizona, Tucson, AZ  85721, United States},
abstract={In this paper, a service-oriented simulation integration platform is proposed to support manufacturing production planning and control of a complex manufacturing system. In particular, a multi-level service composition structure is considered, where simulation models at different levels of the hierarchy (e.g. equipment, shop, and enterprise) can be seamlessly and efficiently integrated. The Service oriented architecture Modeling Language (SoaML) is then employed to specify the service capabilities, service interfaces, service data model and chorography related to production planning and control. Furthermore, the proposed approach is demonstrated through single-period and multi-period inventory management. For the single-period inventory control, the optimal product price is estimated under different demand variability. For the multi-period inventory control, the convergence of a multi-agent reinforcement learning algorithm is demonstrated considering the eligibility trace. The proposed platform has been successfully deployed for integrating various different simulation models (e.g. discrete-event, agent-based, systems dynamics, process simulation). In addition, experiments illustrate the impact of demand variability on the product price, and the learning results of the optimal decision policy.},
author_keywords={Inventory control;  Reinforcement learning;  Service-oriented architecture;  Simulation},
keywords={Discrete event simulation;  Information services;  Inventory control;  Manufacture;  Multi agent systems;  Planning;  Production control;  Reinforcement learning, Complex manufacturing systems;  Inventory management;  Multi-agent reinforcement learning;  Production planning and control;  Service compositions;  Service oriented architecture modeling languages;  Simulation;  Simulation integration, Service oriented architecture (SOA)},
references={(2006) SOA-Based Services Buying Trends: A 2006 Survey of U.S. Companies, , May 1 IDC Document Number: 201644; Adams, D., McNamara, R., (2006) Service-Oriented Architecture and Best Practices, , TIBCO Software, Inc; Graves, S.C., (1999) Manufacturing Planning and Control, , Massachusetts Institute of Technology; Karnouskos, S., Baecker, O., Souza, L.M.S., Spieb, P., Integration of SOA-ready networked embedded devices in enterprise systems via a cross-layered web service infrastructure (2007) Emerging Technologies and Factory Automation, pp. 293-300. , IEEE Conference on, September 25-28, Patras, Greece; Shen, W., Hao, Q., Wang, S., Li, Y., Ghenniwa, H., An agent-based service-oriented integration architecture for collaborative intelligent manufacturing (2007) Robotics and Computer-Integrated Manufacturing, 23, pp. 315-325; Savio, D., Karnouskos, S., Wuwer, D., Bangemann, T., Dynamically optimized production planning using cross-layer SOA (2008) Proceedings of the 32nd Annual IEEE International Computer Software and Applications Conference, pp. 1361-1365. , July 28 - August 1, Turku, Finland; Jung, J.J., Service chain-based business alliance formation in service-oriented architecture (2011) Expert Systems with Applications, 38, pp. 2206-2211; Rathore, A., Balaraman, B., Zhao, X., Venkateswaran, J., Son, Y.J., Wysk, R.A., Development and benchmarking of an epoch time synchronization method for distributed simulation (2005) Journal of Manufacturing Systems, 24, pp. 69-78; Lee, S., Son, Y.J., Wysk, R.A., Simulation-based planning and control: From shop floor to top floor (2007) Journal of Manufacturing Systems, 26, pp. 85-98; Khouja, M., The single-period (Newsvendor) problem: Literature review and suggestions for future research (1999) Omega, International Journal of Management Science, 27 (5), pp. 537-553; Qin, Y., Wang, R., Vakharia, A.J., Chen, Y., Seref, M.M.H., The newsvendor problem: Review and directions for future research (2011) European Journal of Operational Research, 213, pp. 361-374; Giannoccaro, I., Pontrandolfo, P., Inventory management in supply chains: A reinforcement learning approach (2002) International Journal of Production Economics, 78, pp. 153-161; Ravulapati, K.K., Rao, J., Das, T.K., A reinforcement learning approach to stochastic business games (2004) IIE Transactions, 36, pp. 373-385; Chinthalapati, R.V.L., Yadati, N., Karumanchi, R., Learning dynamic prices in MultiSeller electronic retail markets with price sensitive customers, stochastic demands, and inventory replenishments (2006) IEEE Transactions on Systems, Man, and Cybernetics - Part C: Applications and Reviews, 36 (1), pp. 92-106; Chen, F.Y., Yan, H., Yao, L., A newsvendor pricing game (2004) IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans, 34 (4), pp. 450-456; Petruzzi, N.C., Dada, M., Pricing and the newsvendor problem: A review with extensions (1999) Operations Research, 47 (2), pp. 183-194; Busoniu, L., Babuska, R., Schutter, B., A comprehensive survey of multiagent reinforcement learning (2008) IEEE Transactions on Systems, Man, and Cybernetics - Part C: Applications and Reviews, 38 (2), pp. 156-172},
correspondence_address1={Son, Y.-J.; Systems and Industrial Engineering, University of ArizonaUnited States},
sponsors={Amazon; Arena; Boeing; et al.; UFC; VirginiaTech - College of Engineering},
publisher={Institute of Industrial Engineers},
isbn={9780983762430},
language={English},
abbrev_source_title={IIE Annual Conf. Expo},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Palombarini2012202,
author={Palombarini, J. and Martínez, E.},
title={SmartGantt - An interactive system for generating and updating rescheduling knowledge using relational abstractions},
journal={Computers and Chemical Engineering},
year={2012},
volume={47},
pages={202-216},
doi={10.1016/j.compchemeng.2012.06.021},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869501412&doi=10.1016%2fj.compchemeng.2012.06.021&partnerID=40&md5=50ae2bf2d5679bf10130167341f65a3b},
affiliation={GISIQ (UTN), Av. Universidad 450, Villa María 5900, Argentina; INGAR (CONICET-UTN), Avellaneda 3657, Santa Fe S3002 GJC, Argentina},
abstract={Generating and updating rescheduling knowledge that can be used in real time has become a key issue in reactive scheduling due to the dynamic and uncertain nature of industrial environments and the emergent trend towards cognitive systems in production planning and execution control. Disruptive events have a significant impact on the feasibility of plans and schedules. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. An industrial example where a current schedule must be repaired in response to unplanned events such as the arrival of a rush order, raw material delay, or an equipment failure which gives rise to the need for rescheduling is discussed. A software prototype (SmartGantt) for interactive schedule repair in real-time is presented. Results demonstrate that responsiveness is dramatically improved by using relational reinforcement learning and relational abstractions to develop a repair policy. © 2012 Elsevier Ltd.},
author_keywords={Batch plant management;  Cognitive production systems;  Manufacturing control;  Relational reinforcement learning;  Rescheduling;  Uncertainty},
keywords={Manufacturing control;  Production system;  Relational reinforcement learning;  Rescheduling;  Uncertainty, Abstracting;  Cognitive systems;  Plant management;  Production control;  Production engineering;  Software prototyping, Uncertainty analysis},
references={Adhitya, A., Srinivasan, R., Karimi, I.A., Heuristic rescheduling of crude oil operations to manage abnormal supply chain events (2007) AIChE Journal, 53, p. 397; Blockeel, H., De Raedt, L., Top-down induction of first order logical decision trees (1998) Artificial Intelligence, 101, p. 285; Blockeel, H., De Raedt, L., Jacobs, N., Demoen, B., Scaling up inductive logic programming by learning from interpretations (1999) Data Mining and Knowledge Discovery, 3, p. 59; Chapman, D., Kaelbling, L.P., Input generalization in delayed reinforcement learning: An algorithm and performance comparison (1991) Proceedings of the 12th international joint conference on Artificial intelligence (IJCAI'91), 2, pp. 726-731. , Morgan Kaufmann Publishers Inc., San Francisco, USA; Croonenborghs, T., (2009) Model-assisted approaches to relational reinforcement learning, , Ph.D. dissertation, Dept. of C. Sc., K. U. Leuven, Leuven, Belgium; De Raedt, L., (2008) Logical and relational learning, , Springer-Verlag, Berlin; Driessens, K., Ramon, J., Blockeel, H., Speeding up relational reinforcement learning through the use of an incremental first order decision tree learner (2001) Thirteenth European conference on machine learning, 2167, p. 97. , Springer, Heidelberg; Driessens, K., Ramon, J., Relational instance based regression for relational reinforcement learning (2003) 20th International conference on machine learning, 123. , AAAI Press, Washington; Džeroski, S., De Raedt, L., Driessens, K., Relational reinforcement learning (2001) Machine Learning, 43, p. 7; Henning, G., Cerdá, J., Knowledge-based predictive and reactive scheduling in industrial environments (2000) Computers and Chemical Engineering, 24, p. 2315; Li, Z., Ierapetritou, M., Reactive scheduling using parametric programming (2008) AIChE Journal, 54 (10), p. 2610; Martínez, E., Solving batch process scheduling/planning tasks using reinforcement learning (1999) Computers and Chemical Engineering, 23, p. 527; Miyashita, K., Sycara, K., CABINS: A framework of knowledge acquisition and iterative revision for schedule improvement and iterative repair (1994) Artificial Intelligence, 76, p. 377; Miyashita, K., Learning scheduling control through reinforcements (2000) International Transactions in Operational Research, 7, p. 125; Musier, R., Evans, L., An approximate method for the production scheduling of industrial batch processes with parallel units (1989) Computers and Chemical Engineering, 13, p. 229; Palombarini, J., Martínez, E., Learning to repair plans and schedules using a relational (deictic) representation (2010) Brazilian Journal of Chemical Engineering, 27 (3), p. 413; Pfeiffer, A., Kádár, B., Monostori, L., Stability-oriented evaluation of rescheduling strategies, by using simulation (2007) Computers in Industry, 58, pp. 630-643; Rangsaritratsamee, R., Ferrell, W.G., Kurz, M.B., Dynamic rescheduling that simultaneously considers efficiency and stability (2004) Computers and Industrial Engineering, 46, pp. 1-15; Shapiro, D., Langley, P., Shachter, R., Using background knowledge to speed reinforcement learning in physical agents (2001) Fifth international conference on autonomous agents, pp. 254-261; Sutton, R., Barto, A., (1998) Reinforcement learning: An introduction, , Boston, MIT Press; Trentesaux, D., Distributed control of production systems (2009) Engineering Applications of Artificial Intelligence, 22, p. 971; Van Otterlo, M., (2009) The logic of adaptive behavior, , Amsterdam, IOS Press; Vieira, G., Herrmann, J., Lin, E., Rescheduling manufacturing systems: A framework of strategies, policies and methods (2003) Journal of Scheduling, 6, p. 39; Watkins, C., (1989), Learning from delayed rewards. PhD Thesis, Cambridge University; Wilson, J., Gantt charts: A centenary appreciation (2003) European Journal of Operational Research, 149, pp. 430-437; Zaeh, M., Reinhart, G., Ostgathe, M., Geiger, F., Lau, C., A holistic approach for the cognitive control of production systems (2010) Advanced Engineering Informatics, 24, p. 300; Zhu, G., Bard, J., Yu, G., Disruption management for resource-constrained project scheduling (2005) Journal of the Operational Research Society, 56, pp. 365-381; Zweben, M., Davis, E., Doun, B., Deale, M., Iterative repair of scheduling and rescheduling (1993) IEEE Transactions on Systems, Man and Cybernetics, 23, p. 1588},
correspondence_address1={Martínez, E.; INGAR (CONICET-UTN), Avellaneda 3657, Santa Fe S3002 GJC, Argentina; email: ecmarti@santafe-conicet.gob.ar},
issn={00981354},
coden={CCEND},
language={English},
abbrev_source_title={Comput. Chem. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shin20128736,
author={Shin, M. and Ryu, K. and Jung, M.},
title={Reinforcement learning approach to goal-regulation in a self-evolutionary manufacturing system},
journal={Expert Systems with Applications},
year={2012},
volume={39},
number={10},
pages={8736-8743},
doi={10.1016/j.eswa.2012.01.207},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859217892&doi=10.1016%2fj.eswa.2012.01.207&partnerID=40&md5=ab19bd2e6b42b29025236138db59f721},
affiliation={Department of Industrial and Management Engineering, Hanbat National University, San 16-1, Duckmyoung-dong, Yuseong-gu, Daejeon 305-719, South Korea; Department of Industrial Engineering, Pusan National University, San 30, Jangjeon-dong, Geumjeong-gu, Busan 690-735, South Korea; School of Technology Management, Ulsan National Institute of Science and Technology (UNIST), Banyeon-ri 100, Ulsan 689-798, South Korea},
abstract={Up-to-date market dynamics has been forcing manufacturing systems to adapt quickly and continuously to the ever-changing environment. Self-evolution of manufacturing systems means a continuous process of adapting to the environment on the basis of autonomous goal-formation and goal-oriented dynamic organization. This paper proposes a goal-regulation mechanism that applies a reinforcement learning approach, which is a principal working mechanism for autonomous goal-formation. Individual goals are regulated by a neural network-based fuzzy inference system, namely, a goal-regulation network (GRN) updated by a reinforcement signal from another neural network called goal-evaluation network (GEN). The GEN approximates the compatibility of goals with current environmental situation. In this paper, a production planning problem is also examined by a simulation study in order to validate the proposed goal regulation mechanism. © 2012 Elsevier Ltd. All rights reserved.},
author_keywords={Agent;  Fractal organization;  Goal-regulation;  Production planning;  Reinforcement learning;  Self-evolutionary manufacturing system},
keywords={Continuous process;  Dynamic organization;  Fuzzy inference systems;  Goal-oriented;  Goal-regulation;  Market dynamics;  Network-based;  Production planning;  Regulation mechanisms;  Reinforcement learning approach;  Reinforcement signal;  Simulation studies;  Working mechanisms, Agents;  Neural networks;  Production control;  Reinforcement learning, Manufacture},
funding_details={National Research Foundation of KoreaNational Research Foundation of Korea, NRF},
funding_details={Ministry of Education, Science and TechnologyMinistry of Education, Science and Technology, MEST, 2009-0077660},
funding_text 1={This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science, and Technology ( 2009-0077660 ). The authors would like to express their gratitude for the support.},
references={Anderson, C.W., (1986) Learning and Problem Solving with Multilayer Connectionist Systems, , Ph.D. Thesis, University of Massachusetts; Arredondo, F., Martinez, E., Learning and adaptation of a policy for dynamic order acceptance in make-to-order manufacturing (2010) Computers & Industrial Engineering, 58, pp. 70-83; Barto, A.G., Sutton, R.S., Anderson, C.W., Neuronlike adaptive elements that can solve difficult learning control problems (1983) IEEE Transactions on Systems, Man, and Cybernetics, 13 (5), pp. 834-846; Bellman, R.E., (1957) Dynamic Programming, , Princeton University Press Princeton, NJ; Berenji, H.R., An architecture for designing fuzzy controllers using neural networks (1992) International Journal of Approximate Reasoning, 6 (2), pp. 267-292; Berenji, H.R., Khedkar, P., Learning and tuning fuzzy logic controllers through reinforcements (1992) IEEE Transactions on Neural Networks, 3 (5), pp. 724-740; Csáji, B.C., Monostori, L., Kádár, B., Reinforcement learning in a distributed market-based production control system (2006) Advanced Engineering Informatics, 20, pp. 279-288; Frayret, J.M., D'Amours, S., Montreuil, B., Coordination and control in distributed and agent-based manufacturing systems (2004) Production Planning & Control, 15 (1), pp. 42-54; Heragu, S.S., Graves, R.J., Kim, B., Onge, A.St., Intelligent agent based framework for manufacturing systems control (2002) IEEE Transactions on Systems, Man, and Cybernetics, 32 (5), pp. 560-573; Jouffe, L., Fuzzy inference system learning by reinforcement methods (1998) IEEE Transactions on Systems, Man, and Cybernetics-Part C: Applications and Reviews, 28 (3), pp. 338-355; Leitão, P., Restivo, F.J., ADACOR: A holonic architecture for agile and adaptive manufacturing control (2006) Computers in Industry, 57 (2), pp. 121-130; Liu, M., Sun, Z., Yan, J., Kang, J., An adaptive annealing genetic algorithm for the job-shop planning and scheduling problem (2011) Expert Systems with Applications, 38, pp. 9248-9255; Mandelbrot, B.B., (1982) The Fractal Geometry of Nature, , Freeman New York; Maturana, F., Shen, W., Norrie, D.H., MetaMorph: An adaptive agent-based architecture for intelligent manufacturing (1999) International Journal of Production Research, 37 (10), pp. 2159-2174; Mizutani, E., Learning from reinforcement (1997) Neuro-Fuzzy and Soft Computing: A Computational Approaches to Learning and Machine Intelligence, pp. 258-300. , C. Jang, C. Sun, E. Mizutani, Prentice-Hall Upper Saddle River, NJ; Rau, H., Cho, K., Genetic algorithm modeling for the inspection allocation in reentrant production systems (2009) Expert Systems with Applications, 36, pp. 11287-11295; Renna, P., Ambrico, M., Evaluation of cellular manufacturing configurations in dynamic conditions using simulation (2011) International Journal of Advanced Manufacturing Technology, , 10.1007/s00170-011-3255-0; Rumelhart, D., Hinton, G., Williams, R.J., Learning representations of back-propagation errors (1986) Nature, 323, pp. 533-536; Ryu, K., Jung, M., Goal-orientation mechanism in the fractal manufacturing system (2004) International Journal of Production Research, 42 (11), pp. 2207-2225; Ryu, K., Yücesan, E., Jung, M., Dynamic restructuring process for self-reconfiguration in the fractal manufacturing system (2006) International Journal of Production Research, 44 (15), pp. 3105-3129; Shen, W., Maturana, F., Norrie, D.H., MetaMorph II: An agent-based architecture for distributed intelligent design and manufacturing (2000) Journal of Intelligent Manufacturing, 11 (3), pp. 237-251; Shin, M., Cha, Y., Ryu, K., Jung, M., Conflict detection and resolution for goal-formation in the fractal manufacturing system (2006) International Journal of Production Research, 44 (3), pp. 447-465; Shin, M., Mun, J., Jung, M., Self-evolution framework of manufacturing systems based on fractal organization (2009) Computers & Industrial Engineering, 56, pp. 1029-1039; Shin, M., Mun, J., Lee, K., Jung, M., R-FrMS: A relation-driven fractal organisation for distributed manufacturing systems (2009) International Journal of Production Research, 47 (7), pp. 1791-1814; Sutton, R.S., (1984) Temporal Credit Assignment in Reinforcement Learning, , Ph.D. Thesis, University of Massachusetts; Sutton, R.S., Barto, A.G., (1998) Reinforcement Learning: An Introduction, , MIT Press Cambridge; Tan, A.H., Ong, Y.S., Tapanuj, A., A hybrid agent architecture integrating desire, intention and reinforcement learning (2011) Expert Systems with Applications, 38 (7), pp. 8477-8487; Van Brussel, H., Wyns, J., Valckenaers, P., Bongaerts, L., Peeters, P., Reference architecture for holonic manufacturing systems: PROSA (1998) Computers in Industry, 37, pp. 255-274; Wang, Y.C., Usher, J.M., Learning policies for single machine job dispatching (2004) Robotics and Computer-Integrated Manufacturing, 20, pp. 553-562; Wang, Y.C., Usher, J.M., A reinforcement approach for developing routing policies in multi-agent production scheduling (2007) International Journal of Advanced Manufacturing Technology, 33, pp. 323-333; Weiss, G., (1999) Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence, , MIT Press Cambridge; Zhang, Z., Zheng, L., Weng, M.X., Dynamic parallel machine scheduling with mean weighted tardiness objective by Q-learning (2007) International Journal of Advanced Manufacturing Technology, 34, pp. 968-980},
correspondence_address1={Jung, M.; School of Technology Management, Banyeon-ri 100, Ulsan 689-798, South Korea; email: myjung@unist.ac.kr},
issn={09574174},
coden={ESAPE},
language={English},
abbrev_source_title={Expert Sys Appl},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hatono2011347,
author={Hatono, I. and Yokota, K. and Fukunaga, S.},
title={A study on machine learning based modeling of skilled worker agents for production planing laerning support systems in street production},
journal={Tetsu-To-Hagane/Journal of the Iron and Steel Institute of Japan},
year={2011},
volume={97},
number={6},
pages={347-351},
doi={10.2355/tetsutohagane.97.347},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960005657&doi=10.2355%2ftetsutohagane.97.347&partnerID=40&md5=2366c68dea8c98a175e69b3d5a62a4fa},
affiliation={Information Science and Technology Center, Kobe University, 1-1 Rokko-dai, Nada Kobe 657-8501, Japan; Graduate School of Engineering, Kobe University, Japan; Graduate School of System Informatics, Kobe University, Japan},
abstract={This paper deals with machine learning based modelling of skilled worker agents for production planing learning support systems in steel production. In this paper, a try-and-error process in generating a schedule is assumed to consist of three steps: (1) select appropriate priority rules and evaluation items, (2) generate a schedule by using the priority rules, (3) evaluate of the generated schedule and revise the priority rules based on the evacuation. The scheduling generation processis modelledby using Stochastic Learning Automata, whichisakindof reinforcement learning method, to obtain thee.ective 'know-how' fora production planning learning support system. Asimulation experiment has been carried out in order to evaluate the model. The simulation results suggested that the know-hows obtained in the simulation experiments may be able to apply them into a production planning learning support system.},
author_keywords={Agents;  Reinforcement learning;  Scheduling;  Simulation;  Training support system},
keywords={Evaluation items;  Know-how;  Learning support systems;  On-machines;  Priority rules;  Production Planning;  Reinforcement learning method;  Simulation;  Simulation experiments;  Simulation result;  Skilled workers;  Steel production;  Stochastic learning automata;  Support systems;  Training support system, Automata theory;  Experiments;  Planning;  Production control;  Reinforcement learning;  Scheduling;  Steelmaking;  Technology transfer, Production engineering},
references={Tamaki, H., (2007) CAMP-ISIJ, 20, p. 934; Tamaki, H., (2008) CAMP-ISIJ, 21, p. 1094; Fujii, N., Tamaki, H., Hatono, I., (2009) CAMP-ISIJ, 22, p. 1010; Hatono, I., Haneda, S., Yokota, K., Fujii, N., Tamaki, H., (2009) CAMP-ISIJ, 22, p. 1006; Narendra, K., Thathachar, M.A.L., (1994) Learning Automata-An Introduction, , Prentice Hall; Ueno, N., (1993) Syst. Control. Info., 37 (4), p. 237},
correspondence_address1={Hatono, I.; Information Science and Technology Center, 1-1 Rokko-dai, Nada Kobe 657-8501, Japan},
issn={00211575},
coden={TEHAA},
language={Japanese},
abbrev_source_title={Tetsu To Hagane},
document_type={Article},
source={Scopus},
}

@ARTICLE{Arredondo201070,
author={Arredondo, F. and Martinez, E.},
title={Learning and adaptation of a policy for dynamic order acceptance in make-to-order manufacturing},
journal={Computers and Industrial Engineering},
year={2010},
volume={58},
number={1},
pages={70-83},
doi={10.1016/j.cie.2009.08.005},
note={cited By 36},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-72049095063&doi=10.1016%2fj.cie.2009.08.005&partnerID=40&md5=6c4b0fd7492c8e3395a0db87f6245b1f},
affiliation={INGAR(CONICET-UTN), Avellaneda 3657, Santa Fe, S3002 GJC, Argentina},
abstract={Order acceptance under uncertainty is a critical decision-making problem at the interface between customer relationship management and production planning of order-driven manufacturing systems. In this work, a novel approach for simulation-based development and on-line adaptation of a policy for dynamic order acceptance under uncertainty in make-to-order manufacturing using average-reward reinforcement learning is proposed. Locally weighted regression is used to generalize the gain value of accepting or rejecting similar orders regarding attributes such as product mix, price, size and due date. The order acceptance policy is learned by classifying an arriving order as belonging either to the acceptance set or to the rejection set. For exploitation, only orders in the acceptance set must be chosen for shop-floor scheduling. For exploration some orders from the rejection set are also considered as candidates for acceptance. Comparisons made with different order acceptance heuristics highlight the effectiveness of the proposed ARLOA algorithm to maximize the average revenue obtained per unit cost of installed capacity whilst quickly responding to unknown variations in order arrival rates and attributes. © 2009 Elsevier Ltd. All rights reserved.},
author_keywords={Demand management;  Make-to-order manufacturing;  Order acceptance;  Order similarity;  Reinforcement learning;  Revenue management},
keywords={Arrival rates;  Customer relationship management;  Decision-making problem;  Demand management;  Different order;  Due dates;  Dynamic order;  Gain values;  Installed capacity;  Learning and adaptation;  Locally weighted regression;  Make to order;  Manufacturing system;  On-line adaptation;  Order acceptance;  Per unit;  Product mix;  Production Planning;  Revenue management;  Shop floor;  Simulation-based, Industrial applications;  Manufacture;  Planning;  Production control;  Public relations;  Reinforcement;  Reinforcement learning, Engineering education},
references={Atkeson, C., Moore, A., Schaal, S., Locally weighted learning (1997) Artificial Intelligence Review, 11, pp. 11-73; Barut, M., Sridharan, V., Design and evaluation of a dynamic capacity apportionment procedure (2004) European Journal of Operational Research, 155, pp. 112-133; Barut, M., Sridharan, V., Revenue management in order-driven production systems (2005) Decision Sciences, 36, pp. 287-316; Calosso, T., Cantamessa, M., Vu, D., Villa, A., Production planning and order acceptance in business to business electronic commerce (2003) International Journal of Production Economics, 85, pp. 233-249; Calosso, T., Cantamessa, M., Gualano, M., Negotiation support for make-to-order operations in business-to-business electronic commerce (2004) Robotics and Computer-Integrated Manufacturing, 20, pp. 405-416; Das, T., Gosavi, A., Mahavedan, S., Marchalleck, N., Solving semi-markov decision problems using average reward reinforcement learning (1999) Management Science, 45 (4), pp. 560-574; Defregger, F., Kuhn, H., Revenue management for a make-to-order company with limited inventory capacity (2007) OR Spectrum, 29, pp. 137-156; Ebben, M., Hans, E., Olde Weghuis, F., Workload based order acceptance in job-shop environments (2005) OR Spectrum, 27, pp. 107-122; Enns, T., Evaluating shop-floor input control using rapid modelling (2000) International Journal of Production Economics, 63 (3), pp. 229-241; Enns, S., Costa, M., The effectiveness of input control based on aggregate versus bottleneck work loads (2002) Production Planning and Control, 13, pp. 614-624; Herbots, J., Herroelen, W., Leus, R., Dynamic order acceptance and capacity planning on a single bottleneck resource (2007) Naval Research Logistics, 54 (8), pp. 874-889; Ivanescu, C., Fransoo, J., Bertrand, J., Makespan estimation and order acceptance in batch process industries when processing times are uncertain (2002) OR Spectrum, 24, pp. 467-495; Ivanescu, C., (2004) Order acceptance under uncertainty in batch process industries, , PhD thesis, Technische Universiteit Eindhoven, Eindhoven; Jalora, A., (2006) Order acceptance and scheduling at a make-to-order system using revenue management, , PhD thesis, Texas A&M University; Ten Kate, H., Towards a better understanding of order acceptance (1994) International Journal of Production Economics, 37, pp. 139-152; Mainegra Hing, M., van Harten, A., Schuur, P., Order acceptance with reinforcement learning (2001), Technical Report 66. University of Twente, Netherlands; Mainegra Hing, M., (2006) Order acceptance under uncertainty: A reinforcement learning approach, , PhD thesis, Universiteit Twente, Technische Universiteit Eindhoven, Netherlands; Mainegra Hing, M., van Harten, A., Schuur, P., Reinforcement learning versus heuristics for order acceptance on a single resource (2007) Journal of Heuristics, 13, pp. 167-187; Moreira, M., (2005) Planning and controlling job-shop operations, , PhD dissertation, Faculty of Economics, University of Porto, Portugal; Nandi, A., Rogers, P., Behavior of an order release mechanism in a make-to-order manufacturing system with selected order acceptance (2003) Proceedings of the 2003 winter simulation conference, pp. 1251-1259; Nandi, A., Rogers, P., Using simulation to make-to-order acceptance/rejection decision (2004) Simulation, 80 (3), pp. 131-142; Nawijn, W., The optimal look-ahead policy for admission to a single server system (1985) Operations Research, 33 (3), pp. 625-643; Philipoom, P., Fry, T., Capacity-based order review/release strategies to improve manufacturing performance (1992) International Journal of Production Research, 30 (11), pp. 2559-2572; Quante, R., Meyer, H., Fleischmann, M., Revenue management and demand fulfilment: Matching applications, models, and software OR Spectrum, , in press, doi: 10.1007/s00291-008-0125-8; Raaymakers, W., (1999) Order acceptance and capacity loading in batch process industries, , PhD thesis, Technische Universiteit Eindhoven, Netherlands; Raaymakers, W., Bertrand, J., Fransoo, J., The performance of workload rules for order acceptance in batch chemical manufacturing (2000) Journal of Intelligent Manufacturing, 11, pp. 217-228; Raaymakers, W., Bertrand, J., Fransoo, J.F., Using aggregate estimation models for order acceptance in a decentralized production control structure for batch chemical manufacturing (2000) IIE Transactions, 32, pp. 989-998; Schwartz, A., A reinforcement learning method for maximizing undiscounted rewards (1993) Proceedings of the tenth international conference on machine learning, pp. 298-305; Singh, S., Reinforcement learning algorithms for average-payoff markovian decision processes (1994) Proceedings of the 12th national conference in artificial intelligent, , MIT Press. 2007; Smart, W., Pack Kaelbling, L., Practical reinforcement learning in continuous spaces (2000) Proceedings of the 17th international conference on machine learning, pp. 903-910. , Morgan Kaufmann; Sutton, R., Barto, A., (1998) Reinforcement learning: An introduction, , MIT Press, London, England; Snoek, M., Neuro-genetic order acceptance in a job shop setting (2000) Proceedings of 7th international conference on neural information processing, pp. 815-819. , Korea pp; Wang, J., Yang, J., Lee, H., Multicriteria order acceptance decision support in over demanded job-shops: A neural network approach (1994) Mathematical and Computer Modelling, 19 (5), pp. 1-19; Wester, F., Wijngaard, J., Zijm, W., Order acceptance strategies in a production-to-order environment with setup times and due-dates (1992) International Journal of Production Research, 30 (6), pp. 1313-1326; Wight, O., Input/output control: A real handle on lead time (1970) Production and Inventory Management Journal, 11 (3), pp. 9-30; Wouters, M., Relevant cost information for order acceptance decisions (1997) Production Planning and Control, 8 (1), pp. 2-9; Zorzini, M., Corti, D., Pozzetti, A., Due date (DD) quotation and capacity planning in make-to-order companies: Results from an empirical analysis (2008) International Journal of Production Economics, 112 (2), pp. 919-933},
correspondence_address1={Martinez, E.; INGAR(CONICET-UTN), Avellaneda 3657, Santa Fe, S3002 GJC, Argentina; email: ecmarti@santafe-conicet.gov.ar},
issn={03608352},
coden={CINDD},
language={English},
abbrev_source_title={Comput Ind Eng},
document_type={Article},
source={Scopus},
}

@ARTICLE{Reynolds2009281,
author={Reynolds, J.R. and O'Reilly, R.C.},
title={Developing PFC representations using reinforcement learning},
journal={Cognition},
year={2009},
volume={113},
number={3},
pages={281-292},
doi={10.1016/j.cognition.2009.05.015},
note={cited By 48},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350574601&doi=10.1016%2fj.cognition.2009.05.015&partnerID=40&md5=f7a8e188cbe1394c22a940e1bf69fda9},
affiliation={Department of Psychology, University of Denver, 2155 S. Race St, Denver, CO 80208, United States; Department of Psychology, University of Colorado, United States},
abstract={From both functional and biological considerations, it is widely believed that action production, planning, and goal-oriented behaviors supported by the frontal cortex are organized hierarchically [Fuster (1991); Koechlin, E., Ody, C., & Kouneiher, F. (2003). Neuroscience: The architecture of cognitive control in the human prefrontal cortex. Science, 424, 1181-1184; Miller, G. A., Galanter, E., & Pribram, K. H. (1960). Plans and the structure of behavior. New York: Holt]. However, the nature of the different levels of the hierarchy remains unclear, and little attention has been paid to the origins of such a hierarchy. We address these issues through biologically-inspired computational models that develop representations through reinforcement learning. We explore several different factors in these models that might plausibly give rise to a hierarchical organization of representations within the PFC, including an initial connectivity hierarchy within PFC, a hierarchical set of connections between PFC and subcortical structures controlling it, and differential synaptic plasticity schedules. Simulation results indicate that architectural constraints contribute to the segregation of different types of representations, and that this segregation facilitates learning. These findings are consistent with the idea that there is a functional hierarchy in PFC, as captured in our earlier computational models of PFC function and a growing body of empirical data. © 2009 Elsevier B.V. All rights reserved.},
author_keywords={Functional organization;  PFC;  Reinforcement learning;  Representation},
keywords={article;  behavior;  brain cortex;  learning;  mathematical model;  nerve cell network;  nerve cell plasticity;  prefrontal cortex;  priority journal;  reinforcement;  stimulus;  task performance;  working memory, Analysis of Variance;  Computer Simulation;  Humans;  Models, Psychological;  Nerve Net;  Neural Networks (Computer);  Prefrontal Cortex;  Reinforcement (Psychology)},
funding_details={National Institutes of HealthNational Institutes of Health, NIH},
funding_details={National Institute of Mental HealthNational Institute of Mental Health, NIMH, F32MH075300, P50MH079485, R01MH069597},
funding_details={Israel National Road Safety AuthorityIsrael National Road Safety Authority, NRSA, 1 F32 MH075300-01A2},
funding_text 1={The authors would like to thank Nicole Speer, Thomas Hazy, Seth Herd, and the rest of the Computational Cognitive Neuroscience Laboratory for helpful comments and suggestions. This research was supported in part by an NRSA post-doctoral training grant (1 F32 MH075300-01A2) and NIH R01 MH069597-01 (awarded to ROR).},
references={Badre, D., D'Esposito, M., Functional magnetic resonance imaging evidence for a hierarchical organization of the prefrontal cortex (2008) Journal of Cognitive Neuroscience, 19; Barone, P., Joseph, J.P., Prefrontal cortex and spatial sequencing in macaque monkey (1990) Experimental Brain Research, 78, pp. 447-464; Botvinick, M., Niv, Y., Barto, A.C., Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective (2008) Cognition, 113 (3), pp. 262-280; Botvinick, M., Plaut, D.C., Doing without schema hierarchies: A recurrent connectionist approach to normal and impaired routine sequential action (2004) Psychological Review, 111, pp. 395-429; Botvinick, M.M., Multilevel structure in behaviour and in the brain: A model of fuster's hierarchy (2007) Philosophical Transactions of the Royal Society of London Series B - Biological Sciences, 362 (1485), pp. 1615-1626; Botvinick, M.M., Hierarchical models of behavior and prefrontal function (2008) Trends in Cognitive Sciences, 12 (11), pp. 201-208; Braver, T.S., Barch, D.M., Keys, B.A., Carter, C.S., Cohen, J.D., Kaye, J.A., Context processing in older adults: Evidence for a theory relating cognitive control to neurobiology in healthy aging (2001) Journal of Experimental Psychology, 130, pp. 746-763; Braver, T.S., Bongiolatti, S.R., The role of frontopolar cortex in subgoal processing during working memory (2002) Neuroimage, 15, pp. 523-536; Braver, T.S., Cohen, J.D., Barch, D.M., The role of the prefrontal cortex in normal and disordered cognitive control: A cognitive neuroscience perspective (2002) Principles of fontal lobe function, pp. 428-448. , Stuss D.T., and Knight R.T. (Eds), Oxford University Press, Oxford; Braver, T.S., Reynolds, J.R., Donaldson, D.I., Neural mechanisms of transient and sustained cognitive control during task switching (2003) Neuron, 39, pp. 713-726; Brown, T.T., Lugar, H.M., Coalson, R.S., Miezin, F.M., Petersen, S.E., Schlaggar, B.L., Developmental changes in human cerebral functional organization for word generation (2005) Cerebral Cortex (New York, NY), 15, pp. 275-290; Bunge, S.A., Zelazo, P.D., A brain-based account of the development of rule use in childhood (2006) Current Directions in Psychological Science, 14 (3), pp. 118-121; Christoff, K., Gabrieli, J.D.E., The frontopolar cortex and human cognition: Evidence for a rostrocaudal hierarchical organization within the human prefrontal cortex (2000) Psychobiology, 28, pp. 168-186; Christoff, K., Prabhakaran, V., Dorfman, J., Zhao, Z., Kroger, J.K., Holyoak, K.J., Rostrolateral prefrontal cortex involvement in relational integration during reasoning (2001) Neuroimage, 14 (5), pp. 1136-4119; Christoff, K., Ream, J.M., Geddes, L.P.T., Gabrieli, J.D.E., Evaluating self-generated information: Anterior prefrontal contributions to human cognition (2003) Behavioral Neuroscience, 117 (6), pp. 1161-1168; Dayan, P., Bilinearity, rules, and prefrontal cortex (2008) Frontiers in Computational Neuroscience, 1 (1); D'Esposito, M., Postle, B.R., Ballard, D., Lease, J., Maintenance versus manipulation of information held in working memory: An event-related fmri study (1999) Brain and Cognition, 41, pp. 66-86; Duncan, J., Owen, A.M., Common regions of the human frontal lobe recruited by diverse cognitive demands (2000) Trends in Neurosciences, 23, pp. 475-482; Frank, M.J., Loughry, B., O'Reilly, R.C., Interactions between the frontal cortex and basal ganglia in working memory: A computational model (2001) Cognitive, Affective, and Behavioral Neuroscience, 1, pp. 137-160; Fuster, J.M., Prefrontal cortex and the bridging of temporal gaps in the perception-action cycle (1991) The development and neural bases of higher cognitive functions, 608, pp. 318-336. , Diamond A. (Ed), New York Academy of Science Press, New York; Fuster, J.M., Upper processing stages of the perception-action cycle (2004) Trends in Cognitive Sciences, 8 (4), pp. 143-145; Gogtay, N., Giedd, J.N., Lusk, L., Hayashi, K.M., Greenstein, D., Vaituzis, A.C., Dynamic mapping of human cortical development during childhood through early adulthood (2004) Proceedings of the National Academy of Sciences of the United States of America, 101 (21), pp. 8174-8179; Goldman-Rakic, P.S., Circuitry of primate prefrontal cortex and regulation of behavior by representational memory (1987) Handbook of Physiology - The Nervous System, 5, pp. 373-417; Halford, G.S., Can young children integrate premises in transitivity and serial order tasks? (1984) Cognitive Psychology, 16, pp. 65-93; Halford, G.S., (1993) Children's understanding: The development of mental models, , Erlbaum, Hillsdale, NJ; Halford, G.S., Wilson, W.H., Phillips, S., Processing capacity defined by relational complexity: Implications for comparative, developmental, and cognitive psychology (1999) The Behavioral and Brain Sciences, 21, p. 803; Haxby, J.V., Petit, L., Ungerleider, L.G., Courtney, S.M., Distinguishing the functional roles of multiple regions in distributed neural systems for visual working memory (2000) Neuroimage, 11, pp. 380-391; Johnson, M.K., Raye, C.L., Mitchell, K.J., Greene, E.J., Anderson, A.W., Fmri evidence for an organization of prefrontal cortex by both type of process and type of information (2003) Cerebral Cortex (New York, NY), 13, pp. 265-273; Koechlin, E., Basso, G., Pietrini, P., Panzer, S., Grafman, J., The role of the anterior prefrontal cortex in human cognition (1999) Nature, 399, pp. 148-151; Koechlin, E., Hyafil, A., Anterior prefrontal function and the limits of human decision-making (2007) Science (New York, NY), 318 (5850), pp. 594-598; Koechlin, E., Ody, C., Kouneiher, F., Neuroscience: The architecture of cognitive control in the human prefrontal cortex (2003) Science, 424, pp. 1181-1184; Koechlin, E., Summerfield, C., An information theoretical approach to prefrontal executive function (2007) Trends in Cognitive Sciences, 11 (6), pp. 229-235; Kroger, J.K., Sabb, F.W., Fales, C.L., Bookheimer, S.Y., Cohen, M.S., Holyoak, K.J., Recruitment of anterior dorsolateral prefrontal cortex in human reasoning: A parametric study of relational complexity (2002) Cerebral Cortex (New York, NY), 12, pp. 477-485; Lashley, K.S., The problem of serial order in behavior (1951) Cerebral mechanisms in behavior: The hixon symposium, pp. 112-136. , Jeffress L.A. (Ed), Wiley, New York; Long, J.S., Ervin, L.H., Using heteroscedasticity consistent standard errors in the linear regression model (2000) The American Statistician, 54, pp. 217-224; Miller, E.K., The prefrontal cortex: No simple matter (2000) Neuroimage, 11, pp. 447-450; Miller, E.K., Cohen, J.D., An integrative theory of prefrontal cortex function (2001) Annual Review of Neuroscience, 24, pp. 167-202; Miller, G.A., Galanter, E., Pribram, K.H., (1960) Plans and the structure of behavior, , Holt, New York; Nystrom, L.E., Braver, T.S., Sabb, F.W., Delgado, M.R., Noll, D.C., Cohen, J.D., Working memory for letters, shapes, and locations: Fmri evidence against stimulus-based regional organization in human prefrontal cortex (2000) Neuroimage, 11, pp. 424-446; O'Donnell, S., Noseworth, M., Levine, B., Dennis, M., Cortical thickness of the frontopolar area in typically developing children and adolescents (2005) Neuroimage, 24 (4), pp. 948-954; O'Reilly, R.C., Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm (1996) Neural Computation, 8 (5), pp. 895-938; O'Reilly, R.C., Six principles for biologically-based computational models of cortical cognition (1998) Trends in Cognitive Sciences, 2 (11), pp. 455-462; O'Reilly, R.C., Generalization in interactive networks: The benefits of inhibitory competition and Hebbian learning (2001) Neural Computation, 13, pp. 1199-1242; O'Reilly, R.C., Frank, M.J., Making working memory work: A computational model of learning in the prefrontal cortex and basal ganglia (2006) Neural Computation, 18, pp. 283-328; O'Reilly, R.C., Frank, M.J., Hazy, T.E., Watz, B., Pvlv: The primary value and learned value pavlovian learning algorithm (2007) Behavioral Neuroscience, 121, pp. 31-49; O'Reilly, R.C., Munakata, Y., (2000) Computational explorations in cognitive neuroscience: Understanding the mind by simulating the brain, , MIT Press, Cambridge, MA; Paine, R.W., Tani, J., How hierarchical control self-organizes in artificial adaptive systems (2005) Adaptive Behavior, 13 (3), pp. 211-225. , doi:10.1177/105971230501300303; Petrides, M., The role of the mid-dorsolateral prefrontal cortex in working memory (2000) Experimental Brain Research, 133, p. 44; Petrides, M., Pandya, D.N., Efferent association pathways from the rostral prefrontal cortex in the macaque monkey (2007) The Journal of Neuroscience, 27 (43), pp. 11573-11586; Pickett, M., Barto, A., Policyblocks: An algorithm for creating useful macro-actions in reinforcement learning (2002) ICML'02: Proceedings of the nineteenth international conference on machine learning, pp. 506-513. , Morgan Kaufmann Publishers Inc., San Francisco, CA, USA; Poldrack, R.A., Wagner, A.D., Prull, M.W., Desmond, J.E., Glover, G.H., Gabrieli, J.D., Functional specialization for semantic and phonological processing in the left inferior prefrontal cortex (1999) Neuroimage, 10, pp. 15-35; Rao, S.C., Rainer, G., Miller, E.K., Integration of what and where in the primate prefrontal cortex (1997) Science, 276 (5313), pp. 821-824; Raye, C.L., Johnson, M.K., Mitchell, K.J., Reeder, J.A., Greene, E.J., Neuroimaging a single thought: Dorsolateral pfc activity associated with refreshing just-activated information (2002) Neuroimage, 15, pp. 447-453; Reynolds, J., West, R., Braver, T., Distinct neural circuits support transient and sustained processes in prospective memory and working memory (2008) Cerebral Cortex, , New York, NY, doi:10.1093/cercor/bhn164; Reynolds, J., Zacks, J., Braver, T., A computational model of event segmentation from perceptual prediction (2007) Cognitive Science, 31, pp. 613-634; Reynolds, J. R. (2005). On the roles of duration and computational complexity in the recruitment of fronto-polar prefrontal cortex. Ph.D. Thesis. Washington University in St. Louis; Reynolds, J.R., Mozer, M.C., (2009) Temporal dynamics of cognitive control, , Advances in Neural Information Processing Systems; Rizzolatti, G., Arbib, M.A., Language within our grasp (1998) Trends in Neurosciences, 21, pp. 188-194; Rizzolatti, G., Fadiga, L., Gallese, V., Fogassi, L., Premotor cortex and the recognition of motor actions (1996) Cognitive Brain Research, 3, pp. 131-141; Robin, N., Holyoak, K.J., Relational complexity and the functions of prefrontal cortex (1995) The cognitive neurosciences. 1st ed., pp. 987-997. , Gazzaniga M.S. (Ed), MIT Press, Cambridge, MA; Rougier, N.P., Noelle, D., Braver, T.S., Cohen, J.D., O'Reilly, R.C., Prefrontal cortex and the flexibility of cognitive control: Rules without symbols (2005) Proceedings of the National Academy of Sciences, 102 (20), pp. 7338-7343; Shaw, P., Kabani, N., Lerch, J., Eckstrand, K., Lenroot, R., Gogtay, N., Neurodevelopmental trajectories of the human cerebral cortex (2008) Journal of Neuroscience, 28, pp. 3586-3594; Singh, S., Barto, A., Chentanez, N., Intrinsically motivated reinforcement learning (2005) Advances in neural information processing systems 17: Proceedings of the 2004 conference, pp. 1281-1288. , L. K. Saul, Y. Weiss, & L. Bottou Eds, Cambridge: MIT Press; Sowell, E.R., Thompson, P.M., Holmes, C.J., Jernigan, T.L., Toga, A.W., In vivo evidence for post-adolescent brain maturation in frontal and striatal regions (1999) Nature Neuroscience, 2 (10), pp. 859-861; (2002) Principles of frontal lobe function, , Stuss D.T., and Knight R.T. (Eds), Oxford University Press, New York; Sutton, R., Precup, D., Singh, S., Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning (1999) Artificial Intelligence, 112 (1-2), pp. 181-211; Wallis, J.D., Anderson, K.C., Miller, E.K., Single neurons in prefrontal cortex encode abstract rules (2001) Nature, 411, pp. 953-956; Wilson, F.A., Scalaidhe, S.P., Goldman-Rakic, P.S., Dissociation of object and spatial processing domains in primate prefrontal cortex (1993) Science (New York, NY), 260, pp. 1955-1957; Wood, J.N., Grafman, J., Human prefrontal cortex: Processing and representational perspectives (2003) Nature Reviews, 4, pp. 139-147},
correspondence_address1={Reynolds, J.R.; Department of Psychology, 2155 S. Race St, Denver, CO 80208, United States; email: jeremy.reynolds@psy.du.edu},
issn={00100277},
coden={CGTNA},
pubmed_id={19591977},
language={English},
abbrev_source_title={Cognition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Askari-Nasab200861,
author={Askari-Nasab, H. and Frimpong, S. and Szymanski, J.},
title={Investigating continuous time open pit dynamics},
journal={Journal of the Southern African Institute of Mining and Metallurgy},
year={2008},
volume={108},
number={2},
pages={61-71},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-41849086630&partnerID=40&md5=f8a424ffc47ed0938a372964466f4fdc},
affiliation={School of Mining and Petroleum Engineering, University of Alberta, Edmonton, AB, Canada; Department of Mining and Nuclear Engineering, University of Missouri-Rolla, Rolla, United States},
abstract={Current mine production planning, scheduling, and allocation of resources are based on mathematical programming models. In practice, the optimized solution cannot be attained without examining all possible combinations and permutations of the extraction sequence. Operations research methods have limited applications in large-scale surface mining operations because the number of variables becomes too large. The primary objective of this study is to develop and implement a hybrid simulation framework for the open pit scheduling problem. The paper investigates the dynamics of open pit geometry and the subsequent material movement as a continuous system described by time-dependent differential equations. The continuous open pit simulator (COPS) implemented in MATLAB, based on modified elliptical frustum is used to model the evolution of open pit geometry in time and space. Discrete open pit simulator (DOPS) mimics the periodic expansion of the open pit layouts. Function approximation of the discrete simulated push-backs provides the means to convert the set of partial differential equations (PDEs), capturing the dynamics of open pit layouts, to a system of ordinary differential equations (ODEs). Numerical integration with the Runge-Kutta scheme yields the trajectory of the pit geometry over time with the respective volume of materials and the net present value (NPV) of the mining operation. A case study of an iron ore mine with 114 000 blocks was carried out to verify and validate the model. The optimized pit limit was designed using Lerchs-Grossman's algorithm. The best-case annual schedule, generated by the shells node in Whittle Four-X yielded an NPV of $449 million over a 21-year mine life at a discount rate of 10% per annum. DOPS best scenario out of 2 500 simulation iterations resulted in an NPV of $443 million and COPS yielded an NPV of $440 million over the same time span. The hybrid simulation model is the basis for future research using reinforcement learning based on goal-directed intelligent agents. © The Southern African Institute of Mining and Metallurgy, 2008.},
keywords={Mathematical programming models;  Open pit dynamics;  Production planning, Computer simulation;  Continuous time systems;  Mathematical programming;  Operations research;  Ordinary differential equations, Open pit mining},
references={CHANDA, E.K. and WILKE, F.L. An EPD model of open pit short term production scheduling optimization for stratiform orebodies. Proceedings of 23rd APCOM Symposium, SME (ed.), 1992. pp. 759-768; ELVELI, B., Open pit mine design and extraction sequencing by use OR and AI concepts (1995) international Journal of Surface Mining. Reclamation and Environment, 9, pp. 149-153; ERARSLAN, K., CELEBI, N., A simulative model for optimum open pit design (2001) The Canadian Mining and Metallurgical Bulletin, 94, pp. 59-68; HALATCHEV, R.A., A model of discounted profit variation of open pit production sequencing optimization (2005) Proceedings of Application of Computers and Operations Research in the Mineral Industry, pp. 315-323. , Tucson, Arizona. Taylor & Francis Grouped; ONUR, A.H., DOWD, P.A., Open pit optimization-part 2: Production scheduling and inclusion of roadways (1993) Transactions of the Institution of Mining and Metallurgy, 102, pp. A105-A113; TOLWINSKI, B. and UNDERWOOD, R. An algorithm to estimate the optimal evolution of an open pit mine. Proceedings of 23rd APCOM Symposium, University of Arizona. SME (ed.), Littleton, Colorado, 1992. pp. 399-409; DOWD, P.A., ELVAN, L., Dynamic programming applied to grade control in sub-level open stopping (1987) Trans. IMM, 96, pp. A171-A178; CHANDA, E.K., DAGDELEN, R., Optimal blending of mine production using goal programming and interactive graphics system (1995) International Journal of Surface Mining Reclamation and Environment, 9, pp. 203-208; YOUDI, Z., QINGZIANG, C., and LIXIN, W. Combined approach for surface mine short-term planning optimization. Proceedings of 23rd APCOM Symposium, SME (ed.), Colorado, 1992. pp. 499-506; MANN, C. and WILKE, F.L. Open pit short term mine planning for grade control - a combination of CAD techniques and linear programming. Proceedings of 23rd APCOMSymposium, SME (ed.), Colorado, 1992. pp. 487-497; FRIMPONG, S., ASA, E., SZYMANSKI, J.M., ULSOPS: Multivariate optimized pit shells simulator for tactical mine planning (1998) International Journal of Surface Mining, Reclamation & Environment, 12, pp. 163-169; FRIMPONG, S., ASA, E., SUGLO, R.S., Numerical simulation of surface mine production system using pit shell simulator (2001) Mineral Resources Engineering, 10, pp. 185-203; ASKARI-NASAB, H., SZYMANKSI, J., Modelling open pit dynamics using Monte Carlo simulation (2005) Proceedings of Computer Applications in the Minerals Industry (CAM1), Banff, Alberta, Canada, pp. 21-32. , On CD-ROM, The Reading Matrix Inc, CA, USA; ASKARI-NASAB, H., AWUAH-OFFEI, K., FRIMPONG, S., Stochastic simulation of open pit pushbacks with a production simulator Proceedings of CIM Mining Industry Conference and Exhibition, p. 2004. , Edmonton, Alberta, Canada, pp. on CD-ROM; CARTWRIGHT, J.H.E., PIRO, O., The dynamics of Runge-Kutta methods (1992) Int. J. Bifurcations Chaos, 2, pp. 427-449; WHITTLE PROGRAMMING PTY, LTD. Whittle strategic mine planning software, Gemcom Software International Inc., 1998-2004; ABRAMOWITZ, M., (1972) Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, p. 880. , Stegun, I.A.E, eds, New York: Dover; LERCHS, H. and GROSSMANN, I.F. Optimum design of open-pit mines. The Canadian Mining and Metallurgical Bulletin, Transactions. 1965. LXVIII, pp. 17-24; GSLIB geostatistical software library and user's guide (1998) Applied geostatistics series, , DEUTSCH, C.V. and JOURNEL, A.G, eds, New York, Oxford University Press; KRIGE, D.G., (1951) A statistical approach to some basic mine valuation and allied problems at the Witwatersrand, , Masters thesis, University of Witwatersrand, South Africa; MARQUARDT, D., An algorithm for least squares estimation of nonlinear parameters (1963) SIAM J. Appl. Math, 11, pp. 431-441},
correspondence_address1={Askari-Nasab, H.; School of Mining and Petroleum Engineering, , Edmonton, AB, Canada},
issn={22256253},
coden={JSAMA},
language={English},
abbrev_source_title={J S Afr Inst Min Metall},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cao20031417,
author={Cao, H. and Xi, H. and Smith, S.F.},
title={A reinforcement learning approach to production planning in the fabrication/fulfillment manufacturing process},
journal={Winter Simulation Conference Proceedings},
year={2003},
volume={2},
pages={1417-1423},
note={cited By 17; Conference of Proceedings of the 2003 Simulation Conference: Driving Innovation ; Conference Date: 7 December 2003 Through 10 December 2003;  Conference Code:62143},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-1642436882&partnerID=40&md5=b6c7aed3ed944cc5d97397d404e6aa75},
affiliation={IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, United States; Robotics Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={We have used Reinforcement Learning together with Monte Carlo simulation to solve a multi-period production planning problem in a two-stage hybrid manufacturing process (a combination of build-to-plan with build-to-order) with a capacity constraint. Our model minimizes inventory and penalty costs while considering real-world complexities such as different component types sharing the same manufacturing capacity, multi-end-products sharing common components, multi-echelon bill-of-material (BOM), random lead times, etc. To efficiently search in the huge solution space, we designed a two-phase learning scheme where "good" capacity usage ratios are first found for different decision epochs, based on which a detailed production schedule is further improved through learning to minimize costs. We will illustrate our approach through an example and conclude the paper with a discussion of future research directions.},
keywords={Computer simulation;  Constraint theory;  Cost effectiveness;  Decision making;  Heuristic methods;  Inventory control;  Learning algorithms;  Mathematical models;  Monte Carlo methods;  Operations research;  Optimization;  Parallel processing systems;  Parameter estimation;  Resource allocation;  Scheduling;  Strategic planning, Discretization;  Supply chain management (SCM), Production control},
references={Cao, H., Lin, G.Y., Xi, H., Smith, S.F., An Agent Based Enterprise Computing Framework for High Performance Supply Chain Simulation (2002) Post-conference Proceedings of Int'l Conf. on Parallel and Distributed Processing Techniques and Applications (PDPTA'02), , Las Vegas, Nevada, USA, June 24-27; Cao, H., Cheng, F., Smith, S., (2003) A Constraint-based Method for Inventory-service Optimization in a Fabrication/Fulfillment Manufacturing Process, , INFORMS Annual Meeting Atlanta, 2003; Finke, A.D., Medeiros, D.J., Traband, M.T., Shop Scheduling Using Tabu Search and Simulation (2002) Proceedings of the 2002 Winter Simulation Conference, pp. 1013-1017. , ed. E. Yücesan, C.-H. Chen, J.L. Snowdon, and J.M. Charnes, Piscataway, New Jersey: Institute of Electrical and Electronics Engineers. 2002; Joines, J., Gupta, D., Gokce, M.A., King, R.E., Kay, M.G., Supply Chain Multi-Objective Simulation Optimization (2002) Proceedings of the 2002 Winter Simulation Conference, pp. 1306-1313. , ed. E. Yücesan, C.-H. Chen, J.L. Snowdon, and J.M. Charnes, Piscataway, New Jersey: Institute of Electrical and Electronics Engineers. 2002; Kaelbling, L.P., Littman, M., Moore, A., Reinforcement Learning: A Survey (1996) Journal of Artificial Intelligence Research, 4, pp. 237-285. , 1996; Nahmias, S., (1997) Production and Operations Analysis, pp. 338-339. , McGraw-Hill Higher Education; Ólafsson, S., Kim, J., Simulation Optimization (2002) Proceedings of the 2002 Winter Simulation Conference, pp. 79-84. , ed. E. Yücesan, C.-H. Chen, J.L. Snowdon, and J.M. Charnes, Piscataway, New Jersey: Institute of Electrical and Electronics Engineers. 2002; Shi, L., Ólafsson, S., Nested Partitions Method for Global Optimization (2000) Operations Research, (48), p. 3; Van Roy, B., Bertsekas, D.P., Lee, Y., Tsitsiklis, J.N., A Neuro-Dynamic Programming Approach to Retailer Inventory Management (1997) Proceedings of the IEEE Conference on Decision and Control},
correspondence_address1={Cao, H.; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, United States},
editor={Chick S.E., Sanchez P.J., Ferrin D., Morrice D.J.},
sponsors={ASA; ACM/SIGSIM; IEEE/CS; IEEE/SMC},
address={New Orleans, LA},
issn={02750708},
coden={WSCPD},
language={English},
abbrev_source_title={Winter Simul Conf Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Suwa20013567,
author={Suwa, H. and Araki, N. and Masuoka, M. and Fujii, S.},
title={Heuristics Acquisition from Problem Solving Process and Knowledge Representation by State Action Network: Application to Two Dimensional Packing Problems},
journal={Nihon Kikai Gakkai Ronbunshu, C Hen/Transactions of the Japan Society of Mechanical Engineers, Part C},
year={2001},
volume={67},
number={663},
pages={3567-3574},
doi={10.1299/kikaic.67.3567},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024449955&doi=10.1299%2fkikaic.67.3567&partnerID=40&md5=3ab31fd2754d7998a64f430c49e8d578},
affiliation={Department of Industrial and Systems Engineering, Setsunan University, 17 8 Ikedanaka machi, Neyagawa shi, Osaka, 572 8508, Japan},
abstract={This paper proposes a new model and method for knowledge acquisition for planning problems in manufacturing, such as production planning, packing problems, and so forth, based on a problem solving process of a planner. In our proposed model, knowledge used for solving the objective problem is obtained through construction of stale action network. A state action network consists of states describing some phases of the problem solving process and planner’s actions in each phase. Moreover, reinforcement learning is used to refine the obtained state action network and to evaluate the problem solving process. We apply the proposed method to two dimensional packing problems and demonstrate its applicability and effectiveness through some computational experiments. © 2001, The Japan Society of Mechanical Engineers. All rights reserved.},
author_keywords={Heuristics;  Knowledge Acquisition;  Reinforcement Learning;  Two Dimensional Packing Problem},
references={Kusiak, A., Chen, M., Expert Systems for planning and scheduling manufacturing systems (1998) European Journal of Operational Research, 34, pp. 113-130; Hynynen, J., Using artificial intelligence technologies in production management (1992) Computers in industry, 19, pp. 21-35; Artiba, A., Tahon, C., Production planning knowledge-based system for pharmaceutical manufacturing lines (1992) European Journal of Operational Research, 61, pp. 18-29; Minton, S., Machine learning methods for planning (1993), Morgan Kaufman, CA; Doulgeri, Z., D’alessandro, G., Magaletti, N., A hierarchical knowledge-based scheduling and control for FMSs (1993) Int. J. Computer Integrated Manufacturing, 6-3, pp. 191-200; Watkins, C.J.C.H., Dayan, P., Technical note: Q-learing (1992) Machine Learning, 8-3, pp. 279-292; Coffman, E.G., Garey, M.R., Johnson, D.S., Tarjan, R.E., Performance bounds for level-oriented two-dimensional packing algorithm (1980) SIAM journal on computing, 9-4, pp. 808-826; Baker, B.S., Katseff, H.P., A 5/4 algorithm for two-dimensional packing (1981) Journal of Algorithms, 2, pp. 348-368},
correspondence_address1={Suwa, H.; Department of Industrial and Systems Engineering, 17 8 Ikedanaka machi, Neyagawa shi, Osaka, 572 8508, Japan},
issn={03875024},
language={English},
abbrev_source_title={Nihon Kikai Gakkai Ronbunshu C},
document_type={Article},
source={Scopus},
}
